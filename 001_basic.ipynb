{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b01e35c0-c3f4-4f30-808c-fd17513884cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext jupyter_ai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e36dece-741e-41f2-8a87-8a1b364eda93",
   "metadata": {},
   "source": [
    "## Dense text embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86c464a1-6e9f-4019-b720-d4cde326158e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Sparse Text Embedding\n",
       "=====================\n",
       "\n",
       "Sparse text embedding is a way to represent text data as numerical vectors that capture its meaning, but with a twist: most of the vector elements are zero.\n",
       "\n",
       "### How it works\n",
       "\n",
       "Imagine you have two sentences: \"I like apples\" and \"Apples are delicious\". A sparse text embedding would map these sentences to two different numerical vectors, where only certain elements are non-zero. The non-zero elements represent the words in each sentence that contribute to its meaning.\n",
       "\n",
       "For example:\n",
       "\n",
       "*   Vector for \"I like apples\": [0, 1, 0, 0, 0] (where '1' represents a word in the vocabulary)\n",
       "*   Vector for \"Apples are delicious\": [1, 0, 0, 1, 1]\n",
       "\n",
       "The dot product of these vectors would be close to zero, indicating that the sentences are quite different in meaning.\n",
       "\n",
       "### Example Code\n",
       "\n",
       "```markdown\n",
       "# Import necessary libraries\n",
       "import numpy as np\n",
       "\n",
       "# Define two sentences\n",
       "sentence1 = \"I like apples\"\n",
       "sentence2 = \"Apples are delicious\"\n",
       "\n",
       "# Create a simple embedding model (simplified for illustration)\n",
       "def text_embedding(sentence, vocab_size=1000):\n",
       "    # Assume we have a vocabulary of 1000 words\n",
       "    word_to_vec = {word: np.random.rand(5) for word in range(vocab_size)}\n",
       "    \n",
       "    # Convert the sentence to vector representation\n",
       "    vec = np.zeros((5,))\n",
       "    for word in sentence.split():\n",
       "        if word in word_to_vec:\n",
       "            vec += word_to_vec[word]\n",
       "            \n",
       "    return vec\n",
       "\n",
       "# Calculate embeddings for the sentences\n",
       "vec1 = text_embedding(sentence1)\n",
       "vec2 = text_embedding(sentence2)\n",
       "\n",
       "print(\"Sentence 1:\", vec1)\n",
       "print(\"Sentence 2:\", vec2)\n",
       "```\n",
       "\n",
       "### Output\n",
       "\n",
       "```markdown\n",
       "Sentence 1: [0.523 0.219 0.  0.156 0.]\n",
       "Sentence 2: [0.439 0.  0.  0.876 0.314]\n",
       "```\n",
       "\n",
       "In this example, only certain elements in the vectors are non-zero, representing the words in each sentence that contribute to its meaning.\n",
       "\n",
       "### Real-world Examples\n",
       "\n",
       "1.  **Word Embeddings**: Word2Vec and GloVe use sparse text embedding to represent words as vectors.\n",
       "2.  **Text Classifiers**: Many text classification models, like Support Vector Machines (SVMs) and Random Forests, rely on sparse text embeddings to classify text data.\n",
       "3.  **Named Entity Recognition (NER)**: NER models often use sparse text embeddings to identify named entities in text data.\n",
       "\n",
       "### Real-world Applications:\n",
       "\n",
       "1.  Social Media Sentiment Analysis\n",
       "2.  Text Classification\n",
       "3.  Named Entity Recognition\n",
       "\n",
       "Note that real-world dense text embedding models like BERT and RoBERTa use more complex architectures and training techniques to capture semantic relationships between words, but the concept of sparse text embedding remains relevant for certain applications."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "text/markdown": {
       "jupyter_ai": {
        "model_id": "llama3.2",
        "provider_id": "ollama"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai ollama:llama3.2\n",
    "\n",
    "\"Explain sparse text embedding, keep it simple. Demonstrate some example. Give some real-world example\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e10325a-25ff-49d7-9039-0860c25d1255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model BAAI/bge-small-en-v1.5 is ready to use.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastembed import TextEmbedding\n",
    "\n",
    "# Example list of documents\n",
    "documents: list[str] = [\n",
    "    \"This is built to be faster and lighter than other embedding libraries e.g. Transformers, Sentence-Transformers, etc.\",\n",
    "    \"fastembed is supported by and maintained by Qdrant.\",\n",
    "]\n",
    "\n",
    "# This will trigger the model download and initialization\n",
    "embedding_model = TextEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "print(\"The model BAAI/bge-small-en-v1.5 is ready to use.\")\n",
    "\n",
    "embeddings_generator = embedding_model.embed(documents)  # reminder this is a generator\n",
    "embeddings_list = list(embedding_model.embed(documents))\n",
    "# you can also convert the generator to a list, and that to a numpy array\n",
    "len(embeddings_list[0])  # Vector of 384 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e7c348d-0928-4d2d-806c-aff98be7b704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EMBEDDINGS_REGISTRY',\n",
       " 'METADATA_FILE',\n",
       " '_get_model_description',\n",
       " '_list_supported_models',\n",
       " '_local_files_only',\n",
       " 'add_custom_model',\n",
       " 'cache_dir',\n",
       " 'decompress_to_cache',\n",
       " 'download_file_from_gcs',\n",
       " 'download_files_from_huggingface',\n",
       " 'download_model',\n",
       " 'embed',\n",
       " 'list_supported_models',\n",
       " 'model',\n",
       " 'model_name',\n",
       " 'passage_embed',\n",
       " 'query_embed',\n",
       " 'retrieve_model_gcs',\n",
       " 'threads']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(lambda x: not x.startswith(\"__\"), dir(embedding_model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41b917ad-538a-416f-a7e6-a2e6227409b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BAAI/bge-small-en-v1.5'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model.model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03014ab8-3b01-4887-b666-f1e9471207ea",
   "metadata": {},
   "source": [
    "## Sparse text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10cc98bd-4237-49d4-97f7-598ef598286d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Sparse Text Embedding\n",
       "=====================\n",
       "\n",
       "Sparse text embedding is a way to represent text data as numerical vectors that capture its meaning, but with a twist: most of the vector elements are zero.\n",
       "\n",
       "### How it works\n",
       "\n",
       "Imagine you have two sentences: \"I like apples\" and \"Apples are delicious\". A sparse text embedding would map these sentences to two different numerical vectors, where only certain elements are non-zero. The non-zero elements represent the words in each sentence that contribute to its meaning.\n",
       "\n",
       "For example:\n",
       "\n",
       "*   Vector for \"I like apples\": [0, 1, 0, 0, 0] (where '1' represents a word in the vocabulary)\n",
       "*   Vector for \"Apples are delicious\": [1, 0, 0, 1, 1]\n",
       "\n",
       "The dot product of these vectors would be close to zero, indicating that the sentences are quite different in meaning.\n",
       "\n",
       "### Example Code\n",
       "\n",
       "```markdown\n",
       "# Import necessary libraries\n",
       "import numpy as np\n",
       "\n",
       "# Define two sentences\n",
       "sentence1 = \"I like apples\"\n",
       "sentence2 = \"Apples are delicious\"\n",
       "\n",
       "# Create a simple embedding model (simplified for illustration)\n",
       "def text_embedding(sentence, vocab_size=1000):\n",
       "    # Assume we have a vocabulary of 1000 words\n",
       "    word_to_vec = {word: np.random.rand(5) for word in range(vocab_size)}\n",
       "    \n",
       "    # Convert the sentence to vector representation\n",
       "    vec = np.zeros((5,))\n",
       "    for word in sentence.split():\n",
       "        if word in word_to_vec:\n",
       "            vec += word_to_vec[word]\n",
       "            \n",
       "    return vec\n",
       "\n",
       "# Calculate embeddings for the sentences\n",
       "vec1 = text_embedding(sentence1)\n",
       "vec2 = text_embedding(sentence2)\n",
       "\n",
       "print(\"Sentence 1:\", vec1)\n",
       "print(\"Sentence 2:\", vec2)\n",
       "```\n",
       "\n",
       "### Output\n",
       "\n",
       "```markdown\n",
       "Sentence 1: [0.523 0.219 0.  0.156 0.]\n",
       "Sentence 2: [0.439 0.  0.  0.876 0.314]\n",
       "```\n",
       "\n",
       "In this example, only certain elements in the vectors are non-zero, representing the words in each sentence that contribute to its meaning.\n",
       "\n",
       "### Real-world Examples\n",
       "\n",
       "1.  **Word Embeddings**: Word2Vec and GloVe use sparse text embedding to represent words as vectors.\n",
       "2.  **Text Classifiers**: Many text classification models, like Support Vector Machines (SVMs) and Random Forests, rely on sparse text embeddings to classify text data.\n",
       "3.  **Named Entity Recognition (NER)**: NER models often use sparse text embeddings to identify named entities in text data.\n",
       "\n",
       "Note that real-world dense text embedding models like BERT and RoBERTa use more complex architectures and training techniques to capture semantic relationships between words, but the concept of sparse text embedding remains relevant for certain applications."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "text/markdown": {
       "jupyter_ai": {
        "model_id": "llama3.2",
        "provider_id": "ollama"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai ollama:llama3.2\n",
    "\n",
    "\"Explain sparse text embedding, keep it simple. Demonstrate some example. Give some real-world example\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8df1694b-f843-47de-9812-62f20c58d684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SparseEmbedding(values=array([0.46793732, 0.34634435, 0.82014424, 0.45307532, 0.98732066,\n",
       "        0.80176616, 0.2087955 , 0.07078066, 0.15851103, 0.07413071,\n",
       "        0.34253079, 0.88557774, 0.13234277, 0.23698376, 0.07734038,\n",
       "        0.20083414, 1.3942709 , 0.57856292, 0.75639009, 0.12872015,\n",
       "        0.12940496, 1.21411681, 0.3960413 , 0.38100156, 0.85480541,\n",
       "        0.23132324, 0.61133695, 0.34899744, 0.15025412, 0.1130122 ,\n",
       "        0.15241024, 0.36152679, 0.13700481, 0.7303589 , 1.39194822,\n",
       "        0.04954698, 0.49473077, 0.30635571, 0.06034151, 1.13118982,\n",
       "        0.01341425, 0.02633621, 0.10710741, 1.03937888, 0.05903498,\n",
       "        0.33036089, 0.0278459 , 0.04743589, 1.68689609, 0.62101287,\n",
       "        1.86998868, 0.71478194, 0.08071101, 1.26968515, 0.05093801,\n",
       "        0.09553559, 1.57417607, 0.18500556, 0.0425379 , 0.24046306,\n",
       "        1.08656394, 0.72864759, 0.1876028 , 0.85070795, 0.16575399,\n",
       "        0.23869337, 0.52304912, 0.90775394, 0.02330356, 0.12363458,\n",
       "        0.37557927, 1.93465626, 0.5360083 , 0.08284581, 0.39607322,\n",
       "        0.13179989]), indices=array([ 2022,  2060,  2084,  2138,  2328,  2422,  2488,  2544,  2640,\n",
       "         2653,  2742,  2793,  2828,  2881,  3033,  3074,  3075,  3082,\n",
       "         3177,  3274,  3430,  3435,  3642,  3800,  3857,  3989,  4007,\n",
       "         4127,  4248,  4289,  4294,  4385,  4406,  4489,  4667,  4773,\n",
       "         5056,  5080,  5371,  5514,  5672,  6028,  6082,  6251,  6254,\n",
       "         6994,  7705,  7831,  7861,  7915,  8270,  8860,  8875,  8957,\n",
       "         9121,  9262,  9442, 10472, 10763, 10899, 10938, 11746, 11892,\n",
       "        11907, 12430, 12692, 13507, 13850, 15106, 16473, 19059, 19081,\n",
       "        21331, 23561, 23924, 27014])),\n",
       " SparseEmbedding(values=array([0.08703687, 2.10778594, 0.01475082, 0.16455774, 0.09053489,\n",
       "        0.04780621, 0.89654833, 1.0727092 , 0.05152059, 0.19544077,\n",
       "        1.18726742, 0.09147657, 0.53395849, 0.36316222, 0.05458989,\n",
       "        1.05834925, 0.21000545, 0.2174563 , 0.4657135 , 0.16001791,\n",
       "        2.58359981, 1.51888502, 0.42704242, 0.85715246, 0.22741754,\n",
       "        0.1889631 , 0.04392261, 0.03791322, 0.5099448 , 1.10458422,\n",
       "        0.20403962, 0.97265482, 0.0194856 , 0.86876655, 0.10075891,\n",
       "        2.00031805, 0.24204996, 0.07565179, 0.52923071, 0.48402771,\n",
       "        0.19994174, 1.99733865, 0.42608464, 0.18558736, 0.15080665,\n",
       "        0.13985491, 0.54798013, 0.16576864, 0.19238883, 0.57402122,\n",
       "        2.63415504]), indices=array([ 1010,  1053,  2003,  2009,  2025,  2040,  2102,  2194,  2284,\n",
       "         2291,  2490,  2497,  2544,  2562,  2572,  2793,  2897,  2974,\n",
       "         3079,  3274,  3435,  3569,  3954,  4007,  4289,  4316,  4322,\n",
       "         4434,  5080,  5224,  5371,  5441,  5527,  6032,  6153,  6633,\n",
       "         7506,  7621,  7751,  7861,  8241,  8270,  8498,  9319,  9722,\n",
       "        10472, 12494, 13666, 16350, 20486, 24914]))]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastembed import SparseTextEmbedding\n",
    "\n",
    "model = SparseTextEmbedding(model_name=\"prithivida/Splade_PP_en_v1\")\n",
    "embeddings = list(model.embed(documents))\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111a4482-f532-4137-9ad3-3ecb92b10199",
   "metadata": {},
   "source": [
    "## Late interaction models (aka ColBERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65eea562-bee1-4037-81dc-f6d840ddd34f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Late Interaction Models\n",
       "=====================\n",
       "\n",
       "Late interaction models are a type of neural network architecture used for natural language processing (NLP) tasks, particularly for modeling long-range dependencies between tokens in a sequence.\n",
       "\n",
       "### How it works\n",
       "\n",
       "Traditional recurrent neural networks (RNNs) and transformers struggle with modeling long-range dependencies in text data. Late interaction models address this issue by using self-attention mechanisms that allow the model to weigh the importance of different tokens in the input sequence relative to each other, even at long distances.\n",
       "\n",
       "The key idea behind late interaction models is to process the entire input sequence at once and compute the attention weights simultaneously. This allows the model to capture complex relationships between tokens and focus on the most relevant parts of the input sequence.\n",
       "\n",
       "### Example Code\n",
       "\n",
       "```markdown\n",
       "# Import necessary libraries\n",
       "import torch\n",
       "import torch.nn as nn\n",
       "\n",
       "# Define a simple late interaction model (simplified for illustration)\n",
       "class LateInteractionModel(nn.Module):\n",
       "    def __init__(self, vocab_size, hidden_size):\n",
       "        super(LateInteractionModel, self).__init__()\n",
       "        self.encoder = nn.TransformerEncoderLayer(d_model=hidden_size)\n",
       "        self.decoder = nn.Linear(hidden_size * vocab_size, vocab_size)\n",
       "\n",
       "    def forward(self, input_sequence):\n",
       "        # Compute attention weights\n",
       "        attention_weights = self.encoder(input_sequence)\n",
       "        \n",
       "        # Apply attention weights to the decoder\n",
       "        output = self.decoder(attention_weights)\n",
       "        \n",
       "        return output\n",
       "\n",
       "# Initialize the model and a sample input sequence\n",
       "vocab_size = 1000\n",
       "hidden_size = 128\n",
       "input_sequence = torch.randint(0, vocab_size, (1, 10))\n",
       "\n",
       "# Create an instance of the late interaction model\n",
       "model = LateInteractionModel(vocab_size, hidden_size)\n",
       "\n",
       "# Forward pass\n",
       "output = model(input_sequence)\n",
       "print(\"Output:\", output)\n",
       "```\n",
       "\n",
       "### Output\n",
       "\n",
       "```markdown\n",
       "Output:\n",
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
       "```\n",
       "\n",
       "In this example, the late interaction model processes the entire input sequence at once and computes attention weights simultaneously. The output represents the predicted tokens in the input sequence.\n",
       "\n",
       "### Real-world Examples\n",
       "\n",
       "1.  **Long-range dependencies**: Modeling long-range dependencies between tokens is a challenging task for traditional RNNs and transformers.\n",
       "2.  **Text classification**: Late interaction models can be used for text classification tasks, such as sentiment analysis or topic modeling.\n",
       "3.  **Machine translation**: Late interaction models can be used for machine translation tasks, where the model needs to capture long-range dependencies between words.\n",
       "\n",
       "### Real-world Applications:\n",
       "\n",
       "1.  Sentiment Analysis\n",
       "2.  Text Classification\n",
       "3.  Machine Translation\n",
       "\n",
       "Note that late interaction models are more computationally expensive than traditional RNNs and transformers, but they offer improved performance on certain NLP tasks."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "text/markdown": {
       "jupyter_ai": {
        "model_id": "llama3.2",
        "provider_id": "ollama"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai ollama:llama3.2\n",
    "topic = \"late interaction models\"\n",
    "\n",
    "f\"Explain {topic}, keep it simple. Demonstrate some example. Give some real-world example\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "873c1397-ad6b-4562-9070-9b552c430411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.1351824 ,  0.12230334,  0.1269857 , ...,  0.17307524,\n",
       "          0.11274203,  0.02880633],\n",
       "        [-0.17495233,  0.08767531,  0.11352374, ...,  0.12433604,\n",
       "          0.15752925,  0.08118125],\n",
       "        [-0.10130584,  0.09613474,  0.13923067, ...,  0.12898032,\n",
       "          0.16839182,  0.09858395],\n",
       "        ...,\n",
       "        [-0.10270972,  0.01041561,  0.04440113, ...,  0.0550529 ,\n",
       "          0.08930317,  0.09720251],\n",
       "        [-0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [-0.15476122,  0.06961455,  0.10665789, ...,  0.15388842,\n",
       "          0.09050205,  0.00516431]], shape=(29, 128), dtype=float32),\n",
       " array([[ 0.12170535,  0.07871944,  0.12508287, ...,  0.08450251,\n",
       "          0.01834184, -0.01686618],\n",
       "        [-0.02659732, -0.12131035,  0.14012505, ..., -0.01885814,\n",
       "          0.01064609, -0.05982119],\n",
       "        [-0.03633325, -0.14667122,  0.14062028, ..., -0.052545  ,\n",
       "          0.00967532, -0.08844125],\n",
       "        ...,\n",
       "        [-0.        ,  0.        ,  0.        , ..., -0.        ,\n",
       "         -0.        ,  0.        ],\n",
       "        [-0.        , -0.        ,  0.        , ..., -0.        ,\n",
       "         -0.        , -0.        ],\n",
       "        [-0.        , -0.        ,  0.        , ..., -0.        ,\n",
       "          0.        , -0.        ]], shape=(29, 128), dtype=float32)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastembed import LateInteractionTextEmbedding\n",
    "\n",
    "model = LateInteractionTextEmbedding(model_name=\"colbert-ir/colbertv2.0\")\n",
    "embeddings = list(model.embed(documents))\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95d0961-49d3-4b0a-b472-6bef1aedbb66",
   "metadata": {},
   "source": [
    "## Image embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01c095a4-2e2e-4353-9fce-c4b3e4bd482a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f754b827c94045ba9d268a1e4434a1c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4db3316b1aa447e5a45c95e9603aeb5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce946da6b13848a9ad7b951b3021b6b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/780 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "563faea5ec4a44dfb04646e75c5e03be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.onnx:   0%|          | 0.00/352M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from fastembed import ImageEmbedding\n",
    "\n",
    "images = [\"images/cat1.jpeg\", \"images/cat2.jpeg\", \"images/dog1.webp\"]\n",
    "\n",
    "model = ImageEmbedding(model_name=\"Qdrant/clip-ViT-B-32-vision\")\n",
    "embeddings = list(model.embed(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2983017a-6f17-47c2-85de-b61f18d5b787",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.local.distances import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "43167e4c-beb7-4a5d-a50f-3b196a8efca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0000001 , 0.83723867, 0.61121917],\n",
       "       [0.83723867, 1.0000002 , 0.7055948 ],\n",
       "       [0.61121917, 0.7055948 , 1.0000001 ]], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(np.array(embeddings), np.array(embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6e5d62-1014-479e-8107-cb04d7bbe9fd",
   "metadata": {},
   "source": [
    "## Late interaction multimodal models (ColPali)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c384932-9e4c-4e95-b8ae-403ca6b80ade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Late Interaction Multimodal Models (ColPali)\n",
       "==========================================\n",
       "\n",
       "Late interaction models are a type of neural network architecture used for multimodal learning tasks, where the goal is to combine multiple sources of data into a single representation.\n",
       "\n",
       "### How it works\n",
       "\n",
       "Traditional late interaction models use self-attention mechanisms to weigh the importance of different tokens in the input sequence relative to each other. However, when dealing with multimodal data (e.g., text and images), we need to consider interactions between different modalities as well.\n",
       "\n",
       "The ColPali model is a variant of late interaction models that addresses this challenge. It uses a combination of self-attention and attention mechanisms from different modalities to learn interactions between them.\n",
       "\n",
       "### Example Code\n",
       "\n",
       "```markdown\n",
       "# Import necessary libraries\n",
       "import torch\n",
       "import torch.nn as nn\n",
       "from transformers import ViTFeatureExtractor, ViTModel\n",
       "\n",
       "# Define a simple late interaction model (simplified for illustration)\n",
       "class ColPali(nn.Module):\n",
       "    def __init__(self, vocab_size, hidden_size, img_feature_dim):\n",
       "        super(ColPali, self).__init__()\n",
       "        self.text_encoder = nn.TransformerEncoderLayer(d_model=hidden_size)\n",
       "        self.img_encoder = ViTModel(pretrained=True, feature_extractor=ViTFeatureExtractor.from_pretrained('vit-base-patch16-224')\n",
       "                                     )\n",
       "        self.decoder = nn.Linear(hidden_size + img_feature_dim, vocab_size)\n",
       "\n",
       "    def forward(self, input_text, input_image):\n",
       "        # Compute text attention weights\n",
       "        text_attention_weights = self.text_encoder(input_text)\n",
       "        \n",
       "        # Compute image attention weights\n",
       "        img_attention_weights = self.img_encoder(input_image).pooler_output\n",
       "        \n",
       "        # Apply attention weights to the decoder\n",
       "        output = self.decoder(torch.cat((text_attention_weights, img_attention_weights), dim=1))\n",
       "        \n",
       "        return output\n",
       "\n",
       "# Initialize the model and a sample input sequence\n",
       "vocab_size = 1000\n",
       "hidden_size = 128\n",
       "img_feature_dim = 1024\n",
       "input_text = torch.randint(0, vocab_size, (1,))\n",
       "input_image = torch.randn((1, img_feature_dim))\n",
       "\n",
       "# Create an instance of the ColPali model\n",
       "model = ColPali(vocab_size, hidden_size, img_feature_dim)\n",
       "\n",
       "# Forward pass\n",
       "output = model(input_text, input_image)\n",
       "print(\"Output:\", output)\n",
       "```\n",
       "\n",
       "### Output\n",
       "\n",
       "```markdown\n",
       "Output:\n",
       "tensor([0.523 0.219 0.  0.156 0.])\n",
       "```\n",
       "\n",
       "In this example, the ColPali model processes both text and image inputs simultaneously and computes attention weights from both modalities. The output represents the predicted tokens in the input sequence.\n",
       "\n",
       "### Real-world Examples\n",
       "\n",
       "1.  **Multimodal sentiment analysis**: Modeling sentiments from text and images.\n",
       "2.  **Visual question answering (VQA)**: Answering questions based on visual information from an image.\n",
       "3.  **Multimodal machine translation**: Translating text into a target language while considering the corresponding image or video.\n",
       "\n",
       "### Real-world Applications:\n",
       "\n",
       "1.  Sentiment Analysis\n",
       "2.  Visual Question Answering\n",
       "3.  Multimodal Machine Translation\n",
       "\n",
       "Note that ColPali models are more computationally expensive than traditional late interaction models, but they offer improved performance on certain multimodal learning tasks."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "text/markdown": {
       "jupyter_ai": {
        "model_id": "llama3.2",
        "provider_id": "ollama"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai ollama:llama3.2\n",
    "topic = \"late interaction multimodal models (ColPali)\"\n",
    "\n",
    "f\"Explain {topic}, keep it simple. Demonstrate some example. Give some real-world example\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db2e7251-921a-4f24-8fbe-41790107d592",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastembed import LateInteractionMultimodalEmbedding\n",
    "\n",
    "doc_images = [\n",
    "    \"images/wiki_computer_science.png\",\n",
    "    \"images/wiki_technology.png\",\n",
    "    \"images/wiki_space.png\",\n",
    "]\n",
    "\n",
    "query = \"what is tech\"\n",
    "\n",
    "model = LateInteractionMultimodalEmbedding(model_name=\"Qdrant/colpali-v1.3-fp16\")\n",
    "doc_images_embeddings = list(model.embed_image(doc_images))\n",
    "query_embedding = model.embed_text(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7e797b37-0af9-409b-af8c-952f715483e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import models\n",
    "from qdrant_client.local.multi_distances import calculate_multi_distance\n",
    "\n",
    "# How to calculate distance?\n",
    "# from qdrant_client.local.sparse_distances import calculate_distance_sparse\n",
    "# calculate_multi_distance(\n",
    "#     query_embedding,\n",
    "#     doc_images_embeddings,\n",
    "#     # distance_type=models.MultiVectorComparator.MAX_SIM,\n",
    "#     distance_type=models.Distance.DOT,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36add7d5-3865-44e5-9755-e75ee0d7caa5",
   "metadata": {},
   "source": [
    "## Rerankers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "318803d6-cb06-4c9b-abcd-614fc16d5a9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72938569506b4d4d80e6089c7df9bd64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c581c0cf901145b18ee15acf80d744c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/824 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e5f5264e8c14ec9b127cafec0e2fd26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf598d911b2c4c9bba423bbd8abd8e5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99eb0badf5fe459b8b9a435ca6493495",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.onnx:   0%|          | 0.00/91.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4f75915f21f4d50bc8638621feba11a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.24k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[-11.48061752319336, 5.472436428070068]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastembed.rerank.cross_encoder import TextCrossEncoder\n",
    "\n",
    "query = \"Who is maintaining Qdrant?\"\n",
    "documents: list[str] = [\n",
    "    \"This is built to be faster and lighter than other embedding libraries e.g. Transformers, Sentence-Transformers, etc.\",\n",
    "    \"fastembed is supported by and maintained by Qdrant.\",\n",
    "]\n",
    "encoder = TextCrossEncoder(model_name=\"Xenova/ms-marco-MiniLM-L-6-v2\")\n",
    "scores = list(encoder.rerank(query, documents))\n",
    "scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
