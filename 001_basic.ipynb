{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42e1779f-d5db-4d37-bc63-64b220c9c74a",
   "metadata": {},
   "source": [
    "# Text Embedding\n",
    "\n",
    "\n",
    "## Goal\n",
    "\n",
    "In this notebook, we will learn how to generate text embeddings using `fastembed` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b01e35c0-c3f4-4f30-808c-fd17513884cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The jupyter_ai extension is already loaded. To reload it, use:\n",
      "  %reload_ext jupyter_ai\n"
     ]
    }
   ],
   "source": [
    "%load_ext jupyter_ai\n",
    "%config AiMagics.default_language_model = \"ollama:llama3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e076cf60-5ae7-4d65-b490-4a2d55a69889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**What is Text Embedding?**\n",
       "==========================\n",
       "\n",
       "Text embedding is a technique to represent words or phrases as numerical vectors, called embeddings, that capture their semantic meaning.\n",
       "\n",
       "**Key Features:**\n",
       "\n",
       "*   **Numerical Representation**: Words are converted into dense, high-dimensional vectors.\n",
       "*   **Similarity Measures**: Distance between word vectors can be used to measure semantic similarity.\n",
       "*   **Contextual Understanding**: Embeddings can capture relationships between words in a sentence or document.\n",
       "\n",
       "**Example:**\n",
       "Word \"dog\" might be represented as an embedding that is closer to the word \"cat\" than \"house\", even though they are not semantically similar."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "text/markdown": {
       "jupyter_ai": {
        "model_id": "llama3.2",
        "provider_id": "ollama"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai\n",
    "\n",
    "\"what is text embedding? keep it short and simple\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b8f7835-423e-48d8-bbb2-b02649437796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Create a wordcloud for the topic text embedding\"\n",
    "# \"Create a simple syllabus for learning the following topic text embedding\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e36dece-741e-41f2-8a87-8a1b364eda93",
   "metadata": {},
   "source": [
    "## Dense text embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86c464a1-6e9f-4019-b720-d4cde326158e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## What are Dense Text Embeddings?\n",
       "\n",
       "Dense text embeddings are a way to convert text into numerical representations that capture its meaning and context.\n",
       "\n",
       "### How Do They Work?\n",
       "\n",
       "1. **Vectorize Words:** Turn words into vectors (numbers) that represent their meaning.\n",
       "2. **Learned Through Data:** The model learns from a massive amount of text data and generates these vectors based on the relationships between words.\n",
       "\n",
       "### Example:\n",
       "\n",
       "*   **“Dog”** -> [0.8, 0.2, 0.9]\n",
       "*   **“Cat”** -> [0.7, 0.3, 0.8]\n",
       "*   **“Car”** -> [0.1, 0.9, 0.2]\n",
       "*   **“Apple”** -> [0.3, 0.5, 0.7]\n",
       "\n",
       "Notice how similar words are closer together.\n",
       "\n",
       "### Real-World Examples:\n",
       "\n",
       "1.  **Search Engines:** Convert search queries into embeddings to find relevant documents.\n",
       "2.  **Recommendation Systems:** Use embeddings to suggest movies or songs based on user preferences.\n",
       "3.  **Sentiment Analysis:** Analyze text to determine its sentiment (positive, negative, neutral).\n",
       "4.  **Chatbots & Conversational AI:** Embeddings help chatbots understand user input and respond accordingly.\n",
       "\n",
       "### Key Benefits:\n",
       "\n",
       "*   Capture complex relationships between words\n",
       "*   Efficiently represent large amounts of text data\n",
       "*   Enable applications like search, recommendation, and sentiment analysis"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "text/markdown": {
       "jupyter_ai": {
        "model_id": "llama3.2",
        "provider_id": "ollama"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai\n",
    "\n",
    "\"Explain dense text embedding, keep it simple. Demonstrate some example. Give some real-world example\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e10325a-25ff-49d7-9039-0860c25d1255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model BAAI/bge-small-en-v1.5 is ready to use.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastembed import TextEmbedding\n",
    "\n",
    "# Example list of documents\n",
    "documents: list[str] = [\n",
    "    \"This is built to be faster and lighter than other embedding libraries e.g. Transformers, Sentence-Transformers, etc.\",\n",
    "    \"fastembed is supported by and maintained by Qdrant.\",\n",
    "]\n",
    "\n",
    "# This will trigger the model download and initialization\n",
    "embedding_model = TextEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "print(f\"The model {embedding_model.model_name} is ready to use.\")\n",
    "\n",
    "embeddings_generator = embedding_model.embed(documents)  # reminder this is a generator\n",
    "embeddings_list = list(embedding_model.embed(documents))\n",
    "# you can also convert the generator to a list, and that to a numpy array\n",
    "len(embeddings_list[0])  # Vector of 384 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e7c348d-0928-4d2d-806c-aff98be7b704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EMBEDDINGS_REGISTRY',\n",
       " 'METADATA_FILE',\n",
       " '_get_model_description',\n",
       " '_list_supported_models',\n",
       " '_local_files_only',\n",
       " 'add_custom_model',\n",
       " 'cache_dir',\n",
       " 'decompress_to_cache',\n",
       " 'download_file_from_gcs',\n",
       " 'download_files_from_huggingface',\n",
       " 'download_model',\n",
       " 'embed',\n",
       " 'list_supported_models',\n",
       " 'model',\n",
       " 'model_name',\n",
       " 'passage_embed',\n",
       " 'query_embed',\n",
       " 'retrieve_model_gcs',\n",
       " 'threads']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(lambda x: not x.startswith(\"__\"), dir(embedding_model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41b917ad-538a-416f-a7e6-a2e6227409b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BAAI/bge-small-en-v1.5'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model.model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03014ab8-3b01-4887-b666-f1e9471207ea",
   "metadata": {},
   "source": [
    "## Sparse text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10cc98bd-4237-49d4-97f7-598ef598286d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Sparse Text Embeddings**\n",
       "========================\n",
       "\n",
       "### What is a Sparse Vector?\n",
       "\n",
       "A sparse vector is a numerical representation of a piece of text where most values are 0.\n",
       "\n",
       "### How Does it Work?\n",
       "\n",
       "Imagine you're looking at a word and thinking of related words, like \"apple\" and \"fruit\". A sparse vector would have a few non-zero values representing the relationships between these words, but most values would be 0 (representing unrelated words).\n",
       "\n",
       "### Example\n",
       "--------\n",
       "\n",
       "Let's say we have a piece of text: \"The quick brown fox jumps over the lazy dog.\"\n",
       "\n",
       "We can represent this as two sparse vectors:\n",
       "\n",
       "* Vector A: `[0.1, 0, 0, ...]` (related to the word \"quick\")\n",
       "* Vector B: `[0, 0.4, 0, ...]` (related to the word \"brown\")\n",
       "\n",
       "Our sparse vector would have only non-zero values representing the relationships between these two words.\n",
       "\n",
       "### Real-World Examples\n",
       "--------------------\n",
       "\n",
       "1. **Text Classification**: Sparse embeddings can be used as input features for text classification models, such as sentiment analysis or spam detection.\n",
       "2. **Topic Modeling**: Sparse embeddings can help identify topics in large collections of text by capturing the underlying relationships between words.\n",
       "3. **Word Embeddings**: Sparse embeddings can be used to represent individual words, like Word2Vec or GloVe.\n",
       "\n",
       "### Code Example\n",
       "--------------\n",
       "\n",
       "```python\n",
       "# Import necessary libraries\n",
       "import numpy as np\n",
       "\n",
       "# Define a function to create a simple sparse vector representation\n",
       "def create_sparse_vector(text):\n",
       "    # Split the text into words\n",
       "    words = text.split()\n",
       "\n",
       "    # Create a dictionary to store word vectors\n",
       "    word_vectors = {}\n",
       "\n",
       "    # Add each word to the dictionary\n",
       "    for i, word in enumerate(words):\n",
       "        # Use a random numerical value as the vector for simplicity\n",
       "        word_vector = np.random.rand(768)\n",
       "        word_vector[i] = 1.0  # Set the correct value\n",
       "        word_vectors[word] = word_vector\n",
       "\n",
       "    # Combine all word vectors into a single sparse vector\n",
       "    sparse_vector = np.array([word_vectors[word][i] for i, word in enumerate(words)])\n",
       "\n",
       "    return sparse_vector\n",
       "\n",
       "# Test the function\n",
       "text = \"The quick brown fox jumps over the lazy dog.\"\n",
       "sparse_vector = create_sparse_vector(text)\n",
       "print(sparse_vector.shape)  # Output: (1, 768)\n",
       "```\n",
       "\n",
       "In this example, we use a simple dictionary to store word vectors and then combine them into a single sparse vector. The `i`-th value in each word vector is set to 1.0 to represent the correct relationship between words."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "text/markdown": {
       "jupyter_ai": {
        "model_id": "llama3.2",
        "provider_id": "ollama"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai\n",
    "\n",
    "\"Explain sparse text embedding, keep it simple. Demonstrate some example. Give some real-world example\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8df1694b-f843-47de-9812-62f20c58d684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SparseEmbedding(values=array([0.46793732, 0.34634435, 0.82014424, 0.45307532, 0.98732066,\n",
       "        0.80176616, 0.2087955 , 0.07078066, 0.15851103, 0.07413071,\n",
       "        0.34253079, 0.88557774, 0.13234277, 0.23698376, 0.07734038,\n",
       "        0.20083414, 1.3942709 , 0.57856292, 0.75639009, 0.12872015,\n",
       "        0.12940496, 1.21411681, 0.3960413 , 0.38100156, 0.85480541,\n",
       "        0.23132324, 0.61133695, 0.34899744, 0.15025412, 0.1130122 ,\n",
       "        0.15241024, 0.36152679, 0.13700481, 0.7303589 , 1.39194822,\n",
       "        0.04954698, 0.49473077, 0.30635571, 0.06034151, 1.13118982,\n",
       "        0.01341425, 0.02633621, 0.10710741, 1.03937888, 0.05903498,\n",
       "        0.33036089, 0.0278459 , 0.04743589, 1.68689609, 0.62101287,\n",
       "        1.86998868, 0.71478194, 0.08071101, 1.26968515, 0.05093801,\n",
       "        0.09553559, 1.57417607, 0.18500556, 0.0425379 , 0.24046306,\n",
       "        1.08656394, 0.72864759, 0.1876028 , 0.85070795, 0.16575399,\n",
       "        0.23869337, 0.52304912, 0.90775394, 0.02330356, 0.12363458,\n",
       "        0.37557927, 1.93465626, 0.5360083 , 0.08284581, 0.39607322,\n",
       "        0.13179989]), indices=array([ 2022,  2060,  2084,  2138,  2328,  2422,  2488,  2544,  2640,\n",
       "         2653,  2742,  2793,  2828,  2881,  3033,  3074,  3075,  3082,\n",
       "         3177,  3274,  3430,  3435,  3642,  3800,  3857,  3989,  4007,\n",
       "         4127,  4248,  4289,  4294,  4385,  4406,  4489,  4667,  4773,\n",
       "         5056,  5080,  5371,  5514,  5672,  6028,  6082,  6251,  6254,\n",
       "         6994,  7705,  7831,  7861,  7915,  8270,  8860,  8875,  8957,\n",
       "         9121,  9262,  9442, 10472, 10763, 10899, 10938, 11746, 11892,\n",
       "        11907, 12430, 12692, 13507, 13850, 15106, 16473, 19059, 19081,\n",
       "        21331, 23561, 23924, 27014])),\n",
       " SparseEmbedding(values=array([0.08703687, 2.10778594, 0.01475082, 0.16455774, 0.09053489,\n",
       "        0.04780621, 0.89654833, 1.0727092 , 0.05152059, 0.19544077,\n",
       "        1.18726742, 0.09147657, 0.53395849, 0.36316222, 0.05458989,\n",
       "        1.05834925, 0.21000545, 0.2174563 , 0.4657135 , 0.16001791,\n",
       "        2.58359981, 1.51888502, 0.42704242, 0.85715246, 0.22741754,\n",
       "        0.1889631 , 0.04392261, 0.03791322, 0.5099448 , 1.10458422,\n",
       "        0.20403962, 0.97265482, 0.0194856 , 0.86876655, 0.10075891,\n",
       "        2.00031805, 0.24204996, 0.07565179, 0.52923071, 0.48402771,\n",
       "        0.19994174, 1.99733865, 0.42608464, 0.18558736, 0.15080665,\n",
       "        0.13985491, 0.54798013, 0.16576864, 0.19238883, 0.57402122,\n",
       "        2.63415504]), indices=array([ 1010,  1053,  2003,  2009,  2025,  2040,  2102,  2194,  2284,\n",
       "         2291,  2490,  2497,  2544,  2562,  2572,  2793,  2897,  2974,\n",
       "         3079,  3274,  3435,  3569,  3954,  4007,  4289,  4316,  4322,\n",
       "         4434,  5080,  5224,  5371,  5441,  5527,  6032,  6153,  6633,\n",
       "         7506,  7621,  7751,  7861,  8241,  8270,  8498,  9319,  9722,\n",
       "        10472, 12494, 13666, 16350, 20486, 24914]))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastembed import SparseTextEmbedding\n",
    "\n",
    "model = SparseTextEmbedding(model_name=\"prithivida/Splade_PP_en_v1\")\n",
    "embeddings = list(model.embed(documents))\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111a4482-f532-4137-9ad3-3ecb92b10199",
   "metadata": {},
   "source": [
    "## Late interaction models (aka ColBERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "65eea562-bee1-4037-81dc-f6d840ddd34f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Late Interaction Models (ColBERT)**\n",
       "=====================================\n",
       "\n",
       "### What is Late Interaction?\n",
       "\n",
       "Late interaction models are a type of search model that improves the performance of early interaction methods, such as BM25.\n",
       "\n",
       "### How Does it Work?\n",
       "\n",
       "Imagine you're searching for documents related to a query. Early interaction methods like BM25 use features extracted from the query and the document to rank them. Late interaction models refine these rankings by using additional information, such as the entire document or other relevant documents.\n",
       "\n",
       "### Example\n",
       "--------\n",
       "\n",
       "Let's say we have a search model that uses BM25 to rank documents for a query \"computer science\". The top 5 ranked documents are:\n",
       "\n",
       "1. **A paper on computer vision**\n",
       "2. **A blog post on machine learning**\n",
       "3. **A Wikipedia page on artificial intelligence**\n",
       "4. **A news article on data science**\n",
       "5. **A book review on software engineering**\n",
       "\n",
       "Late interaction models like ColBERT can improve these rankings by considering additional features, such as:\n",
       "\n",
       "* The entire document text\n",
       "* Other relevant documents in the corpus\n",
       "\n",
       "### Real-World Examples\n",
       "--------------------\n",
       "\n",
       "1. **E-commerce Search**: Late interaction models can improve search results for e-commerce platforms by incorporating product descriptions, reviews, and other relevant information.\n",
       "2. **Knowledge Graph Search**: Late interaction models can be used to search knowledge graphs, which are databases of entities and their relationships.\n",
       "3. **Recommendation Systems**: Late interaction models can be used to recommend items based on user behavior and preferences.\n",
       "\n",
       "### Code Example\n",
       "--------------\n",
       "\n",
       "```python\n",
       "# Import necessary libraries\n",
       "import numpy as np\n",
       "\n",
       "# Define a function to create a ColBERT model\n",
       "def create_colbert_model(query, documents):\n",
       "    # Extract features from the query and documents using BM25\n",
       "    bm25_features = bm25_query_and_document(query, documents)\n",
       "\n",
       "    # Add additional features using late interaction\n",
       "    colbert_features = add_late_interaction(bm25_features, query, documents)\n",
       "\n",
       "    return colbert_features\n",
       "\n",
       "# Define a function to calculate BM25 features\n",
       "def bm25_query_and_document(query, documents):\n",
       "    # Implement BM25 calculation here\n",
       "    pass\n",
       "\n",
       "# Define a function to add late interaction features\n",
       "def add_late_interaction(features, query, documents):\n",
       "    # Implement late interaction calculation here\n",
       "    pass\n",
       "```\n",
       "\n",
       "In this example, we define a simple ColBERT model that uses BM25 as an early interaction method and adds additional features using late interaction. The `bm25_query_and_document` function calculates BM25 features, and the `add_late_interaction` function calculates late interaction features."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "text/markdown": {
       "jupyter_ai": {
        "model_id": "llama3.2",
        "provider_id": "ollama"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai ollama:llama3.2\n",
    "\n",
    "\"Explain late interaction models (aka ColBERT), keep it simple. Demonstrate some example. Give some real-world example\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "873c1397-ad6b-4562-9070-9b552c430411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.1351824 ,  0.12230334,  0.1269857 , ...,  0.17307524,\n",
       "          0.11274203,  0.02880633],\n",
       "        [-0.17495233,  0.08767531,  0.11352374, ...,  0.12433604,\n",
       "          0.15752925,  0.08118125],\n",
       "        [-0.10130584,  0.09613474,  0.13923067, ...,  0.12898032,\n",
       "          0.16839182,  0.09858395],\n",
       "        ...,\n",
       "        [-0.10270972,  0.01041561,  0.04440113, ...,  0.0550529 ,\n",
       "          0.08930317,  0.09720251],\n",
       "        [-0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [-0.15476122,  0.06961455,  0.10665789, ...,  0.15388842,\n",
       "          0.09050205,  0.00516431]], shape=(29, 128), dtype=float32),\n",
       " array([[ 0.12170535,  0.07871944,  0.12508287, ...,  0.08450251,\n",
       "          0.01834184, -0.01686618],\n",
       "        [-0.02659732, -0.12131035,  0.14012505, ..., -0.01885814,\n",
       "          0.01064609, -0.05982119],\n",
       "        [-0.03633325, -0.14667122,  0.14062028, ..., -0.052545  ,\n",
       "          0.00967532, -0.08844125],\n",
       "        ...,\n",
       "        [-0.        ,  0.        ,  0.        , ..., -0.        ,\n",
       "         -0.        ,  0.        ],\n",
       "        [-0.        , -0.        ,  0.        , ..., -0.        ,\n",
       "         -0.        , -0.        ],\n",
       "        [-0.        , -0.        ,  0.        , ..., -0.        ,\n",
       "          0.        , -0.        ]], shape=(29, 128), dtype=float32)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastembed import LateInteractionTextEmbedding\n",
    "\n",
    "model = LateInteractionTextEmbedding(model_name=\"colbert-ir/colbertv2.0\")\n",
    "embeddings = list(model.embed(documents))\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95d0961-49d3-4b0a-b472-6bef1aedbb66",
   "metadata": {},
   "source": [
    "## Image embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "01c095a4-2e2e-4353-9fce-c4b3e4bd482a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from fastembed import ImageEmbedding\n",
    "\n",
    "images = [\"images/cat1.jpeg\", \"images/cat2.jpeg\", \"images/dog1.webp\"]\n",
    "\n",
    "model = ImageEmbedding(model_name=\"Qdrant/clip-ViT-B-32-vision\")\n",
    "embeddings = list(model.embed(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2983017a-6f17-47c2-85de-b61f18d5b787",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.local.distances import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "43167e4c-beb7-4a5d-a50f-3b196a8efca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0000001 , 0.83723867, 0.61121917],\n",
       "       [0.83723867, 1.0000002 , 0.7055948 ],\n",
       "       [0.61121917, 0.7055948 , 1.0000001 ]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(np.array(embeddings), np.array(embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6e5d62-1014-479e-8107-cb04d7bbe9fd",
   "metadata": {},
   "source": [
    "## Late interaction multimodal models (ColPali)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c384932-9e4c-4e95-b8ae-403ca6b80ade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Late Interaction Multimodal Models (ColPali)**\n",
       "=============================================\n",
       "\n",
       "### What is Late Interaction?\n",
       "\n",
       "Late interaction models are a type of search model that improves the performance of early interaction methods, such as BM25.\n",
       "\n",
       "### How Does it Work?\n",
       "\n",
       "Imagine you're searching for documents related to a query. Early interaction methods like BM25 use features extracted from the query and the document to rank them. Late interaction models refine these rankings by using additional information, such as the entire document or other relevant documents.\n",
       "\n",
       "### Multimodal Models\n",
       "\n",
       "Multimodal models extend late interaction models to incorporate multiple input modalities, such as text, images, audio, and video.\n",
       "\n",
       "### Example\n",
       "--------\n",
       "\n",
       "Let's say we have a search model that uses ColPali to rank documents for a query \"computer science\". The input modalities are:\n",
       "\n",
       "* **Text**: A passage of text related to the query.\n",
       "* **Image**: An image related to the query (e.g. a diagram of a computer chip).\n",
       "* **Audio**: An audio file related to the query (e.g. a lecture on machine learning).\n",
       "\n",
       "The ColPali model extracts features from each modality and combines them using late interaction. The final ranking is based on the weighted sum of these features.\n",
       "\n",
       "### Real-World Examples\n",
       "--------------------\n",
       "\n",
       "1. **Image Search with Text Description**: Late interaction multimodal models can be used to search images based on a text description.\n",
       "2. **Speech Recognition with Visual Feedback**: ColPali can be used in speech recognition systems that incorporate visual feedback, such as displaying the recognized text in real-time.\n",
       "3. **Multimodal Question Answering**: Late interaction multimodal models can be used to answer questions that require information from multiple input modalities (e.g. text, images, audio).\n",
       "\n",
       "### Code Example\n",
       "--------------\n",
       "\n",
       "```python\n",
       "# Import necessary libraries\n",
       "import numpy as np\n",
       "\n",
       "# Define a function to create a ColPali model\n",
       "def create_colpali_model(query, text, image, audio):\n",
       "    # Extract features from each modality using late interaction\n",
       "    text_features = extract_features(text, query)\n",
       "    image_features = extract_features(image, query)\n",
       "    audio_features = extract_features(audio, query)\n",
       "\n",
       "    # Combine features using weighted sum\n",
       "    combined_features = combine_features(text_features, image_features, audio_features)\n",
       "\n",
       "    return combined_features\n",
       "\n",
       "# Define a function to extract features from text using late interaction\n",
       "def extract_features(text, query):\n",
       "    # Implement late interaction calculation here\n",
       "    pass\n",
       "\n",
       "# Define a function to combine features using weighted sum\n",
       "def combine_features(text_features, image_features, audio_features):\n",
       "    # Implement weighted sum calculation here\n",
       "    pass\n",
       "```\n",
       "\n",
       "In this example, we define a simple ColPali model that uses late interaction to extract features from each modality and combines them using a weighted sum. The `extract_features` function calculates features for each modality, and the `combine_features` function calculates the final combined features."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "text/markdown": {
       "jupyter_ai": {
        "model_id": "llama3.2",
        "provider_id": "ollama"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai\n",
    "\n",
    "\"Explain late interaction multimodal models (aka ColPali), keep it simple. Demonstrate some example. Give some real-world example\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2e7251-921a-4f24-8fbe-41790107d592",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastembed import LateInteractionMultimodalEmbedding\n",
    "\n",
    "doc_images = [\n",
    "    \"images/wiki_computer_science.png\",\n",
    "    \"images/wiki_technology.png\",\n",
    "    \"images/wiki_space.png\",\n",
    "]\n",
    "\n",
    "query = \"what is tech\"\n",
    "\n",
    "model = LateInteractionMultimodalEmbedding(model_name=\"Qdrant/colpali-v1.3-fp16\")\n",
    "doc_images_embeddings = list(model.embed_image(doc_images))\n",
    "query_embedding = model.embed_text(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e797b37-0af9-409b-af8c-952f715483e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import models\n",
    "from qdrant_client.local.multi_distances import calculate_multi_distance\n",
    "\n",
    "# How to calculate distance?\n",
    "# from qdrant_client.local.sparse_distances import calculate_distance_sparse\n",
    "# calculate_multi_distance(\n",
    "#     query_embedding,\n",
    "#     doc_images_embeddings,\n",
    "#     # distance_type=models.MultiVectorComparator.MAX_SIM,\n",
    "#     distance_type=models.Distance.DOT,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36add7d5-3865-44e5-9755-e75ee0d7caa5",
   "metadata": {},
   "source": [
    "## Rerankers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318803d6-cb06-4c9b-abcd-614fc16d5a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastembed.rerank.cross_encoder import TextCrossEncoder\n",
    "\n",
    "query = \"Who is maintaining Qdrant?\"\n",
    "documents: list[str] = [\n",
    "    \"This is built to be faster and lighter than other embedding libraries e.g. Transformers, Sentence-Transformers, etc.\",\n",
    "    \"fastembed is supported by and maintained by Qdrant.\",\n",
    "]\n",
    "encoder = TextCrossEncoder(model_name=\"Xenova/ms-marco-MiniLM-L-6-v2\")\n",
    "scores = list(encoder.rerank(query, documents))\n",
    "scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
