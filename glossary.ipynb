{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f897234c-9f93-4f88-9c7b-7b34fb568bd2",
   "metadata": {},
   "source": [
    "# Glossary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7422dbcb-111b-4a16-8e99-c16dbc5c0755",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext jupyter_ai\n",
    "%config AiMagics.default_language_model = \"ollama:llama3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93c76d0c-0e4c-4db1-8f8d-7211d5b97b29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Text Embedding\n",
       "=====================================\n",
       "\n",
       "Text embedding is a technique used to represent text data as numerical vectors, allowing for efficient and effective processing of natural language inputs.\n",
       "\n",
       "### What's the Problem?\n",
       "\n",
       "Traditional approaches to text analysis involve converting text into numerical representations using techniques such as bag-of-words or term-frequency, inverse-document-frequency (TF-IDF). However, these methods have limitations:\n",
       "\n",
       "* High dimensionality: Text data has a vast number of features (words), resulting in high-dimensional vectors that are computationally expensive to process and store.\n",
       "* Lack of semantic meaning: These representations do not capture the underlying semantic relationships between words.\n",
       "\n",
       "### Solution: Embeddings\n",
       "\n",
       "Text embeddings aim to learn a lower-dimensional, dense representation of text data that captures its semantic meaning. This is achieved through complex machine learning models, such as:\n",
       "\n",
       "* Word2Vec\n",
       "* GloVe\n",
       "* BERT (Bidirectional Encoder Representations from Transformers)\n",
       "\n",
       "These models generate vector representations for words or phrases in the language, which can be used for various NLP tasks, including:\n",
       "\n",
       "* Text classification\n",
       "* Sentiment analysis\n",
       "* Information retrieval\n",
       "\n",
       "### Types of Embeddings\n",
       "\n",
       "There are two main types of embeddings:\n",
       "\n",
       "* **Fixed-length embeddings**: These are obtained using word2vec or glove models and have a fixed vector length.\n",
       "* **Context-dependent embeddings**: These are generated by BERT and other transformer-based models, which consider the context in which a word is used to generate its representation.\n",
       "\n",
       "### Benefits\n",
       "\n",
       "Text embeddings offer several benefits:\n",
       "\n",
       "* Efficient processing: Embeddings can be processed using efficient algorithms and data structures, making them suitable for large-scale applications.\n",
       "* Improved performance: Embeddings capture semantic relationships between words, leading to improved performance on NLP tasks.\n",
       "* Flexibility: Embeddings can be used as input features for machine learning models or as a standalone representation."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "text/markdown": {
       "jupyter_ai": {
        "model_id": "llama3.2",
        "provider_id": "ollama"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai\n",
    "\n",
    "\"what is text embedding\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0a88b62-d136-4ab9-a334-429e751aaaa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Sparse vs Dense Text Embeddings\n",
       "=====================================\n",
       "\n",
       "Text embeddings are representations of text data as numerical vectors. The main difference between sparse and dense embeddings lies in their dimensionality.\n",
       "\n",
       "### Dense Text Embeddings\n",
       "\n",
       "* **Fixed**: Every word or phrase has a fixed, non-zero value in the embedding vector.\n",
       "* **Higher-dimensional**: Typically have a higher number of dimensions to capture more nuanced semantic relationships.\n",
       "* **More computationally expensive**: Require more memory and computational resources to store and process.\n",
       "\n",
       "Example:\n",
       "```markdown\n",
       " Dense Embedding: [0.1, 0.2, 0.3, 0.4, ...]\n",
       "```\n",
       "### Sparse Text Embeddings\n",
       "\n",
       "* **Variable**: Only non-zero values are stored in the embedding vector.\n",
       "* **Lower-dimensional**: Have fewer dimensions compared to dense embeddings, reducing computational costs and storage requirements.\n",
       "* **Less accurate for complex relationships**: May not capture as nuanced semantic relationships due to reduced dimensionality.\n",
       "\n",
       "Example:\n",
       "```markdown\n",
       " Sparse Embedding: [0.1, 0.2]\n",
       "```\n",
       "In summary, sparse text embeddings are more efficient in terms of memory and computation but may sacrifice some accuracy in capturing complex semantic relationships."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "text/markdown": {
       "jupyter_ai": {
        "model_id": "llama3.2",
        "provider_id": "ollama"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai\n",
    "\n",
    "\"how is sparse text embedding different from dense text embedding? keep it simple\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35216eb1-b8a9-46f3-9d1f-3e369f1848be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Sparse Text Embedding vs BM25 Search\n",
       "=====================================\n",
       "\n",
       "BM25 (Bayesian Multiplicative Weighted) is a widely used algorithm for information retrieval and text search, while sparse text embeddings represent text data as numerical vectors.\n",
       "\n",
       "### Key Differences\n",
       "\n",
       "* **Purpose**:\n",
       "\t+ BM25: Designed for ranking and retrieving documents based on relevance to a query.\n",
       "\t+ Sparse Text Embeddings: Used for NLP tasks such as text classification, sentiment analysis, and information retrieval.\n",
       "* **Dimensionality**:\n",
       "\t+ BM25: Treats each word as a feature, resulting in a high-dimensional vector space.\n",
       "\t+ Sparse Text Embeddings: Typically have a much lower dimensionality (e.g., 128-512) compared to the number of unique words in a corpus.\n",
       "* **Weighting and Scoring**:\n",
       "\t+ BM25: Assigns weights to each word based on its importance, relevance, and rarity in the document.\n",
       "\t+ Sparse Text Embeddings: Uses learned representations (e.g., word2vec) to capture semantic meaning, often without explicit weighting or scoring.\n",
       "* **Query Representation**:\n",
       "\t+ BM25: Typically represents a query as a bag-of-words, using term frequencies and inverse documents frequencies.\n",
       "\t+ Sparse Text Embeddings: Often uses a dense representation of the query (e.g., dense vector) to compute similarity scores.\n",
       "\n",
       "### Example Workflow\n",
       "\n",
       "BM25 Search:\n",
       "\n",
       "1. Preprocess text data (tokenization, stemming, etc.)\n",
       "2. Compute BM25 weights for each word in a document\n",
       "3. Rank documents based on their relevance to the query\n",
       "\n",
       "Sparse Text Embeddings:\n",
       "\n",
       "1. Preprocess text data (tokenization, stemming, etc.)\n",
       "2. Learn dense vector representations for words using techniques like word2vec\n",
       "3. Use these vectors as input features for NLP tasks or compute similarity scores"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "text/markdown": {
       "jupyter_ai": {
        "model_id": "llama3.2",
        "provider_id": "ollama"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai\n",
    "\n",
    "\"how is sparse text embedding different from bm25 search?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c95514d-1783-46e4-a2d3-468cd7b5ab6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
