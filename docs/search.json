[
  {
    "objectID": "003_fastembed_text_embeddings.html",
    "href": "003_fastembed_text_embeddings.html",
    "title": "How to generate Text Embeddings with FastEmbed",
    "section": "",
    "text": "from fastembed import TextEmbedding\ndocuments: [str] = [\n    \"FastEmbed is lighter than Transformers & Sentence-Transformers.\",\n    \"FastEmbed is supported by and maintained by Qdrant.\",\n    \"I have a cat\",\n]\nembedding_model = TextEmbedding()\nembedding_model.model_name\n\n'BAAI/bge-small-en-v1.5'\nembeddings_generator = embedding_model.embed(documents)\nembeddings_list = list(embeddings_generator)\nembeddings_list[0][:10]\n\narray([-0.09479211,  0.01008398, -0.03087804,  0.02379127,  0.00236447,\n        0.00065356, -0.08248352,  0.00084713,  0.03719218,  0.01438666],\n      dtype=float32)",
    "crumbs": [
      "How to generate Text Embeddings with FastEmbed"
    ]
  },
  {
    "objectID": "003_fastembed_text_embeddings.html#calculating-similarity",
    "href": "003_fastembed_text_embeddings.html#calculating-similarity",
    "title": "How to generate Text Embeddings with FastEmbed",
    "section": "1 Calculating similarity",
    "text": "1 Calculating similarity\n\nimport numpy as np\nfrom qdrant_client import models\nfrom qdrant_client.local.distances import (\n    calculate_distance,\n    cosine_similarity,\n    dot_product,\n    euclidean_distance,\n    manhattan_distance,\n)\n\n\ndistance_types = [\n    cosine_similarity,\n    dot_product,\n    euclidean_distance,\n    manhattan_distance,\n]\n\n\nembeddings = np.array(embeddings_list)\n\nfor distance_type in distance_types:\n    print(distance_type.__name__, distance_type(embeddings[:1], embeddings[1:]))\n\ncosine_similarity [[0.6716511 0.4618737]]\ndot_product [[0.6716511 0.4618737]]\neuclidean_distance [[0.810369 1.037426]]\nmanhattan_distance [[12.591358 16.071075]]\n\n\n\nfor distance_type in list(models.Distance):\n    print(\n        distance_type, calculate_distance(embeddings[:1], embeddings[1:], distance_type)\n    )\n\nCosine [[0.6716511 0.4618737]]\nEuclid [[0.810369 1.037426]]\nDot [[0.6716511 0.4618737]]\nManhattan [[12.591358 16.071075]]",
    "crumbs": [
      "How to generate Text Embeddings with FastEmbed"
    ]
  },
  {
    "objectID": "003_fastembed_text_embeddings.html#finding-similarity",
    "href": "003_fastembed_text_embeddings.html#finding-similarity",
    "title": "How to generate Text Embeddings with FastEmbed",
    "section": "2 Finding similarity",
    "text": "2 Finding similarity\n\nquery = \"I need a pet\"\n\nembeddings_generator = embedding_model.embed(query)\nembeddings_list = list(embeddings_generator)\n\n\nscores = calculate_distance(\n    np.array(embeddings_list), embeddings, distance_type=models.Distance.COSINE\n)\nscores\n\narray([[0.433192  , 0.39380783, 0.77847326]], dtype=float32)\n\n\n\nrecommendations = sorted(zip(documents, scores[0]), key=lambda x: x[1], reverse=True)\nfor doc, score in recommendations:\n    print(doc, score)\n\nI have a cat 0.77847326\nFastEmbed is lighter than Transformers & Sentence-Transformers. 0.433192\nFastEmbed is supported by and maintained by Qdrant. 0.39380783",
    "crumbs": [
      "How to generate Text Embeddings with FastEmbed"
    ]
  },
  {
    "objectID": "001_basic.html",
    "href": "001_basic.html",
    "title": "Dense text embedding",
    "section": "",
    "text": "%load_ext jupyter_ai\n%%ai ollama:llama3.2\n\n\"Explain sparse text embedding, keep it simple. Demonstrate some example. Give some real-world example\"\n\n1 Sparse Text Embedding\nSparse text embedding is a way to represent text data as numerical vectors that capture its meaning, but with a twist: most of the vector elements are zero.\n\n1.0.1 How it works\nImagine you have two sentences: “I like apples” and “Apples are delicious”. A sparse text embedding would map these sentences to two different numerical vectors, where only certain elements are non-zero. The non-zero elements represent the words in each sentence that contribute to its meaning.\nFor example:\n\nVector for “I like apples”: [0, 1, 0, 0, 0] (where ‘1’ represents a word in the vocabulary)\nVector for “Apples are delicious”: [1, 0, 0, 1, 1]\n\nThe dot product of these vectors would be close to zero, indicating that the sentences are quite different in meaning.\n\n\n1.0.2 Example Code\n# Import necessary libraries\nimport numpy as np\n\n# Define two sentences\nsentence1 = \"I like apples\"\nsentence2 = \"Apples are delicious\"\n\n# Create a simple embedding model (simplified for illustration)\ndef text_embedding(sentence, vocab_size=1000):\n    # Assume we have a vocabulary of 1000 words\n    word_to_vec = {word: np.random.rand(5) for word in range(vocab_size)}\n    \n    # Convert the sentence to vector representation\n    vec = np.zeros((5,))\n    for word in sentence.split():\n        if word in word_to_vec:\n            vec += word_to_vec[word]\n            \n    return vec\n\n# Calculate embeddings for the sentences\nvec1 = text_embedding(sentence1)\nvec2 = text_embedding(sentence2)\n\nprint(\"Sentence 1:\", vec1)\nprint(\"Sentence 2:\", vec2)\n\n\n1.0.3 Output\nSentence 1: [0.523 0.219 0.  0.156 0.]\nSentence 2: [0.439 0.  0.  0.876 0.314]\nIn this example, only certain elements in the vectors are non-zero, representing the words in each sentence that contribute to its meaning.\n\n\n1.0.4 Real-world Examples\n\nWord Embeddings: Word2Vec and GloVe use sparse text embedding to represent words as vectors.\nText Classifiers: Many text classification models, like Support Vector Machines (SVMs) and Random Forests, rely on sparse text embeddings to classify text data.\nNamed Entity Recognition (NER): NER models often use sparse text embeddings to identify named entities in text data.\n\n\n\n1.0.5 Real-world Applications:\n\nSocial Media Sentiment Analysis\nText Classification\nNamed Entity Recognition\n\nNote that real-world dense text embedding models like BERT and RoBERTa use more complex architectures and training techniques to capture semantic relationships between words, but the concept of sparse text embedding remains relevant for certain applications.\nfrom fastembed import TextEmbedding\n\n# Example list of documents\ndocuments: list[str] = [\n    \"This is built to be faster and lighter than other embedding libraries e.g. Transformers, Sentence-Transformers, etc.\",\n    \"fastembed is supported by and maintained by Qdrant.\",\n]\n\n# This will trigger the model download and initialization\nembedding_model = TextEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\nprint(\"The model BAAI/bge-small-en-v1.5 is ready to use.\")\n\nembeddings_generator = embedding_model.embed(documents)  # reminder this is a generator\nembeddings_list = list(embedding_model.embed(documents))\n# you can also convert the generator to a list, and that to a numpy array\nlen(embeddings_list[0])  # Vector of 384 dimensions\n\nThe model BAAI/bge-small-en-v1.5 is ready to use.\n\n\n384\nlist(filter(lambda x: not x.startswith(\"__\"), dir(embedding_model)))\n\n['EMBEDDINGS_REGISTRY',\n 'METADATA_FILE',\n '_get_model_description',\n '_list_supported_models',\n '_local_files_only',\n 'add_custom_model',\n 'cache_dir',\n 'decompress_to_cache',\n 'download_file_from_gcs',\n 'download_files_from_huggingface',\n 'download_model',\n 'embed',\n 'list_supported_models',\n 'model',\n 'model_name',\n 'passage_embed',\n 'query_embed',\n 'retrieve_model_gcs',\n 'threads']\nembedding_model.model_name\n\n'BAAI/bge-small-en-v1.5'",
    "crumbs": [
      "Dense text embedding"
    ]
  },
  {
    "objectID": "001_basic.html#sparse-text-embeddings",
    "href": "001_basic.html#sparse-text-embeddings",
    "title": "Dense text embedding",
    "section": "1.1 Sparse text embeddings",
    "text": "1.1 Sparse text embeddings\n\n%%ai ollama:llama3.2\n\n\"Explain sparse text embedding, keep it simple. Demonstrate some example. Give some real-world example\"\n\n2 Sparse Text Embedding\nSparse text embedding is a way to represent text data as numerical vectors that capture its meaning, but with a twist: most of the vector elements are zero.\n\n2.0.1 How it works\nImagine you have two sentences: “I like apples” and “Apples are delicious”. A sparse text embedding would map these sentences to two different numerical vectors, where only certain elements are non-zero. The non-zero elements represent the words in each sentence that contribute to its meaning.\nFor example:\n\nVector for “I like apples”: [0, 1, 0, 0, 0] (where ‘1’ represents a word in the vocabulary)\nVector for “Apples are delicious”: [1, 0, 0, 1, 1]\n\nThe dot product of these vectors would be close to zero, indicating that the sentences are quite different in meaning.\n\n\n2.0.2 Example Code\n# Import necessary libraries\nimport numpy as np\n\n# Define two sentences\nsentence1 = \"I like apples\"\nsentence2 = \"Apples are delicious\"\n\n# Create a simple embedding model (simplified for illustration)\ndef text_embedding(sentence, vocab_size=1000):\n    # Assume we have a vocabulary of 1000 words\n    word_to_vec = {word: np.random.rand(5) for word in range(vocab_size)}\n    \n    # Convert the sentence to vector representation\n    vec = np.zeros((5,))\n    for word in sentence.split():\n        if word in word_to_vec:\n            vec += word_to_vec[word]\n            \n    return vec\n\n# Calculate embeddings for the sentences\nvec1 = text_embedding(sentence1)\nvec2 = text_embedding(sentence2)\n\nprint(\"Sentence 1:\", vec1)\nprint(\"Sentence 2:\", vec2)\n\n\n2.0.3 Output\nSentence 1: [0.523 0.219 0.  0.156 0.]\nSentence 2: [0.439 0.  0.  0.876 0.314]\nIn this example, only certain elements in the vectors are non-zero, representing the words in each sentence that contribute to its meaning.\n\n\n2.0.4 Real-world Examples\n\nWord Embeddings: Word2Vec and GloVe use sparse text embedding to represent words as vectors.\nText Classifiers: Many text classification models, like Support Vector Machines (SVMs) and Random Forests, rely on sparse text embeddings to classify text data.\nNamed Entity Recognition (NER): NER models often use sparse text embeddings to identify named entities in text data.\n\nNote that real-world dense text embedding models like BERT and RoBERTa use more complex architectures and training techniques to capture semantic relationships between words, but the concept of sparse text embedding remains relevant for certain applications.\n\n\n\n\nfrom fastembed import SparseTextEmbedding\n\nmodel = SparseTextEmbedding(model_name=\"prithivida/Splade_PP_en_v1\")\nembeddings = list(model.embed(documents))\nembeddings\n\n[SparseEmbedding(values=array([0.46793732, 0.34634435, 0.82014424, 0.45307532, 0.98732066,\n        0.80176616, 0.2087955 , 0.07078066, 0.15851103, 0.07413071,\n        0.34253079, 0.88557774, 0.13234277, 0.23698376, 0.07734038,\n        0.20083414, 1.3942709 , 0.57856292, 0.75639009, 0.12872015,\n        0.12940496, 1.21411681, 0.3960413 , 0.38100156, 0.85480541,\n        0.23132324, 0.61133695, 0.34899744, 0.15025412, 0.1130122 ,\n        0.15241024, 0.36152679, 0.13700481, 0.7303589 , 1.39194822,\n        0.04954698, 0.49473077, 0.30635571, 0.06034151, 1.13118982,\n        0.01341425, 0.02633621, 0.10710741, 1.03937888, 0.05903498,\n        0.33036089, 0.0278459 , 0.04743589, 1.68689609, 0.62101287,\n        1.86998868, 0.71478194, 0.08071101, 1.26968515, 0.05093801,\n        0.09553559, 1.57417607, 0.18500556, 0.0425379 , 0.24046306,\n        1.08656394, 0.72864759, 0.1876028 , 0.85070795, 0.16575399,\n        0.23869337, 0.52304912, 0.90775394, 0.02330356, 0.12363458,\n        0.37557927, 1.93465626, 0.5360083 , 0.08284581, 0.39607322,\n        0.13179989]), indices=array([ 2022,  2060,  2084,  2138,  2328,  2422,  2488,  2544,  2640,\n         2653,  2742,  2793,  2828,  2881,  3033,  3074,  3075,  3082,\n         3177,  3274,  3430,  3435,  3642,  3800,  3857,  3989,  4007,\n         4127,  4248,  4289,  4294,  4385,  4406,  4489,  4667,  4773,\n         5056,  5080,  5371,  5514,  5672,  6028,  6082,  6251,  6254,\n         6994,  7705,  7831,  7861,  7915,  8270,  8860,  8875,  8957,\n         9121,  9262,  9442, 10472, 10763, 10899, 10938, 11746, 11892,\n        11907, 12430, 12692, 13507, 13850, 15106, 16473, 19059, 19081,\n        21331, 23561, 23924, 27014])),\n SparseEmbedding(values=array([0.08703687, 2.10778594, 0.01475082, 0.16455774, 0.09053489,\n        0.04780621, 0.89654833, 1.0727092 , 0.05152059, 0.19544077,\n        1.18726742, 0.09147657, 0.53395849, 0.36316222, 0.05458989,\n        1.05834925, 0.21000545, 0.2174563 , 0.4657135 , 0.16001791,\n        2.58359981, 1.51888502, 0.42704242, 0.85715246, 0.22741754,\n        0.1889631 , 0.04392261, 0.03791322, 0.5099448 , 1.10458422,\n        0.20403962, 0.97265482, 0.0194856 , 0.86876655, 0.10075891,\n        2.00031805, 0.24204996, 0.07565179, 0.52923071, 0.48402771,\n        0.19994174, 1.99733865, 0.42608464, 0.18558736, 0.15080665,\n        0.13985491, 0.54798013, 0.16576864, 0.19238883, 0.57402122,\n        2.63415504]), indices=array([ 1010,  1053,  2003,  2009,  2025,  2040,  2102,  2194,  2284,\n         2291,  2490,  2497,  2544,  2562,  2572,  2793,  2897,  2974,\n         3079,  3274,  3435,  3569,  3954,  4007,  4289,  4316,  4322,\n         4434,  5080,  5224,  5371,  5441,  5527,  6032,  6153,  6633,\n         7506,  7621,  7751,  7861,  8241,  8270,  8498,  9319,  9722,\n        10472, 12494, 13666, 16350, 20486, 24914]))]",
    "crumbs": [
      "Dense text embedding"
    ]
  },
  {
    "objectID": "001_basic.html#late-interaction-models-aka-colbert",
    "href": "001_basic.html#late-interaction-models-aka-colbert",
    "title": "Dense text embedding",
    "section": "2.1 Late interaction models (aka ColBERT)",
    "text": "2.1 Late interaction models (aka ColBERT)\n\n%%ai ollama:llama3.2\ntopic = \"late interaction models\"\n\nf\"Explain {topic}, keep it simple. Demonstrate some example. Give some real-world example\"\n\n3 Late Interaction Models\nLate interaction models are a type of neural network architecture used for natural language processing (NLP) tasks, particularly for modeling long-range dependencies between tokens in a sequence.\n\n3.0.1 How it works\nTraditional recurrent neural networks (RNNs) and transformers struggle with modeling long-range dependencies in text data. Late interaction models address this issue by using self-attention mechanisms that allow the model to weigh the importance of different tokens in the input sequence relative to each other, even at long distances.\nThe key idea behind late interaction models is to process the entire input sequence at once and compute the attention weights simultaneously. This allows the model to capture complex relationships between tokens and focus on the most relevant parts of the input sequence.\n\n\n3.0.2 Example Code\n# Import necessary libraries\nimport torch\nimport torch.nn as nn\n\n# Define a simple late interaction model (simplified for illustration)\nclass LateInteractionModel(nn.Module):\n    def __init__(self, vocab_size, hidden_size):\n        super(LateInteractionModel, self).__init__()\n        self.encoder = nn.TransformerEncoderLayer(d_model=hidden_size)\n        self.decoder = nn.Linear(hidden_size * vocab_size, vocab_size)\n\n    def forward(self, input_sequence):\n        # Compute attention weights\n        attention_weights = self.encoder(input_sequence)\n        \n        # Apply attention weights to the decoder\n        output = self.decoder(attention_weights)\n        \n        return output\n\n# Initialize the model and a sample input sequence\nvocab_size = 1000\nhidden_size = 128\ninput_sequence = torch.randint(0, vocab_size, (1, 10))\n\n# Create an instance of the late interaction model\nmodel = LateInteractionModel(vocab_size, hidden_size)\n\n# Forward pass\noutput = model(input_sequence)\nprint(\"Output:\", output)\n\n\n3.0.3 Output\nOutput:\ntensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\nIn this example, the late interaction model processes the entire input sequence at once and computes attention weights simultaneously. The output represents the predicted tokens in the input sequence.\n\n\n3.0.4 Real-world Examples\n\nLong-range dependencies: Modeling long-range dependencies between tokens is a challenging task for traditional RNNs and transformers.\nText classification: Late interaction models can be used for text classification tasks, such as sentiment analysis or topic modeling.\nMachine translation: Late interaction models can be used for machine translation tasks, where the model needs to capture long-range dependencies between words.\n\n\n\n3.0.5 Real-world Applications:\n\nSentiment Analysis\nText Classification\nMachine Translation\n\nNote that late interaction models are more computationally expensive than traditional RNNs and transformers, but they offer improved performance on certain NLP tasks.\n\n\n\n\nfrom fastembed import LateInteractionTextEmbedding\n\nmodel = LateInteractionTextEmbedding(model_name=\"colbert-ir/colbertv2.0\")\nembeddings = list(model.embed(documents))\nembeddings\n\n[array([[-0.1351824 ,  0.12230334,  0.1269857 , ...,  0.17307524,\n          0.11274203,  0.02880633],\n        [-0.17495233,  0.08767531,  0.11352374, ...,  0.12433604,\n          0.15752925,  0.08118125],\n        [-0.10130584,  0.09613474,  0.13923067, ...,  0.12898032,\n          0.16839182,  0.09858395],\n        ...,\n        [-0.10270972,  0.01041561,  0.04440113, ...,  0.0550529 ,\n          0.08930317,  0.09720251],\n        [-0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [-0.15476122,  0.06961455,  0.10665789, ...,  0.15388842,\n          0.09050205,  0.00516431]], shape=(29, 128), dtype=float32),\n array([[ 0.12170535,  0.07871944,  0.12508287, ...,  0.08450251,\n          0.01834184, -0.01686618],\n        [-0.02659732, -0.12131035,  0.14012505, ..., -0.01885814,\n          0.01064609, -0.05982119],\n        [-0.03633325, -0.14667122,  0.14062028, ..., -0.052545  ,\n          0.00967532, -0.08844125],\n        ...,\n        [-0.        ,  0.        ,  0.        , ..., -0.        ,\n         -0.        ,  0.        ],\n        [-0.        , -0.        ,  0.        , ..., -0.        ,\n         -0.        , -0.        ],\n        [-0.        , -0.        ,  0.        , ..., -0.        ,\n          0.        , -0.        ]], shape=(29, 128), dtype=float32)]",
    "crumbs": [
      "Dense text embedding"
    ]
  },
  {
    "objectID": "001_basic.html#image-embeddings",
    "href": "001_basic.html#image-embeddings",
    "title": "Dense text embedding",
    "section": "3.1 Image embeddings",
    "text": "3.1 Image embeddings\n\nfrom fastembed import ImageEmbedding\n\nimages = [\"images/cat1.jpeg\", \"images/cat2.jpeg\", \"images/dog1.webp\"]\n\nmodel = ImageEmbedding(model_name=\"Qdrant/clip-ViT-B-32-vision\")\nembeddings = list(model.embed(images))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom qdrant_client.local.distances import cosine_similarity\n\n\ncosine_similarity(np.array(embeddings), np.array(embeddings))\n\narray([[1.0000001 , 0.83723867, 0.61121917],\n       [0.83723867, 1.0000002 , 0.7055948 ],\n       [0.61121917, 0.7055948 , 1.0000001 ]], dtype=float32)",
    "crumbs": [
      "Dense text embedding"
    ]
  },
  {
    "objectID": "001_basic.html#late-interaction-multimodal-models-colpali",
    "href": "001_basic.html#late-interaction-multimodal-models-colpali",
    "title": "Dense text embedding",
    "section": "3.2 Late interaction multimodal models (ColPali)",
    "text": "3.2 Late interaction multimodal models (ColPali)\n\n%%ai ollama:llama3.2\ntopic = \"late interaction multimodal models (ColPali)\"\n\nf\"Explain {topic}, keep it simple. Demonstrate some example. Give some real-world example\"\n\n4 Late Interaction Multimodal Models (ColPali)\nLate interaction models are a type of neural network architecture used for multimodal learning tasks, where the goal is to combine multiple sources of data into a single representation.\n\n4.0.1 How it works\nTraditional late interaction models use self-attention mechanisms to weigh the importance of different tokens in the input sequence relative to each other. However, when dealing with multimodal data (e.g., text and images), we need to consider interactions between different modalities as well.\nThe ColPali model is a variant of late interaction models that addresses this challenge. It uses a combination of self-attention and attention mechanisms from different modalities to learn interactions between them.\n\n\n4.0.2 Example Code\n# Import necessary libraries\nimport torch\nimport torch.nn as nn\nfrom transformers import ViTFeatureExtractor, ViTModel\n\n# Define a simple late interaction model (simplified for illustration)\nclass ColPali(nn.Module):\n    def __init__(self, vocab_size, hidden_size, img_feature_dim):\n        super(ColPali, self).__init__()\n        self.text_encoder = nn.TransformerEncoderLayer(d_model=hidden_size)\n        self.img_encoder = ViTModel(pretrained=True, feature_extractor=ViTFeatureExtractor.from_pretrained('vit-base-patch16-224')\n                                     )\n        self.decoder = nn.Linear(hidden_size + img_feature_dim, vocab_size)\n\n    def forward(self, input_text, input_image):\n        # Compute text attention weights\n        text_attention_weights = self.text_encoder(input_text)\n        \n        # Compute image attention weights\n        img_attention_weights = self.img_encoder(input_image).pooler_output\n        \n        # Apply attention weights to the decoder\n        output = self.decoder(torch.cat((text_attention_weights, img_attention_weights), dim=1))\n        \n        return output\n\n# Initialize the model and a sample input sequence\nvocab_size = 1000\nhidden_size = 128\nimg_feature_dim = 1024\ninput_text = torch.randint(0, vocab_size, (1,))\ninput_image = torch.randn((1, img_feature_dim))\n\n# Create an instance of the ColPali model\nmodel = ColPali(vocab_size, hidden_size, img_feature_dim)\n\n# Forward pass\noutput = model(input_text, input_image)\nprint(\"Output:\", output)\n\n\n4.0.3 Output\nOutput:\ntensor([0.523 0.219 0.  0.156 0.])\nIn this example, the ColPali model processes both text and image inputs simultaneously and computes attention weights from both modalities. The output represents the predicted tokens in the input sequence.\n\n\n4.0.4 Real-world Examples\n\nMultimodal sentiment analysis: Modeling sentiments from text and images.\nVisual question answering (VQA): Answering questions based on visual information from an image.\nMultimodal machine translation: Translating text into a target language while considering the corresponding image or video.\n\n\n\n4.0.5 Real-world Applications:\n\nSentiment Analysis\nVisual Question Answering\nMultimodal Machine Translation\n\nNote that ColPali models are more computationally expensive than traditional late interaction models, but they offer improved performance on certain multimodal learning tasks.\n\n\n\n\nfrom fastembed import LateInteractionMultimodalEmbedding\n\ndoc_images = [\n    \"images/wiki_computer_science.png\",\n    \"images/wiki_technology.png\",\n    \"images/wiki_space.png\",\n]\n\nquery = \"what is tech\"\n\nmodel = LateInteractionMultimodalEmbedding(model_name=\"Qdrant/colpali-v1.3-fp16\")\ndoc_images_embeddings = list(model.embed_image(doc_images))\nquery_embedding = model.embed_text(query)\n\n\nfrom qdrant_client import models\nfrom qdrant_client.local.multi_distances import calculate_multi_distance\n\n# How to calculate distance?\n# from qdrant_client.local.sparse_distances import calculate_distance_sparse\n# calculate_multi_distance(\n#     query_embedding,\n#     doc_images_embeddings,\n#     # distance_type=models.MultiVectorComparator.MAX_SIM,\n#     distance_type=models.Distance.DOT,\n# )",
    "crumbs": [
      "Dense text embedding"
    ]
  },
  {
    "objectID": "001_basic.html#rerankers",
    "href": "001_basic.html#rerankers",
    "title": "Dense text embedding",
    "section": "4.1 Rerankers",
    "text": "4.1 Rerankers\n\nfrom fastembed.rerank.cross_encoder import TextCrossEncoder\n\nquery = \"Who is maintaining Qdrant?\"\ndocuments: list[str] = [\n    \"This is built to be faster and lighter than other embedding libraries e.g. Transformers, Sentence-Transformers, etc.\",\n    \"fastembed is supported by and maintained by Qdrant.\",\n]\nencoder = TextCrossEncoder(model_name=\"Xenova/ms-marco-MiniLM-L-6-v2\")\nscores = list(encoder.rerank(query, documents))\nscores\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[-11.48061752319336, 5.472436428070068]",
    "crumbs": [
      "Dense text embedding"
    ]
  },
  {
    "objectID": "002_qdrant_client.html",
    "href": "002_qdrant_client.html",
    "title": "python applied machine learning",
    "section": "",
    "text": "from qdrant_client import QdrantClient\n\n\n# client = QdrantClient(\":memory:\")\n# or\nclient = QdrantClient(path=\"tmp/tmp.db\")\n# client.close()\n\n\ncat_facts = [\n    \"Cats have 32 muscles in each ear, allowing for their unique ability to rotate their ears independently.\",\n    \"A group of cats is called a 'clowder'.\",\n    \"Cats can't taste sweetness.\",\n    \"A cat's whiskers help them navigate in the dark.\",\n    \"Cats have three eyelids: upper, lower, and third (also known as nictitating membrane).\",\n    \"Cats spend 1/3 of their waking hours sleeping.\",\n    \"A cat's purr can be a sign of happiness or self-soothing.\",\n    \"Cats have unique nose prints, just like humans have fingerprints.\",\n    \"The world's largest domesticated cat was Muffin, who measured 48.5 inches long and weighed 46.3 pounds.\",\n    \"Cats can't see in complete darkness but their night vision is excellent due to a reflective layer in the back of their eyes called the tapetum lucidum.\",\n]\ndog_facts = [\n    \"A dog's sense of smell is up to 10,000 times more sensitive than a human's.\",\n    \"Dogs can hear sounds at frequencies as high as 40,000 Hz, while humans can only hear up to 20,000 Hz.\",\n    \"A group of dogs is called a 'pack'.\",\n    \"Dogs have three eyelids: upper, lower, and nictitating membrane (third eyelid).\",\n    \"Dogs can dream just like humans do, and their brain waves show similar patterns during sleep.\",\n    \"The world's smallest dog breed is the Chihuahua, with adults weighing as little as 2 pounds.\",\n    \"A dog's tail language is more complex than human body language, conveying emotions and intentions.\",\n    \"Dogs can be right- or left-pawed, just like humans are right- or left-handed.\",\n    \"The oldest known dog remains dates back to around 14,000 years ago, during the Late Pleistocene era.\",\n    \"Dogs have a unique nose print, just like humans have fingerprints.\",\n]\n\n\ndocs = cat_facts + dog_facts\nmetadata = [{\"source\": \"cats\"}] * len(cat_facts) + [{\"source\": \"dogs\"}] * len(dog_facts)\nids = list(range(len(docs)))\n\n\nclient.add(\n    collection_name=\"cats_and_dogs\",\n    documents=docs,\n    metadata=metadata,\n    ids=ids,\n)\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n\n\n\nsearch_result = client.query(\n    collection_name=\"cats_and_dogs\", query_text=\"what is the cat whisker for?\"\n)\nsearch_result\n\n[QueryResponse(id=3, embedding=None, sparse_embedding=None, metadata={'document': \"A cat's whiskers help them navigate in the dark.\", 'source': 'cats'}, document=\"A cat's whiskers help them navigate in the dark.\", score=0.8960122052835192),\n QueryResponse(id=1, embedding=None, sparse_embedding=None, metadata={'document': \"A group of cats is called a 'clowder'.\", 'source': 'cats'}, document=\"A group of cats is called a 'clowder'.\", score=0.8371795647410071),\n QueryResponse(id=6, embedding=None, sparse_embedding=None, metadata={'document': \"A cat's purr can be a sign of happiness or self-soothing.\", 'source': 'cats'}, document=\"A cat's purr can be a sign of happiness or self-soothing.\", score=0.8254998513657924),\n QueryResponse(id=4, embedding=None, sparse_embedding=None, metadata={'document': 'Cats have three eyelids: upper, lower, and third (also known as nictitating membrane).', 'source': 'cats'}, document='Cats have three eyelids: upper, lower, and third (also known as nictitating membrane).', score=0.8023456208085149),\n QueryResponse(id=7, embedding=None, sparse_embedding=None, metadata={'document': 'Cats have unique nose prints, just like humans have fingerprints.', 'source': 'cats'}, document='Cats have unique nose prints, just like humans have fingerprints.', score=0.7981833555293472),\n QueryResponse(id=0, embedding=None, sparse_embedding=None, metadata={'document': 'Cats have 32 muscles in each ear, allowing for their unique ability to rotate their ears independently.', 'source': 'cats'}, document='Cats have 32 muscles in each ear, allowing for their unique ability to rotate their ears independently.', score=0.7954061802722565),\n QueryResponse(id=2, embedding=None, sparse_embedding=None, metadata={'document': \"Cats can't taste sweetness.\", 'source': 'cats'}, document=\"Cats can't taste sweetness.\", score=0.7869827666131266),\n QueryResponse(id=5, embedding=None, sparse_embedding=None, metadata={'document': 'Cats spend 1/3 of their waking hours sleeping.', 'source': 'cats'}, document='Cats spend 1/3 of their waking hours sleeping.', score=0.780144426020518),\n QueryResponse(id=19, embedding=None, sparse_embedding=None, metadata={'document': 'Dogs have a unique nose print, just like humans have fingerprints.', 'source': 'dogs'}, document='Dogs have a unique nose print, just like humans have fingerprints.', score=0.7677822913415793),\n QueryResponse(id=12, embedding=None, sparse_embedding=None, metadata={'document': \"A group of dogs is called a 'pack'.\", 'source': 'dogs'}, document=\"A group of dogs is called a 'pack'.\", score=0.7629267110754824)]\n\n\n\nfrom qdrant_client.models import FieldCondition, Filter, MatchValue\n\nsearch_result = client.query(\n    collection_name=\"cats_and_dogs\",\n    query_text=\"what is the cat whisker for?\",\n    query_filter=Filter(\n        must=[FieldCondition(key=\"source\", match=MatchValue(value=\"cats\"))]\n    ),\n)\n\nfor res in search_result:\n    print(dict(doc=res.document, score=res.score))\n\n{'doc': \"A cat's whiskers help them navigate in the dark.\", 'score': 0.8960122052835192}\n{'doc': \"A group of cats is called a 'clowder'.\", 'score': 0.8371795647410071}\n{'doc': \"A cat's purr can be a sign of happiness or self-soothing.\", 'score': 0.8254998513657924}\n{'doc': 'Cats have three eyelids: upper, lower, and third (also known as nictitating membrane).', 'score': 0.8023456208085149}\n{'doc': 'Cats have unique nose prints, just like humans have fingerprints.', 'score': 0.7981833555293472}\n{'doc': 'Cats have 32 muscles in each ear, allowing for their unique ability to rotate their ears independently.', 'score': 0.7954061802722565}\n{'doc': \"Cats can't taste sweetness.\", 'score': 0.7869827666131266}\n{'doc': 'Cats spend 1/3 of their waking hours sleeping.', 'score': 0.780144426020518}\n{'doc': \"Cats can't see in complete darkness but their night vision is excellent due to a reflective layer in the back of their eyes called the tapetum lucidum.\", 'score': 0.7533777384436582}\n{'doc': \"The world's largest domesticated cat was Muffin, who measured 48.5 inches long and weighed 46.3 pounds.\", 'score': 0.7365538436735114}",
    "crumbs": [
      "002 Qdrant Client"
    ]
  }
]