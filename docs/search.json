[
  {
    "objectID": "ss101_004_measure_and_improve_retrieval_quality.html",
    "href": "ss101_004_measure_and_improve_retrieval_quality.html",
    "title": "Semantic Search 101: Measure and Improve Retrieval Quality in Semantic Search",
    "section": "",
    "text": "https://qdrant.tech/documentation/beginner-tutorials/retrieval-quality/\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\n    \"Qdrant/arxiv-titles-instructorxl-embeddings\", split=\"train\", streaming=True\n)\n\n\n\n\n\ndataset_iterator = iter(dataset)\ntrain_dataset = [next(dataset_iterator) for _ in range(6000)]\ntest_dataset = [next(dataset_iterator) for _ in range(100)]\n\n\nfrom qdrant_client import QdrantClient, models\n\nclient = QdrantClient(path=\"tmp/arxiv-titles\")\nif not client.collection_exists(\"arxiv-titles-instructorxl-embeddings\"):\n    client.create_collection(\n        collection_name=\"arxiv-titles-instructorxl-embeddings\",\n        vectors_config=models.VectorParams(\n            size=768,  # Size of the embeddings generated by InstructorXL model\n            distance=models.Distance.COSINE,\n        ),\n    )\n\n\nclient.upload_points(  # upload_points is available as of qdrant-client v1.7.1\n    collection_name=\"arxiv-titles-instructorxl-embeddings\",\n    points=[\n        models.PointStruct(\n            id=item[\"id\"],\n            vector=item[\"vector\"],\n            payload=item,\n        )\n        for item in train_dataset\n    ],\n)\n\nwhile True:\n    collection_info = client.get_collection(\n        collection_name=\"arxiv-titles-instructorxl-embeddings\"\n    )\n    if collection_info.status == models.CollectionStatus.GREEN:\n        # Collection status is green, which means the indexing is finished\n        break\n\n\ndef avg_precision_at_k(k: int):\n    precisions = []\n    for item in test_dataset:\n        ann_result = client.query_points(\n            collection_name=\"arxiv-titles-instructorxl-embeddings\",\n            query=item[\"vector\"],\n            limit=k,\n        ).points\n\n        knn_result = client.query_points(\n            collection_name=\"arxiv-titles-instructorxl-embeddings\",\n            query=item[\"vector\"],\n            limit=k,\n            search_params=models.SearchParams(\n                exact=True,  # Turns on the exact search mode\n            ),\n        ).points\n\n        # We can calculate the precision@k by comparing the ids of the search results\n        ann_ids = set(item.id for item in ann_result)\n        knn_ids = set(item.id for item in knn_result)\n        precision = len(ann_ids.intersection(knn_ids)) / k\n        precisions.append(precision)\n\n    return sum(precisions) / len(precisions)\n\n\nprint(f\"avg(precision@5) = {avg_precision_at_k(k=5)}\")\n\navg(precision@5) = 1.0\n\n\n\nclient.update_collection(\n    collection_name=\"arxiv-titles-instructorxl-embeddings\",\n    hnsw_config=models.HnswConfigDiff(\n        m=32,  # Increase the number of edges per node from the default 16 to 32\n        ef_construct=200,  # Increase the number of neighbours from the default 100 to 200\n    ),\n)\n\nwhile True:\n    collection_info = client.get_collection(\n        collection_name=\"arxiv-titles-instructorxl-embeddings\"\n    )\n    if collection_info.status == models.CollectionStatus.GREEN:\n        # Collection status is green, which means the indexing is finished\n        break\n\n\nprint(f\"avg(precision@5) = {avg_precision_at_k(k=5)}\")\n\navg(precision@5) = 1.0",
    "crumbs": [
      "Semantic Search 101: Measure and Improve Retrieval Quality in Semantic Search"
    ]
  },
  {
    "objectID": "ss101_002_build_neural_search.html",
    "href": "ss101_002_build_neural_search.html",
    "title": "Semantic Search 101: Build a Neural Search Service",
    "section": "",
    "text": "https://qdrant.tech/documentation/beginner-tutorials/neural-search/\n!curl https://storage.googleapis.com/generall-shared-data/startups_demo.json -O startups_demo.json\n\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 21.1M  100 21.1M    0     0  4564k      0  0:00:04  0:00:04 --:--:-- 5832k\ncurl: (6) Could not resolve host: startups_demo.json\n!head -n 1 startups_demo.json\n\n{\"name\":\"SaferCodes\",\"images\":\"https:\\/\\/safer.codes\\/img\\/brand\\/logo-icon.png\",\"alt\":\"SaferCodes Logo QR codes generator system forms for COVID-19\",\"description\":\"QR codes systems for COVID-19.\\nSimple tools for bars, restaurants, offices, and other small proximity businesses.\",\"link\":\"https:\\/\\/safer.codes\",\"city\":\"Chicago\"}\nimport json\n\nimport numpy as np\nimport pandas as pd\nfrom fastembed import TextEmbedding\nfrom tqdm.notebook import tqdm\nmodel_name = \"sentence-transformers/all-MiniLM-L6-v2\"\nfor model in TextEmbedding.list_supported_models():\n    if model[\"model\"] == model_name:\n        break\n\nmodel\n\n{'model': 'sentence-transformers/all-MiniLM-L6-v2',\n 'dim': 384,\n 'description': 'Text embeddings, Unimodal (text), English, 256 input tokens truncation, Prefixes for queries/documents: not necessary, 2021 year.',\n 'license': 'apache-2.0',\n 'size_in_GB': 0.09,\n 'sources': {'url': 'https://storage.googleapis.com/qdrant-fastembed/sentence-transformers-all-MiniLM-L6-v2.tar.gz',\n  'hf': 'qdrant/all-MiniLM-L6-v2-onnx'},\n 'model_file': 'model.onnx'}\nembedding_model = TextEmbedding(model_name=model_name)",
    "crumbs": [
      "Semantic Search 101: Build a Neural Search Service"
    ]
  },
  {
    "objectID": "ss101_002_build_neural_search.html#prepare-sample-dataset",
    "href": "ss101_002_build_neural_search.html#prepare-sample-dataset",
    "title": "Semantic Search 101: Build a Neural Search Service",
    "section": "1 Prepare sample dataset",
    "text": "1 Prepare sample dataset\n\ndf = pd.read_json(\"startups_demo.json\", lines=True)\ndf.head()\n\n\n\n\n\n\n\n\nname\nimages\nalt\ndescription\nlink\ncity\n\n\n\n\n0\nSaferCodes\nhttps://safer.codes/img/brand/logo-icon.png\nSaferCodes Logo QR codes generator system form...\nQR codes systems for COVID-19.\\nSimple tools f...\nhttps://safer.codes\nChicago\n\n\n1\nHuman Practice\nhttps://d1qb2nb5cznatu.cloudfront.net/startups...\nHuman Practice - health care information tech...\nPoint-of-care word of mouth\\nPreferral is a mo...\nhttp://humanpractice.com\nChicago\n\n\n2\nStyleSeek\nhttps://d1qb2nb5cznatu.cloudfront.net/startups...\nStyleSeek - e-commerce fashion mass customiza...\nPersonalized e-commerce for lifestyle products...\nhttp://styleseek.com\nChicago\n\n\n3\nScout\nhttps://d1qb2nb5cznatu.cloudfront.net/startups...\nScout - security consumer electronics interne...\nHassle-free Home Security\\nScout is a self-ins...\nhttp://www.scoutalarm.com\nChicago\n\n\n4\nInvitation codes\nhttps://invitation.codes/img/inv-brand-fb3.png\nInvitation App - Share referral codes community\nThe referral community\\nInvitation App is a so...\nhttps://invitation.codes\nChicago\n\n\n\n\n\n\n\n\nvectors = list(\n    embedding_model.embed([row.alt + \". \" + row.description for row in df.itertuples()])\n)\n\n\nnp.save(\"startup_vectors.npy\", vectors, allow_pickle=False)",
    "crumbs": [
      "Semantic Search 101: Build a Neural Search Service"
    ]
  },
  {
    "objectID": "ss101_002_build_neural_search.html#upload-data-to-qdrant",
    "href": "ss101_002_build_neural_search.html#upload-data-to-qdrant",
    "title": "Semantic Search 101: Build a Neural Search Service",
    "section": "2 Upload data to qdrant",
    "text": "2 Upload data to qdrant\n\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.models import Distance, VectorParams\n\n\nclient = QdrantClient(\":memory:\")\n\n\nif not client.collection_exists(\"startups\"):\n    client.create_collection(\n        collection_name=\"startups\",\n        vectors_config=VectorParams(size=384, distance=Distance.COSINE),\n    )\n\n\nfd = open(\"startups_demo.json\")\n\n# Payload is now an iterator over startup data\npayload = map(json.loads, fd)\n\n# Load all vectors into memory, numpy array works as iterable afor itself.\n# Other options would be to use Mmap, if you don't want to load all data into RAM.\nvectors = np.load(\"startup_vectors.npy\")\n\n\nclient.upload_collection(\n    collection_name=\"startups\",\n    vectors=vectors,\n    payload=payload,\n    ids=None,  # Vector ids will be assigned automatically\n    batch_size=256,  # How many vectors will be uploaded in a single request?\n)\n\n/var/folders/v5/8v9k6wcn65jbbct8spl3wwsh0000gn/T/ipykernel_59718/2314715236.py:1: UserWarning: Local mode is not recommended for collections with more than 20,000 points. Current collection contains 40474 points. Consider using Qdrant in Docker or Qdrant Cloud for better performance with large datasets.\n  client.upload_collection(",
    "crumbs": [
      "Semantic Search 101: Build a Neural Search Service"
    ]
  },
  {
    "objectID": "ss101_002_build_neural_search.html#build-the-search-api",
    "href": "ss101_002_build_neural_search.html#build-the-search-api",
    "title": "Semantic Search 101: Build a Neural Search Service",
    "section": "3 Build the search API",
    "text": "3 Build the search API\n\ndef search(text: str):\n    # Convert text query into vector.\n    vector = list(embedding_model.embed(text))[0]\n\n    # Use `vector` to search for closest vectors in the collection.\n    search_result = client.query_points(\n        collection_name=\"startups\", query=vector, query_filter=None, limit=5\n    ).points\n\n    payloads = [hit.payload for hit in search_result]\n    return payloads\n\n\nsearch(\"robotic ai\")\n\n[{'name': 'Orchid Robotics',\n  'images': 'https://d1qb2nb5cznatu.cloudfront.net/startups/i/281423-bd01451f06bd2888ebf239580d502263-thumb_jpg.jpg?buster=1392329500',\n  'alt': 'Orchid Robotics -  robotics machine learning artificial intelligence industrial automation',\n  'description': \"Advanced Software for Robots\\nGoogle's machine learning techniques applied to robotics.\",\n  'link': 'http://www.orchidrobotics.com',\n  'city': 'Boston'},\n {'name': 'Robotbase',\n  'images': 'https://d1qb2nb5cznatu.cloudfront.net/startups/i/190707-d6f705aa7d803faa6bd5c33c281698b4-thumb_jpg.jpg?buster=1420224362',\n  'alt': 'Robotbase -  robotics artificial intelligence internet of things hardware + software',\n  'description': \"The World's First Artificial Intelligence Personal Robot\\nORDER NOW ON KICKSTARTER\\nhttps://www.kickstarter.com/projects/403524037/personal-robot\\nMeet the world’s first Artificial Intelligence Personal Robot. \\xa0\\nShe's a friend, a multi-talented personal assistant, an awesome photographer, a reliable security guard, a ...\",\n  'link': 'http://www.robotbase.com',\n  'city': 'New York'},\n {'name': 'Robots Alive - Simplified Robotics',\n  'images': 'https://d1qb2nb5cznatu.cloudfront.net/startups/i/477280-85f224c8ed6429bd9b65b33506aaa34e-thumb_jpg.jpg?buster=1409540159',\n  'alt': 'Robots Alive - Simplified Robotics -  manufacturing robotics industrial automation fmcg',\n  'description': 'Simplistic Industrial Robots for the SME\\nWe are a Bangalore based robotics technology firm working on simple and easy to use industrial robots for the SME\\nOur robots are equipped with intelligent software which allows the SME machine operator to program the application on the robot in a few minutes\\nHence ...',\n  'link': 'http://www.robots-alive.com',\n  'city': 'Bangalore'},\n {'name': 'Robotic Wares',\n  'images': 'https://d1qb2nb5cznatu.cloudfront.net/startups/i/484731-0aedee32a15b36c57132b952cbf15506-thumb_jpg.jpg?buster=1410386454',\n  'alt': 'Robotic Wares - ',\n  'description': 'Get the Latest News, Analysis, Opinion & Multimedia from the world of Business & Finance.',\n  'link': 'http://www.livemint.com/',\n  'city': 'New Delhi'},\n {'name': 'Neurala',\n  'images': 'https://d1qb2nb5cznatu.cloudfront.net/startups/i/136219-e6edc37bb162e0dc5ecb7629435010e0-thumb_jpg.jpg?buster=1352511607',\n  'alt': 'Neurala -  robotics artificial intelligence video conferencing software',\n  'description': 'Building brains for bots.\\nThe Neurala for telepresence software provides robots with learning, intelligence, and autonomy. Neurala frees you from driving a robot and instead allows you to focus on communicating and interacting with someone.\\nA robot preloaded with Neurala’s software will ...',\n  'link': 'http://neurala.com',\n  'city': 'Boston'}]\n\n\n\nsearch(\"financial\")\n\n[{'name': 'Finomial',\n  'images': 'https://d1qb2nb5cznatu.cloudfront.net/startups/i/65946-437a75bc2055ce89a40402860c2a53b6-thumb_jpg.jpg?buster=1408376313',\n  'alt': 'Finomial -  finance',\n  'description': '',\n  'link': 'http://finomial.com',\n  'city': 'New York'},\n {'name': 'U.S. Fiduciary',\n  'images': 'https://angel.co/images/shared/nopic_startup.png',\n  'alt': 'U.S. Fiduciary -  finance',\n  'description': '',\n  'link': 'http://www.usfiduciary.com',\n  'city': 'Houston'},\n {'name': 'American Express',\n  'images': 'https://d1qb2nb5cznatu.cloudfront.net/startups/i/42490-41d65934240d652086fde74873a443fa-thumb_jpg.jpg?buster=1326850941',\n  'alt': 'American Express -  finance',\n  'description': '',\n  'link': 'https://www.americanexpress.com',\n  'city': 'New York'},\n {'name': 'Paymentus',\n  'images': 'https://d1qb2nb5cznatu.cloudfront.net/startups/i/63392-f817eedec057e834b69dacb82c5c1f38-thumb_jpg.jpg?buster=1408893681',\n  'alt': 'Paymentus -  finance',\n  'description': '',\n  'link': 'http://www.paymentus.com',\n  'city': 'Atlanta'},\n {'name': 'FinancialAsk',\n  'images': 'https://d1qb2nb5cznatu.cloudfront.net/startups/i/367328-7805129c5017b6bd7fb575d17276c23a-thumb_jpg.jpg?buster=1417054915',\n  'alt': 'FinancialAsk -  personal finance finance technology',\n  'description': 'A financial advice marketplace\\nFinancialAsk makes financial advice easy to get via apps and the web. \\xa0It connects qualified financial advisers with consumers in a transparent marketplace that makes financial advices accessible and affordable.\\nUsers simply ask a financial question which is answered ...',\n  'link': 'http://www.andrewlai.org',\n  'city': 'Melbourne'}]",
    "crumbs": [
      "Semantic Search 101: Build a Neural Search Service"
    ]
  },
  {
    "objectID": "images/ar101_003_navigate_codebase.html",
    "href": "images/ar101_003_navigate_codebase.html",
    "title": "Advanced Retrieval 101: Navigate Your Codebase with Semantic Search and Qdrant",
    "section": "",
    "text": "https://qdrant.tech/documentation/advanced-tutorials/code-search/\n# !curl https://storage.googleapis.com/tutorial-attachments/code-search/structures.jsonl -O\n\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 4805k  100 4805k    0     0   913k      0  0:00:05  0:00:05 --:--:-- 1137k\n# !tail structures.jsonl\nimport json\n\nstructures = []\nwith open(\"structures.jsonl\", \"r\") as fp:\n    for i, row in enumerate(fp):\n        entry = json.loads(row)\n        structures.append(entry)",
    "crumbs": [
      "Images",
      "Advanced Retrieval 101: Navigate Your Codebase with Semantic Search and Qdrant"
    ]
  },
  {
    "objectID": "images/ar101_003_navigate_codebase.html#code-to-natural-language-conversion",
    "href": "images/ar101_003_navigate_codebase.html#code-to-natural-language-conversion",
    "title": "Advanced Retrieval 101: Navigate Your Codebase with Semantic Search and Qdrant",
    "section": "1 Code to natural language conversion",
    "text": "1 Code to natural language conversion\n\nimport re\nfrom typing import Any, Dict\n\nimport inflection\n\n\ndef textify(chunk: Dict[str, Any]) -&gt; str:\n    # Get rid of all the camel case / snake case\n    # - inflection.underscore changes the camel case to snake case\n    # - inflection.humanize converts the snake case to human readable form\n    name = inflection.humanize(inflection.underscore(chunk[\"name\"]))\n    signature = inflection.humanize(inflection.underscore(chunk[\"signature\"]))\n\n    # Check if docstring is provided\n    docstring = \"\"\n    if chunk[\"docstring\"]:\n        docstring = f\"that does {chunk['docstring']} \"\n\n    # Extract the location of that snippet of code\n    context = (\n        f\"module {chunk['context']['module']} \" f\"file {chunk['context']['file_name']}\"\n    )\n    if chunk[\"context\"][\"struct_name\"]:\n        struct_name = inflection.humanize(\n            inflection.underscore(chunk[\"context\"][\"struct_name\"])\n        )\n        context = f\"defined in struct {struct_name} {context}\"\n\n    # Combine all the bits and pieces together\n    text_representation = (\n        f\"{chunk['code_type']} {name} \"\n        f\"{docstring}\"\n        f\"defined as {signature} \"\n        f\"{context}\"\n    )\n\n    # Remove any special characters and concatenate the tokens\n    tokens = re.split(r\"\\W\", text_representation)\n    tokens = filter(lambda x: x, tokens)\n    return \" \".join(tokens)",
    "crumbs": [
      "Images",
      "Advanced Retrieval 101: Navigate Your Codebase with Semantic Search and Qdrant"
    ]
  },
  {
    "objectID": "images/ar101_003_navigate_codebase.html#natural-language-embeddings",
    "href": "images/ar101_003_navigate_codebase.html#natural-language-embeddings",
    "title": "Advanced Retrieval 101: Navigate Your Codebase with Semantic Search and Qdrant",
    "section": "2 Natural language embeddings",
    "text": "2 Natural language embeddings\n\ntext_representations = list(map(textify, structures))\n\n\nfrom sentence_transformers import SentenceTransformer\n\nnlp_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\nnlp_embeddings = nlp_model.encode(\n    text_representations,\n    show_progress_bar=True,\n)",
    "crumbs": [
      "Images",
      "Advanced Retrieval 101: Navigate Your Codebase with Semantic Search and Qdrant"
    ]
  },
  {
    "objectID": "images/ar101_003_navigate_codebase.html#code-embeddings",
    "href": "images/ar101_003_navigate_codebase.html#code-embeddings",
    "title": "Advanced Retrieval 101: Navigate Your Codebase with Semantic Search and Qdrant",
    "section": "3 Code embeddings",
    "text": "3 Code embeddings\n\n# Extract the code snippets from the structures to a separate list\ncode_snippets = [structure[\"context\"][\"snippet\"] for structure in structures]\ncode_model = SentenceTransformer(\n    \"jinaai/jina-embeddings-v2-base-code\", trust_remote_code=True\n)\ncode_model.max_seq_length = 8192  # increase the context length window\ncode_embeddings = code_model.encode(\n    code_snippets,\n    batch_size=4,\n    show_progress_bar=True,\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA new version of the following files was downloaded from https://huggingface.co/jinaai/jina-bert-v2-qk-post-norm:\n- configuration_bert.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n/Users/alextanhongpin/.cache/huggingface/modules/transformers_modules/jinaai/jina-bert-v2-qk-post-norm/3baf9e3ac750e76e8edd3019170176884695fb94/configuration_bert.py:29: UserWarning: optimum is not installed. To use OnnxConfig and BertOnnxConfig, make sure that `optimum` package is installed\n  warnings.warn(\"optimum is not installed. To use OnnxConfig and BertOnnxConfig, make sure that `optimum` package is installed\")\n\n\n\n\n\nA new version of the following files was downloaded from https://huggingface.co/jinaai/jina-bert-v2-qk-post-norm:\n- modeling_bert.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.",
    "crumbs": [
      "Images",
      "Advanced Retrieval 101: Navigate Your Codebase with Semantic Search and Qdrant"
    ]
  },
  {
    "objectID": "images/ar101_003_navigate_codebase.html#create-collection",
    "href": "images/ar101_003_navigate_codebase.html#create-collection",
    "title": "Advanced Retrieval 101: Navigate Your Codebase with Semantic Search and Qdrant",
    "section": "4 Create collection",
    "text": "4 Create collection\n\nfrom qdrant_client import QdrantClient, models\n\nclient = QdrantClient(\":memory:\")\nclient.create_collection(\n    \"qdrant-sources\",\n    vectors_config={\n        \"text\": models.VectorParams(\n            size=nlp_embeddings.shape[1],\n            distance=models.Distance.COSINE,\n        ),\n        \"code\": models.VectorParams(\n            size=code_embeddings.shape[1],\n            distance=models.Distance.COSINE,\n        ),\n    },\n)\n\nTrue\n\n\n\nimport uuid\n\npoints = [\n    models.PointStruct(\n        id=uuid.uuid4().hex,\n        vector={\n            \"text\": text_embedding,\n            \"code\": code_embedding,\n        },\n        payload=structure,\n    )\n    for text_embedding, code_embedding, structure in zip(\n        nlp_embeddings, code_embeddings, structures\n    )\n]\n\nclient.upload_points(\"qdrant-sources\", points=points, batch_size=64)",
    "crumbs": [
      "Images",
      "Advanced Retrieval 101: Navigate Your Codebase with Semantic Search and Qdrant"
    ]
  },
  {
    "objectID": "images/ar101_003_navigate_codebase.html#querying-the-codebase",
    "href": "images/ar101_003_navigate_codebase.html#querying-the-codebase",
    "title": "Advanced Retrieval 101: Navigate Your Codebase with Semantic Search and Qdrant",
    "section": "5 Querying the codebase",
    "text": "5 Querying the codebase\n\n5.1 Using text embedding\n\nquery = \"How do I count points in a collection?\"\n\nhits = client.query_points(\n    \"qdrant-sources\",\n    query=nlp_model.encode(query).tolist(),\n    using=\"text\",\n    limit=5,\n).points\n\n\nimport pprint\n\npprint.pp(hits[0].payload)\n\n{'name': 'count',\n 'signature': 'async fn count (& self , collection_name : & str , request : '\n              'CountRequestInternal , read_consistency : Option &lt; '\n              'ReadConsistency &gt; , shard_selection : ShardSelectorInternal ,) '\n              '-&gt; Result &lt; CountResult , StorageError &gt;',\n 'code_type': 'Function',\n 'docstring': '= \" Count points in the collection.\"',\n 'line': 120,\n 'line_from': 108,\n 'line_to': 132,\n 'context': {'module': 'toc',\n             'file_path': 'lib/storage/src/content_manager/toc/point_ops.rs',\n             'file_name': 'point_ops.rs',\n             'struct_name': 'TableOfContent',\n             'snippet': '    /// Count points in the collection.\\n'\n                        '    ///\\n'\n                        '    /// # Arguments\\n'\n                        '    ///\\n'\n                        '    /// * `collection_name` - in what collection do '\n                        'we count\\n'\n                        '    /// * `request` - [`CountRequestInternal`]\\n'\n                        '    /// * `shard_selection` - which local shard to '\n                        'use\\n'\n                        '    ///\\n'\n                        '    /// # Result\\n'\n                        '    ///\\n'\n                        '    /// Number of points in the collection.\\n'\n                        '    ///\\n'\n                        '    pub async fn count(\\n'\n                        '        &self,\\n'\n                        '        collection_name: &str,\\n'\n                        '        request: CountRequestInternal,\\n'\n                        '        read_consistency: Option&lt;ReadConsistency&gt;,\\n'\n                        '        shard_selection: ShardSelectorInternal,\\n'\n                        '    ) -&gt; Result&lt;CountResult, StorageError&gt; {\\n'\n                        '        let collection = '\n                        'self.get_collection(collection_name).await?;\\n'\n                        '        collection\\n'\n                        '            .count(request, read_consistency, '\n                        '&shard_selection)\\n'\n                        '            .await\\n'\n                        '            .map_err(|err| err.into())\\n'\n                        '    }\\n'}}\n\n\n\n\n5.2 Using code embedding\n\nhits = client.query_points(\n    \"qdrant-sources\",\n    query=code_model.encode(query).tolist(),\n    using=\"code\",\n    limit=5,\n).points\n\n\npprint.pp(hits[0].payload)\n\n{'name': 'count_indexed_points',\n 'signature': 'fn count_indexed_points (& self) -&gt; usize',\n 'code_type': 'Function',\n 'docstring': None,\n 'line': 612,\n 'line_from': 612,\n 'line_to': 614,\n 'context': {'module': 'field_index',\n             'file_path': 'lib/segment/src/index/field_index/geo_index.rs',\n             'file_name': 'geo_index.rs',\n             'struct_name': 'GeoMapIndex',\n             'snippet': '    fn count_indexed_points(&self) -&gt; usize {\\n'\n                        '        self.points_count()\\n'\n                        '    }\\n'}}\n\n\n\n\n5.3 Using text + code embedding\n\nresponses = client.query_batch_points(\n    \"qdrant-sources\",\n    requests=[\n        models.QueryRequest(\n            query=nlp_model.encode(query).tolist(),\n            using=\"text\",\n            with_payload=True,\n            limit=5,\n        ),\n        models.QueryRequest(\n            query=code_model.encode(query).tolist(),\n            using=\"code\",\n            with_payload=True,\n            limit=5,\n        ),\n    ],\n)\n\nresults = [response.points for response in responses]\n\n\npprint.pp(results[0][0].payload)\n\n{'name': 'count',\n 'signature': 'async fn count (& self , collection_name : & str , request : '\n              'CountRequestInternal , read_consistency : Option &lt; '\n              'ReadConsistency &gt; , shard_selection : ShardSelectorInternal ,) '\n              '-&gt; Result &lt; CountResult , StorageError &gt;',\n 'code_type': 'Function',\n 'docstring': '= \" Count points in the collection.\"',\n 'line': 120,\n 'line_from': 108,\n 'line_to': 132,\n 'context': {'module': 'toc',\n             'file_path': 'lib/storage/src/content_manager/toc/point_ops.rs',\n             'file_name': 'point_ops.rs',\n             'struct_name': 'TableOfContent',\n             'snippet': '    /// Count points in the collection.\\n'\n                        '    ///\\n'\n                        '    /// # Arguments\\n'\n                        '    ///\\n'\n                        '    /// * `collection_name` - in what collection do '\n                        'we count\\n'\n                        '    /// * `request` - [`CountRequestInternal`]\\n'\n                        '    /// * `shard_selection` - which local shard to '\n                        'use\\n'\n                        '    ///\\n'\n                        '    /// # Result\\n'\n                        '    ///\\n'\n                        '    /// Number of points in the collection.\\n'\n                        '    ///\\n'\n                        '    pub async fn count(\\n'\n                        '        &self,\\n'\n                        '        collection_name: &str,\\n'\n                        '        request: CountRequestInternal,\\n'\n                        '        read_consistency: Option&lt;ReadConsistency&gt;,\\n'\n                        '        shard_selection: ShardSelectorInternal,\\n'\n                        '    ) -&gt; Result&lt;CountResult, StorageError&gt; {\\n'\n                        '        let collection = '\n                        'self.get_collection(collection_name).await?;\\n'\n                        '        collection\\n'\n                        '            .count(request, read_consistency, '\n                        '&shard_selection)\\n'\n                        '            .await\\n'\n                        '            .map_err(|err| err.into())\\n'\n                        '    }\\n'}}\n\n\n\n\n5.4 Grouping the results\n\nresults = client.query_points_groups(\n    \"qdrant-sources\",\n    query=code_model.encode(query).tolist(),\n    using=\"code\",\n    group_by=\"context.module\",\n    limit=5,\n    group_size=1,\n)\n\n\npprint.pp(results.groups[0].hits[0].payload)\n\n{'name': 'count_indexed_points',\n 'signature': 'fn count_indexed_points (& self) -&gt; usize',\n 'code_type': 'Function',\n 'docstring': None,\n 'line': 612,\n 'line_from': 612,\n 'line_to': 614,\n 'context': {'module': 'field_index',\n             'file_path': 'lib/segment/src/index/field_index/geo_index.rs',\n             'file_name': 'geo_index.rs',\n             'struct_name': 'GeoMapIndex',\n             'snippet': '    fn count_indexed_points(&self) -&gt; usize {\\n'\n                        '        self.points_count()\\n'\n                        '    }\\n'}}",
    "crumbs": [
      "Images",
      "Advanced Retrieval 101: Navigate Your Codebase with Semantic Search and Qdrant"
    ]
  },
  {
    "objectID": "ar101_001_collaborative_filtering.html",
    "href": "ar101_001_collaborative_filtering.html",
    "title": "Advance Retrieval 101: Use Collaborative Filtering to Build a Movie Recommendation System",
    "section": "",
    "text": "https://qdrant.tech/documentation/advanced-tutorials/collaborative-filtering/\nfrom collections import defaultdict\n\nimport pandas as pd\nfrom qdrant_client import QdrantClient, models\nfrom qdrant_client.models import NamedSparseVector, PointStruct, SparseVector",
    "crumbs": [
      "Advance Retrieval 101: Use Collaborative Filtering to Build a Movie Recommendation System"
    ]
  },
  {
    "objectID": "ar101_001_collaborative_filtering.html#prepare-the-data",
    "href": "ar101_001_collaborative_filtering.html#prepare-the-data",
    "title": "Advance Retrieval 101: Use Collaborative Filtering to Build a Movie Recommendation System",
    "section": "1 Prepare the data",
    "text": "1 Prepare the data\n\n# Load CSV file\nratings_df = pd.read_csv(\"data/ml-latest-small/ratings.csv\", low_memory=False)\nmovies_df = pd.read_csv(\"data/ml-latest-small/movies.csv\", low_memory=False)\n\n# Convert movieId in ratings_df and movies_df to string\nratings_df[\"movieId\"] = ratings_df[\"movieId\"].astype(str)\nmovies_df[\"movieId\"] = movies_df[\"movieId\"].astype(str)\n\nrating = ratings_df[\"rating\"]\n\n# Normalize ratings\nratings_df[\"rating\"] = (rating - rating.mean()) / rating.std()\n\n# Merge rating with movie metadata to get movie titles\nmerged_df = ratings_df.merge(\n    movies_df[[\"movieId\", \"title\"]], left_on=\"movieId\", right_on=\"movieId\", how=\"inner\"\n)\n\n# Aggregate ratings to handle duplicate (userId, title) pairs\nratings_agg_df = merged_df.groupby([\"userId\", \"movieId\"]).rating.mean().reset_index()\nratings_agg_df.head()\n\n\n\n\n\n\n\n\nuserId\nmovieId\nrating\n\n\n\n\n0\n1\n1\n0.478109\n\n\n1\n1\n1009\n-0.481096\n\n\n2\n1\n101\n1.437315\n\n\n3\n1\n1023\n1.437315\n\n\n4\n1\n1024\n1.437315",
    "crumbs": [
      "Advance Retrieval 101: Use Collaborative Filtering to Build a Movie Recommendation System"
    ]
  },
  {
    "objectID": "ar101_001_collaborative_filtering.html#convert-to-sparse",
    "href": "ar101_001_collaborative_filtering.html#convert-to-sparse",
    "title": "Advance Retrieval 101: Use Collaborative Filtering to Build a Movie Recommendation System",
    "section": "2 Convert to Sparse",
    "text": "2 Convert to Sparse\n\n# Convert ratings to sparse vectors.\nuser_sparse_vectors = defaultdict(lambda: {\"values\": [], \"indices\": []})\nfor row in ratings_agg_df.itertuples():\n    user_sparse_vectors[row.userId][\"values\"].append(row.rating)\n    user_sparse_vectors[row.userId][\"indices\"].append(int(row.movieId))",
    "crumbs": [
      "Advance Retrieval 101: Use Collaborative Filtering to Build a Movie Recommendation System"
    ]
  },
  {
    "objectID": "ar101_001_collaborative_filtering.html#upload-the-data",
    "href": "ar101_001_collaborative_filtering.html#upload-the-data",
    "title": "Advance Retrieval 101: Use Collaborative Filtering to Build a Movie Recommendation System",
    "section": "3 Upload the data",
    "text": "3 Upload the data\n\ndef data_generator():\n    for user_id, sparse_vector in user_sparse_vectors.items():\n        yield PointStruct(\n            id=user_id,\n            vector={\n                \"ratings\": SparseVector(\n                    indices=sparse_vector[\"indices\"], values=sparse_vector[\"values\"]\n                )\n            },\n            payload={\"user_id\": user_id, \"movie_id\": sparse_vector[\"indices\"]},\n        )\n\n\nclient = QdrantClient(\":memory:\")\nclient.create_collection(\n    collection_name=\"movies\",\n    sparse_vectors_config={\"ratings\": models.SparseVectorParams()},\n    vectors_config={},\n)\nclient.upload_points(\n    collection_name=\"movies\",\n    points=data_generator(),\n)",
    "crumbs": [
      "Advance Retrieval 101: Use Collaborative Filtering to Build a Movie Recommendation System"
    ]
  },
  {
    "objectID": "ar101_001_collaborative_filtering.html#define-query",
    "href": "ar101_001_collaborative_filtering.html#define-query",
    "title": "Advance Retrieval 101: Use Collaborative Filtering to Build a Movie Recommendation System",
    "section": "4 Define query",
    "text": "4 Define query\n\nmy_ratings = {\n    603: 1,  # Matrix\n    13475: 1,  # Star Trek\n    11: 1,  # Star Wars\n    1091: -1,  # The Thing\n    862: 1,  # Toy Story\n    597: -1,  # Titanic\n    680: -1,  # Pulp Fiction\n    13: 1,  # Forrest Gump\n    120: 1,  # Lord of the Rings\n    87: -1,  # Indiana Jones\n    562: -1,  # Die Hard\n}\n\n\n# Create sparse vector from my_ratings\ndef to_vector(ratings):\n    vector = SparseVector(values=[], indices=[])\n    for movie_id, rating in ratings.items():\n        vector.values.append(rating)\n        vector.indices.append(movie_id)\n    return vector",
    "crumbs": [
      "Advance Retrieval 101: Use Collaborative Filtering to Build a Movie Recommendation System"
    ]
  },
  {
    "objectID": "ar101_001_collaborative_filtering.html#run-the-query",
    "href": "ar101_001_collaborative_filtering.html#run-the-query",
    "title": "Advance Retrieval 101: Use Collaborative Filtering to Build a Movie Recommendation System",
    "section": "5 Run the query",
    "text": "5 Run the query\n\n# Perform the search\nresults = client.query_points(\n    collection_name=\"movies\", query=to_vector(my_ratings), using=\"ratings\", limit=20\n).points\n\n\n# Convert results to scores and sort by score\ndef results_to_scores(results):\n    movie_scores = defaultdict(lambda: 0)\n    for result in results:\n        for movie_id in result.payload[\"movie_id\"]:\n            movie_scores[movie_id] += result.score\n    return movie_scores\n\n\n# Convert results to scores and sort by score\nmovie_scores = results_to_scores(results)\ntop_movies = sorted(movie_scores.items(), key=lambda x: x[1], reverse=True)\n\n\nfor movieId, score in top_movies[:5]:\n    movie = movies_df[movies_df['movieId'] == str(movieId)]\n    print(movie.title.values[0], score)\n\nForrest Gump (1994) 44.63442826271057\nPulp Fiction (1994) 43.19412624835968\nPretty Woman (1990) 42.71452331542969\nStar Wars: Episode V - The Empire Strikes Back (1980) 37.91998839378357\nAmerican Beauty (1999) 37.91849493980408",
    "crumbs": [
      "Advance Retrieval 101: Use Collaborative Filtering to Build a Movie Recommendation System"
    ]
  },
  {
    "objectID": "007_reranking_fastembed.html",
    "href": "007_reranking_fastembed.html",
    "title": "Reranking with fastembed",
    "section": "",
    "text": "https://qdrant.tech/documentation/fastembed/fastembed-rerankers/\nfrom fastembed import TextEmbedding\nfrom fastembed.rerank.cross_encoder import TextCrossEncoder\n[model[\"model\"] for model in TextCrossEncoder.list_supported_models()]\n\n['Xenova/ms-marco-MiniLM-L-6-v2',\n 'Xenova/ms-marco-MiniLM-L-12-v2',\n 'BAAI/bge-reranker-base',\n 'jinaai/jina-reranker-v1-tiny-en',\n 'jinaai/jina-reranker-v1-turbo-en',\n 'jinaai/jina-reranker-v2-base-multilingual']\ndense_embedding_model = TextEmbedding(\"sentence-transformers/all-MiniLM-L6-v2\")\nreranker = TextCrossEncoder(model_name=\"jinaai/jina-reranker-v2-base-multilingual\")",
    "crumbs": [
      "Reranking with fastembed"
    ]
  },
  {
    "objectID": "007_reranking_fastembed.html#embed-and-index-data-for-the-first-stage-retrieval",
    "href": "007_reranking_fastembed.html#embed-and-index-data-for-the-first-stage-retrieval",
    "title": "Reranking with fastembed",
    "section": "1 Embed and index data for the first-stage retrieval",
    "text": "1 Embed and index data for the first-stage retrieval\n\ndescriptions = [\n    \"In 1431, Jeanne d'Arc is placed on trial on charges of heresy. The ecclesiastical jurists attempt to force Jeanne to recant her claims of holy visions.\",\n    \"A film projectionist longs to be a detective, and puts his meagre skills to work when he is framed by a rival for stealing his girlfriend's father's pocketwatch.\",\n    \"A group of high-end professional thieves start to feel the heat from the LAPD when they unknowingly leave a clue at their latest heist.\",\n    \"A petty thief with an utter resemblance to a samurai warlord is hired as the lord's double. When the warlord later dies the thief is forced to take up arms in his place.\",\n    \"A young boy named Kubo must locate a magical suit of armour worn by his late father in order to defeat a vengeful spirit from the past.\",\n    \"A biopic detailing the 2 decades that Punjabi Sikh revolutionary Udham Singh spent planning the assassination of the man responsible for the Jallianwala Bagh massacre.\",\n    \"When a machine that allows therapists to enter their patients' dreams is stolen, all hell breaks loose. Only a young female therapist, Paprika, can stop it.\",\n    \"An ordinary word processor has the worst night of his life after he agrees to visit a girl in Soho whom he met that evening at a coffee shop.\",\n    \"A story that revolves around drug abuse in the affluent north Indian State of Punjab and how the youth there have succumbed to it en-masse resulting in a socio-economic decline.\",\n    \"A world-weary political journalist picks up the story of a woman's search for her son, who was taken away from her decades ago after she became pregnant and was forced to live in a convent.\",\n    \"Concurrent theatrical ending of the TV series Neon Genesis Evangelion (1995).\",\n    \"During World War II, a rebellious U.S. Army Major is assigned a dozen convicted murderers to train and lead them into a mass assassination mission of German officers.\",\n    \"The toys are mistakenly delivered to a day-care center instead of the attic right before Andy leaves for college, and it's up to Woody to convince the other toys that they weren't abandoned and to return home.\",\n    \"A soldier fighting aliens gets to relive the same day over and over again, the day restarting every time he dies.\",\n    \"After two male musicians witness a mob hit, they flee the state in an all-female band disguised as women, but further complications set in.\",\n    \"Exiled into the dangerous forest by her wicked stepmother, a princess is rescued by seven dwarf miners who make her part of their household.\",\n    \"A renegade reporter trailing a young runaway heiress for a big story joins her on a bus heading from Florida to New York, and they end up stuck with each other when the bus leaves them behind at one of the stops.\",\n    \"Story of 40-man Turkish task force who must defend a relay station.\",\n    \"Spinal Tap, one of England's loudest bands, is chronicled by film director Marty DiBergi on what proves to be a fateful tour.\",\n    \"Oskar, an overlooked and bullied boy, finds love and revenge through Eli, a beautiful but peculiar girl.\",\n]\n\n\ndescriptions_embeddings = list(dense_embedding_model.embed(descriptions))\nlen(descriptions_embeddings[0])\n\n384\n\n\n\nfrom qdrant_client import QdrantClient, models\n\nqdrant_client = QdrantClient(\":memory:\")  # Qdrant is running from RAM.\n\n\nqdrant_client.create_collection(\n    collection_name=\"movies\",\n    vectors_config={\n        \"embedding\": models.VectorParams(\n            size=384, distance=models.Distance.COSINE  # Size of all-MiniLM-L6-v2\n        )\n    },\n)\n\nTrue\n\n\n\nqdrant_client.upload_points(\n    collection_name=\"movies\",\n    points=[\n        models.PointStruct(\n            id=idx, payload={\"description\": description}, vector={\"embedding\": vector}\n        )\n        for idx, (description, vector) in enumerate(\n            zip(descriptions, descriptions_embeddings)\n        )\n    ],\n)",
    "crumbs": [
      "Reranking with fastembed"
    ]
  },
  {
    "objectID": "007_reranking_fastembed.html#first-stage-retrieval",
    "href": "007_reranking_fastembed.html#first-stage-retrieval",
    "title": "Reranking with fastembed",
    "section": "2 First-stage retrieval",
    "text": "2 First-stage retrieval\n\nquery = \"A story about a strong historically significant female figure.\"\nquery_embedded = list(dense_embedding_model.query_embed(query))[0]\n\ninitial_retrieval = qdrant_client.query_points(\n    collection_name=\"movies\",\n    using=\"embedding\",\n    query=query_embedded,\n    with_payload=True,\n    limit=10,\n)\n\n\ndescription_hits = []\nfor i, hit in enumerate(initial_retrieval.points):\n    print(f'Result number {i+1} is \"{hit.payload['description']}\"')\n    description_hits.append(hit.payload[\"description\"])\n\nResult number 1 is \"A world-weary political journalist picks up the story of a woman's search for her son, who was taken away from her decades ago after she became pregnant and was forced to live in a convent.\"\nResult number 2 is \"Exiled into the dangerous forest by her wicked stepmother, a princess is rescued by seven dwarf miners who make her part of their household.\"\nResult number 3 is \"Oskar, an overlooked and bullied boy, finds love and revenge through Eli, a beautiful but peculiar girl.\"\nResult number 4 is \"A renegade reporter trailing a young runaway heiress for a big story joins her on a bus heading from Florida to New York, and they end up stuck with each other when the bus leaves them behind at one of the stops.\"\nResult number 5 is \"A story that revolves around drug abuse in the affluent north Indian State of Punjab and how the youth there have succumbed to it en-masse resulting in a socio-economic decline.\"\nResult number 6 is \"After two male musicians witness a mob hit, they flee the state in an all-female band disguised as women, but further complications set in.\"\nResult number 7 is \"When a machine that allows therapists to enter their patients' dreams is stolen, all hell breaks loose. Only a young female therapist, Paprika, can stop it.\"\nResult number 8 is \"An ordinary word processor has the worst night of his life after he agrees to visit a girl in Soho whom he met that evening at a coffee shop.\"\nResult number 9 is \"A biopic detailing the 2 decades that Punjabi Sikh revolutionary Udham Singh spent planning the assassination of the man responsible for the Jallianwala Bagh massacre.\"\nResult number 10 is \"In 1431, Jeanne d'Arc is placed on trial on charges of heresy. The ecclesiastical jurists attempt to force Jeanne to recant her claims of holy visions.\"\n\n\n\nnew_scores = list(reranker.rerank(query, description_hits))\nnew_scores\n\n[-1.7871119976043701,\n -1.1165943145751953,\n -1.2816978693008423,\n -1.8776179552078247,\n -2.721012830734253,\n -2.2455098628997803,\n -2.380913257598877,\n -2.8468973636627197,\n -2.8494515419006348,\n -0.6084094047546387]\n\n\n\nranking = [(i, score) for i, score in enumerate(new_scores)]\nranking.sort(key=lambda x: x[1], reverse=True)\n\nfor i, rank in enumerate(ranking):\n    print(f'Reranked result number {i+1} is \"{description_hits[rank[0]]}\"')\n\nReranked result number 1 is \"In 1431, Jeanne d'Arc is placed on trial on charges of heresy. The ecclesiastical jurists attempt to force Jeanne to recant her claims of holy visions.\"\nReranked result number 2 is \"Exiled into the dangerous forest by her wicked stepmother, a princess is rescued by seven dwarf miners who make her part of their household.\"\nReranked result number 3 is \"Oskar, an overlooked and bullied boy, finds love and revenge through Eli, a beautiful but peculiar girl.\"\nReranked result number 4 is \"A world-weary political journalist picks up the story of a woman's search for her son, who was taken away from her decades ago after she became pregnant and was forced to live in a convent.\"\nReranked result number 5 is \"A renegade reporter trailing a young runaway heiress for a big story joins her on a bus heading from Florida to New York, and they end up stuck with each other when the bus leaves them behind at one of the stops.\"\nReranked result number 6 is \"After two male musicians witness a mob hit, they flee the state in an all-female band disguised as women, but further complications set in.\"\nReranked result number 7 is \"When a machine that allows therapists to enter their patients' dreams is stolen, all hell breaks loose. Only a young female therapist, Paprika, can stop it.\"\nReranked result number 8 is \"A story that revolves around drug abuse in the affluent north Indian State of Punjab and how the youth there have succumbed to it en-masse resulting in a socio-economic decline.\"\nReranked result number 9 is \"An ordinary word processor has the worst night of his life after he agrees to visit a girl in Soho whom he met that evening at a coffee shop.\"\nReranked result number 10 is \"A biopic detailing the 2 decades that Punjabi Sikh revolutionary Udham Singh spent planning the assassination of the man responsible for the Jallianwala Bagh massacre.\"",
    "crumbs": [
      "Reranking with fastembed"
    ]
  },
  {
    "objectID": "005_splade.html",
    "href": "005_splade.html",
    "title": "How to generate Sparse Vectors with SPLADE",
    "section": "",
    "text": "https://qdrant.tech/documentation/fastembed/fastembed-splade/\nfrom fastembed import SparseEmbedding, SparseTextEmbedding\nmodels = SparseTextEmbedding.list_supported_models()\nmodels[0]\n\n{'model': 'prithivida/Splade_PP_en_v1',\n 'sources': {'hf': 'Qdrant/SPLADE_PP_en_v1', 'url': None},\n 'model_file': 'model.onnx',\n 'description': 'Independent Implementation of SPLADE++ Model for English.',\n 'license': 'apache-2.0',\n 'size_in_GB': 0.532,\n 'additional_files': [],\n 'requires_idf': None,\n 'vocab_size': 30522}\nmodel_name = \"prithivida/Splade_PP_en_v1\"\nmodel = SparseTextEmbedding(model_name=model_name)",
    "crumbs": [
      "How to generate Sparse Vectors with SPLADE"
    ]
  },
  {
    "objectID": "005_splade.html#embed-data",
    "href": "005_splade.html#embed-data",
    "title": "How to generate Sparse Vectors with SPLADE",
    "section": "1 Embed data",
    "text": "1 Embed data\n\ndocuments: list[str] = [\n    \"Chandrayaan-3 is India's third lunar mission\",\n    \"It aimed to land a rover on the Moon's surface - joining the US, China and Russia\",\n    \"The mission is a follow-up to Chandrayaan-2, which had partial success\",\n    \"Chandrayaan-3 will be launched by the Indian Space Research Organisation (ISRO)\",\n    \"The estimated cost of the mission is around $35 million\",\n    \"It will carry instruments to study the lunar surface and atmosphere\",\n    \"Chandrayaan-3 landed on the Moon's surface on 23rd August 2023\",\n    \"It consists of a lander named Vikram and a rover named Pragyan similar to Chandrayaan-2. Its propulsion module would act like an orbiter.\",\n    \"The propulsion module carries the lander and rover configuration until the spacecraft is in a 100-kilometre (62 mi) lunar orbit\",\n    \"The mission used GSLV Mk III rocket for its launch\",\n    \"Chandrayaan-3 was launched from the Satish Dhawan Space Centre in Sriharikota\",\n    \"Chandrayaan-3 was launched earlier in the year 2023\",\n]\n\n\nsparse_embeddings_list: list[SparseEmbedding] = list(\n    model.embed(documents, batch_size=6)\n)",
    "crumbs": [
      "How to generate Sparse Vectors with SPLADE"
    ]
  },
  {
    "objectID": "005_splade.html#retrieve-embedding",
    "href": "005_splade.html#retrieve-embedding",
    "title": "How to generate Sparse Vectors with SPLADE",
    "section": "2 Retrieve Embedding",
    "text": "2 Retrieve Embedding\n\nindex = 0\nsparse_embeddings_list[index]\n\nSparseEmbedding(values=array([0.05297276, 0.01963477, 0.3645905 , 1.38508415, 0.7177667 ,\n       0.12668137, 0.46230468, 0.44676718, 0.26896986, 1.01519763,\n       1.56553161, 0.29411644, 1.53102267, 0.59785521, 1.10018086,\n       0.02078829, 0.09955899, 0.44248503, 0.09748027, 1.53519893,\n       1.36765647, 0.15741006, 0.49882478, 0.38628468, 0.76612252,\n       1.2580502 , 0.39058524, 0.27236614, 0.45152271, 0.48261923,\n       0.26085106, 1.35912812, 0.70710599, 1.71639597]), indices=array([ 1010,  1011,  1016,  1017,  2001,  2018,  2034,  2093,  2117,\n        2319,  2353,  2509,  2634,  2686,  2796,  2817,  2922,  2959,\n        3003,  3148,  3260,  3390,  3462,  3523,  3822,  4231,  4316,\n        4774,  5590,  5871,  6416, 11926, 12076, 16469]))",
    "crumbs": [
      "How to generate Sparse Vectors with SPLADE"
    ]
  },
  {
    "objectID": "005_splade.html#examine-weights",
    "href": "005_splade.html#examine-weights",
    "title": "How to generate Sparse Vectors with SPLADE",
    "section": "3 Examine weights",
    "text": "3 Examine weights\n\nfor i in range(5):\n    print(\n        f\"Token at index {sparse_embeddings_list[0].indices[i]} has weight {sparse_embeddings_list[0].values[i]}\"\n    )\n\nToken at index 1010 has weight 0.05297275632619858\nToken at index 1011 has weight 0.01963476650416851\nToken at index 1016 has weight 0.36459049582481384\nToken at index 1017 has weight 1.3850841522216797\nToken at index 2001 has weight 0.7177667021751404",
    "crumbs": [
      "How to generate Sparse Vectors with SPLADE"
    ]
  },
  {
    "objectID": "005_splade.html#analyze-results",
    "href": "005_splade.html#analyze-results",
    "title": "How to generate Sparse Vectors with SPLADE",
    "section": "4 Analyze results",
    "text": "4 Analyze results\n\nimport json\n\nfrom tokenizers import Tokenizer\n\ntokenizer = Tokenizer.from_pretrained(\n    SparseTextEmbedding.list_supported_models()[0][\"sources\"][\"hf\"]\n)\n\n\n\n\n\ndef get_tokens_and_weights(sparse_embedding, tokenizer):\n    token_weight_dict = {}\n    for i in range(len(sparse_embedding.indices)):\n        token = tokenizer.decode([sparse_embedding.indices[i]])\n        weight = sparse_embedding.values[i]\n        token_weight_dict[token] = weight\n\n    # Sort the dictionary by weights.\n    token_weight_dict = dict(\n        sorted(token_weight_dict.items(), key=lambda item: item[1], reverse=True)\n    )\n    return token_weight_dict\n\n\nprint(\n    json.dumps(\n        get_tokens_and_weights(sparse_embeddings_list[index], tokenizer), indent=4\n    )\n)\n\n{\n    \"chandra\": 1.7163959741592407,\n    \"third\": 1.565531611442566,\n    \"##ya\": 1.5351989269256592,\n    \"india\": 1.5310226678848267,\n    \"3\": 1.3850841522216797,\n    \"mission\": 1.3676564693450928,\n    \"lunar\": 1.3591281175613403,\n    \"moon\": 1.2580502033233643,\n    \"indian\": 1.1001808643341064,\n    \"##an\": 1.0151976346969604,\n    \"3rd\": 0.7661225199699402,\n    \"was\": 0.7177667021751404,\n    \"spacecraft\": 0.7071059942245483,\n    \"space\": 0.5978552103042603,\n    \"flight\": 0.4988247752189636,\n    \"satellite\": 0.4826192259788513,\n    \"first\": 0.4623046815395355,\n    \"expedition\": 0.45152270793914795,\n    \"three\": 0.4467671811580658,\n    \"fourth\": 0.4424850344657898,\n    \"vehicle\": 0.3905852437019348,\n    \"iii\": 0.3862846791744232,\n    \"2\": 0.36459049582481384,\n    \"##3\": 0.29411643743515015,\n    \"planet\": 0.27236613631248474,\n    \"second\": 0.268969863653183,\n    \"missions\": 0.26085105538368225,\n    \"launched\": 0.15741005539894104,\n    \"had\": 0.1266813725233078,\n    \"largest\": 0.0995589941740036,\n    \"leader\": 0.09748027473688126,\n    \",\": 0.05297275632619858,\n    \"study\": 0.02078828774392605,\n    \"-\": 0.01963476650416851\n}",
    "crumbs": [
      "How to generate Sparse Vectors with SPLADE"
    ]
  },
  {
    "objectID": "003_fastembed_text_embeddings.html",
    "href": "003_fastembed_text_embeddings.html",
    "title": "How to generate Text Embeddings with FastEmbed",
    "section": "",
    "text": "from fastembed import TextEmbedding\ndocuments: [str] = [\n    \"FastEmbed is lighter than Transformers & Sentence-Transformers.\",\n    \"FastEmbed is supported by and maintained by Qdrant.\",\n    \"I have a cat\",\n]\nembedding_model = TextEmbedding()\nembedding_model.model_name\n\n'BAAI/bge-small-en-v1.5'\nembeddings_generator = embedding_model.embed(documents)\nembeddings_list = list(embeddings_generator)\nembeddings_list[0][:10]\n\narray([-0.09479211,  0.01008398, -0.03087804,  0.02379127,  0.00236447,\n        0.00065356, -0.08248352,  0.00084713,  0.03719218,  0.01438666],\n      dtype=float32)",
    "crumbs": [
      "How to generate Text Embeddings with FastEmbed"
    ]
  },
  {
    "objectID": "003_fastembed_text_embeddings.html#calculating-similarity",
    "href": "003_fastembed_text_embeddings.html#calculating-similarity",
    "title": "How to generate Text Embeddings with FastEmbed",
    "section": "1 Calculating similarity",
    "text": "1 Calculating similarity\n\nimport numpy as np\nfrom qdrant_client import models\nfrom qdrant_client.local.distances import (\n    calculate_distance,\n    cosine_similarity,\n    dot_product,\n    euclidean_distance,\n    manhattan_distance,\n)\n\n\ndistance_types = [\n    cosine_similarity,\n    dot_product,\n    euclidean_distance,\n    manhattan_distance,\n]\n\n\nembeddings = np.array(embeddings_list)\n\nfor distance_type in distance_types:\n    print(distance_type.__name__, distance_type(embeddings[:1], embeddings[1:]))\n\ncosine_similarity [[0.6716511 0.4618737]]\ndot_product [[0.6716511 0.4618737]]\neuclidean_distance [[0.810369 1.037426]]\nmanhattan_distance [[12.591358 16.071075]]\n\n\n\nfor distance_type in list(models.Distance):\n    print(\n        distance_type, calculate_distance(embeddings[:1], embeddings[1:], distance_type)\n    )\n\nCosine [[0.6716511 0.4618737]]\nEuclid [[0.810369 1.037426]]\nDot [[0.6716511 0.4618737]]\nManhattan [[12.591358 16.071075]]",
    "crumbs": [
      "How to generate Text Embeddings with FastEmbed"
    ]
  },
  {
    "objectID": "003_fastembed_text_embeddings.html#finding-similarity",
    "href": "003_fastembed_text_embeddings.html#finding-similarity",
    "title": "How to generate Text Embeddings with FastEmbed",
    "section": "2 Finding similarity",
    "text": "2 Finding similarity\n\nquery = \"I need a pet\"\n\nembeddings_generator = embedding_model.embed(query)\nembeddings_list = list(embeddings_generator)\n\n\nscores = calculate_distance(\n    np.array(embeddings_list), embeddings, distance_type=models.Distance.COSINE\n)\nscores\n\narray([[0.433192  , 0.39380783, 0.77847326]], dtype=float32)\n\n\n\nrecommendations = sorted(zip(documents, scores[0]), key=lambda x: x[1], reverse=True)\nfor doc, score in recommendations:\n    print(doc, score)\n\nI have a cat 0.77847326\nFastEmbed is lighter than Transformers & Sentence-Transformers. 0.433192\nFastEmbed is supported by and maintained by Qdrant. 0.39380783",
    "crumbs": [
      "How to generate Text Embeddings with FastEmbed"
    ]
  },
  {
    "objectID": "001_basic.html",
    "href": "001_basic.html",
    "title": "Dense text embedding",
    "section": "",
    "text": "%load_ext jupyter_ai\n%config AiMagics.default_language_model = \"ollama:llama3.2\"\n\nThe jupyter_ai extension is already loaded. To reload it, use:\n  %reload_ext jupyter_ai\n%%ai\n\n\"Explain dense text embedding, keep it simple. Demonstrate some example. Give some real-world example\"\n\n1 Dense Text Embeddings\n\n1.0.1 What is a Dense Vector?\nA dense vector is a high-dimensional numerical representation of a piece of text.\n\n\n1.0.2 How Does it Work?\nImagine you’re looking at a word and thinking of related words, like “apple” and “fruit”. A dense vector does something similar for an entire paragraph or sentence, but instead of words, it uses numerical values to represent the relationships between all the words in the text.\n\n\n1.1 ### Example\nLet’s say we have a piece of text: “The quick brown fox jumps over the lazy dog.”\nWe can represent this as two vectors:\n\nVector A: [0.1, 0.2, 0.3, ...] (related to the word “quick”)\nVector B: [0.4, 0.5, 0.6, ...] (related to the word “brown”)\n\nOur dense vector would be a combination of these two vectors.\n\n\n1.2 ### Real-World Examples\n\nText Classification: Dense embeddings can be used as input features for text classification models, such as sentiment analysis or spam detection.\nTopic Modeling: Dense embeddings can help identify topics in large collections of text by capturing the underlying relationships between words.\nInformation Retrieval: Dense embeddings can improve search results by representing documents and queries as dense vectors, allowing for more accurate matches.\n\n\n\n1.3 ### Code Example\n# Import necessary libraries\nimport numpy as np\n\n# Define a function to create a simple dense vector representation\ndef create_dense_vector(text):\n    # Split the text into words\n    words = text.split()\n\n    # Create a dictionary to store word vectors\n    word_vectors = {}\n\n    # Add each word to the dictionary\n    for i, word in enumerate(words):\n        # Use a random numerical value as the vector for simplicity\n        word_vector = np.random.rand(768)\n        word_vectors[word] = word_vector\n\n    # Combine all word vectors into a single dense vector\n    dense_vector = np.array([word_vectors[word] for word in words])\n\n    return dense_vector\n\n# Test the function\ntext = \"The quick brown fox jumps over the lazy dog.\"\ndense_vector = create_dense_vector(text)\nprint(dense_vector.shape)  # Output: (1, 768)\nIn this example, we use a simple dictionary to store word vectors and then combine them into a single dense vector.\nfrom fastembed import TextEmbedding\n\n# Example list of documents\ndocuments: list[str] = [\n    \"This is built to be faster and lighter than other embedding libraries e.g. Transformers, Sentence-Transformers, etc.\",\n    \"fastembed is supported by and maintained by Qdrant.\",\n]\n\n# This will trigger the model download and initialization\nembedding_model = TextEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\nprint(f\"The model {embedding_model.model_name} is ready to use.\")\n\nembeddings_generator = embedding_model.embed(documents)  # reminder this is a generator\nembeddings_list = list(embedding_model.embed(documents))\n# you can also convert the generator to a list, and that to a numpy array\nlen(embeddings_list[0])  # Vector of 384 dimensions\n\nThe model BAAI/bge-small-en-v1.5 is ready to use.\n\n\n384\nlist(filter(lambda x: not x.startswith(\"__\"), dir(embedding_model)))\n\n['EMBEDDINGS_REGISTRY',\n 'METADATA_FILE',\n '_get_model_description',\n '_list_supported_models',\n '_local_files_only',\n 'add_custom_model',\n 'cache_dir',\n 'decompress_to_cache',\n 'download_file_from_gcs',\n 'download_files_from_huggingface',\n 'download_model',\n 'embed',\n 'list_supported_models',\n 'model',\n 'model_name',\n 'passage_embed',\n 'query_embed',\n 'retrieve_model_gcs',\n 'threads']\nembedding_model.model_name\n\n'BAAI/bge-small-en-v1.5'",
    "crumbs": [
      "Dense text embedding"
    ]
  },
  {
    "objectID": "001_basic.html#example",
    "href": "001_basic.html#example",
    "title": "Dense text embedding",
    "section": "1.1 ### Example",
    "text": "1.1 ### Example\nLet’s say we have a piece of text: “The quick brown fox jumps over the lazy dog.”\nWe can represent this as two vectors:\n\nVector A: [0.1, 0.2, 0.3, ...] (related to the word “quick”)\nVector B: [0.4, 0.5, 0.6, ...] (related to the word “brown”)\n\nOur dense vector would be a combination of these two vectors.",
    "crumbs": [
      "Dense text embedding"
    ]
  },
  {
    "objectID": "001_basic.html#real-world-examples",
    "href": "001_basic.html#real-world-examples",
    "title": "Dense text embedding",
    "section": "1.2 ### Real-World Examples",
    "text": "1.2 ### Real-World Examples\n\nText Classification: Dense embeddings can be used as input features for text classification models, such as sentiment analysis or spam detection.\nTopic Modeling: Dense embeddings can help identify topics in large collections of text by capturing the underlying relationships between words.\nInformation Retrieval: Dense embeddings can improve search results by representing documents and queries as dense vectors, allowing for more accurate matches.",
    "crumbs": [
      "Dense text embedding"
    ]
  },
  {
    "objectID": "001_basic.html#code-example",
    "href": "001_basic.html#code-example",
    "title": "Dense text embedding",
    "section": "1.3 ### Code Example",
    "text": "1.3 ### Code Example\n# Import necessary libraries\nimport numpy as np\n\n# Define a function to create a simple dense vector representation\ndef create_dense_vector(text):\n    # Split the text into words\n    words = text.split()\n\n    # Create a dictionary to store word vectors\n    word_vectors = {}\n\n    # Add each word to the dictionary\n    for i, word in enumerate(words):\n        # Use a random numerical value as the vector for simplicity\n        word_vector = np.random.rand(768)\n        word_vectors[word] = word_vector\n\n    # Combine all word vectors into a single dense vector\n    dense_vector = np.array([word_vectors[word] for word in words])\n\n    return dense_vector\n\n# Test the function\ntext = \"The quick brown fox jumps over the lazy dog.\"\ndense_vector = create_dense_vector(text)\nprint(dense_vector.shape)  # Output: (1, 768)\nIn this example, we use a simple dictionary to store word vectors and then combine them into a single dense vector.",
    "crumbs": [
      "Dense text embedding"
    ]
  },
  {
    "objectID": "001_basic.html#sparse-text-embeddings",
    "href": "001_basic.html#sparse-text-embeddings",
    "title": "Dense text embedding",
    "section": "1.4 Sparse text embeddings",
    "text": "1.4 Sparse text embeddings\n\n%%ai\n\n\"Explain sparse text embedding, keep it simple. Demonstrate some example. Give some real-world example\"\n\n2 Sparse Text Embeddings\n\n2.0.1 What is a Sparse Vector?\nA sparse vector is a numerical representation of a piece of text where most values are 0.\n\n\n2.0.2 How Does it Work?\nImagine you’re looking at a word and thinking of related words, like “apple” and “fruit”. A sparse vector would have a few non-zero values representing the relationships between these words, but most values would be 0 (representing unrelated words).\n\n\n2.1 ### Example\nLet’s say we have a piece of text: “The quick brown fox jumps over the lazy dog.”\nWe can represent this as two sparse vectors:\n\nVector A: [0.1, 0, 0, ...] (related to the word “quick”)\nVector B: [0, 0.4, 0, ...] (related to the word “brown”)\n\nOur sparse vector would have only non-zero values representing the relationships between these two words.\n\n\n2.2 ### Real-World Examples\n\nText Classification: Sparse embeddings can be used as input features for text classification models, such as sentiment analysis or spam detection.\nTopic Modeling: Sparse embeddings can help identify topics in large collections of text by capturing the underlying relationships between words.\nWord Embeddings: Sparse embeddings can be used to represent individual words, like Word2Vec or GloVe.\n\n\n\n2.3 ### Code Example\n# Import necessary libraries\nimport numpy as np\n\n# Define a function to create a simple sparse vector representation\ndef create_sparse_vector(text):\n    # Split the text into words\n    words = text.split()\n\n    # Create a dictionary to store word vectors\n    word_vectors = {}\n\n    # Add each word to the dictionary\n    for i, word in enumerate(words):\n        # Use a random numerical value as the vector for simplicity\n        word_vector = np.random.rand(768)\n        word_vector[i] = 1.0  # Set the correct value\n        word_vectors[word] = word_vector\n\n    # Combine all word vectors into a single sparse vector\n    sparse_vector = np.array([word_vectors[word][i] for i, word in enumerate(words)])\n\n    return sparse_vector\n\n# Test the function\ntext = \"The quick brown fox jumps over the lazy dog.\"\nsparse_vector = create_sparse_vector(text)\nprint(sparse_vector.shape)  # Output: (1, 768)\nIn this example, we use a simple dictionary to store word vectors and then combine them into a single sparse vector. The i-th value in each word vector is set to 1.0 to represent the correct relationship between words.\n\n\n\n\nfrom fastembed import SparseTextEmbedding\n\nmodel = SparseTextEmbedding(model_name=\"prithivida/Splade_PP_en_v1\")\nembeddings = list(model.embed(documents))\nembeddings\n\n[SparseEmbedding(values=array([0.46793732, 0.34634435, 0.82014424, 0.45307532, 0.98732066,\n        0.80176616, 0.2087955 , 0.07078066, 0.15851103, 0.07413071,\n        0.34253079, 0.88557774, 0.13234277, 0.23698376, 0.07734038,\n        0.20083414, 1.3942709 , 0.57856292, 0.75639009, 0.12872015,\n        0.12940496, 1.21411681, 0.3960413 , 0.38100156, 0.85480541,\n        0.23132324, 0.61133695, 0.34899744, 0.15025412, 0.1130122 ,\n        0.15241024, 0.36152679, 0.13700481, 0.7303589 , 1.39194822,\n        0.04954698, 0.49473077, 0.30635571, 0.06034151, 1.13118982,\n        0.01341425, 0.02633621, 0.10710741, 1.03937888, 0.05903498,\n        0.33036089, 0.0278459 , 0.04743589, 1.68689609, 0.62101287,\n        1.86998868, 0.71478194, 0.08071101, 1.26968515, 0.05093801,\n        0.09553559, 1.57417607, 0.18500556, 0.0425379 , 0.24046306,\n        1.08656394, 0.72864759, 0.1876028 , 0.85070795, 0.16575399,\n        0.23869337, 0.52304912, 0.90775394, 0.02330356, 0.12363458,\n        0.37557927, 1.93465626, 0.5360083 , 0.08284581, 0.39607322,\n        0.13179989]), indices=array([ 2022,  2060,  2084,  2138,  2328,  2422,  2488,  2544,  2640,\n         2653,  2742,  2793,  2828,  2881,  3033,  3074,  3075,  3082,\n         3177,  3274,  3430,  3435,  3642,  3800,  3857,  3989,  4007,\n         4127,  4248,  4289,  4294,  4385,  4406,  4489,  4667,  4773,\n         5056,  5080,  5371,  5514,  5672,  6028,  6082,  6251,  6254,\n         6994,  7705,  7831,  7861,  7915,  8270,  8860,  8875,  8957,\n         9121,  9262,  9442, 10472, 10763, 10899, 10938, 11746, 11892,\n        11907, 12430, 12692, 13507, 13850, 15106, 16473, 19059, 19081,\n        21331, 23561, 23924, 27014])),\n SparseEmbedding(values=array([0.08703687, 2.10778594, 0.01475082, 0.16455774, 0.09053489,\n        0.04780621, 0.89654833, 1.0727092 , 0.05152059, 0.19544077,\n        1.18726742, 0.09147657, 0.53395849, 0.36316222, 0.05458989,\n        1.05834925, 0.21000545, 0.2174563 , 0.4657135 , 0.16001791,\n        2.58359981, 1.51888502, 0.42704242, 0.85715246, 0.22741754,\n        0.1889631 , 0.04392261, 0.03791322, 0.5099448 , 1.10458422,\n        0.20403962, 0.97265482, 0.0194856 , 0.86876655, 0.10075891,\n        2.00031805, 0.24204996, 0.07565179, 0.52923071, 0.48402771,\n        0.19994174, 1.99733865, 0.42608464, 0.18558736, 0.15080665,\n        0.13985491, 0.54798013, 0.16576864, 0.19238883, 0.57402122,\n        2.63415504]), indices=array([ 1010,  1053,  2003,  2009,  2025,  2040,  2102,  2194,  2284,\n         2291,  2490,  2497,  2544,  2562,  2572,  2793,  2897,  2974,\n         3079,  3274,  3435,  3569,  3954,  4007,  4289,  4316,  4322,\n         4434,  5080,  5224,  5371,  5441,  5527,  6032,  6153,  6633,\n         7506,  7621,  7751,  7861,  8241,  8270,  8498,  9319,  9722,\n        10472, 12494, 13666, 16350, 20486, 24914]))]",
    "crumbs": [
      "Dense text embedding"
    ]
  },
  {
    "objectID": "001_basic.html#example-1",
    "href": "001_basic.html#example-1",
    "title": "Dense text embedding",
    "section": "2.1 ### Example",
    "text": "2.1 ### Example\nLet’s say we have a piece of text: “The quick brown fox jumps over the lazy dog.”\nWe can represent this as two sparse vectors:\n\nVector A: [0.1, 0, 0, ...] (related to the word “quick”)\nVector B: [0, 0.4, 0, ...] (related to the word “brown”)\n\nOur sparse vector would have only non-zero values representing the relationships between these two words.",
    "crumbs": [
      "Dense text embedding"
    ]
  },
  {
    "objectID": "001_basic.html#real-world-examples-1",
    "href": "001_basic.html#real-world-examples-1",
    "title": "Dense text embedding",
    "section": "2.2 ### Real-World Examples",
    "text": "2.2 ### Real-World Examples\n\nText Classification: Sparse embeddings can be used as input features for text classification models, such as sentiment analysis or spam detection.\nTopic Modeling: Sparse embeddings can help identify topics in large collections of text by capturing the underlying relationships between words.\nWord Embeddings: Sparse embeddings can be used to represent individual words, like Word2Vec or GloVe.",
    "crumbs": [
      "Dense text embedding"
    ]
  },
  {
    "objectID": "001_basic.html#code-example-1",
    "href": "001_basic.html#code-example-1",
    "title": "Dense text embedding",
    "section": "2.3 ### Code Example",
    "text": "2.3 ### Code Example\n# Import necessary libraries\nimport numpy as np\n\n# Define a function to create a simple sparse vector representation\ndef create_sparse_vector(text):\n    # Split the text into words\n    words = text.split()\n\n    # Create a dictionary to store word vectors\n    word_vectors = {}\n\n    # Add each word to the dictionary\n    for i, word in enumerate(words):\n        # Use a random numerical value as the vector for simplicity\n        word_vector = np.random.rand(768)\n        word_vector[i] = 1.0  # Set the correct value\n        word_vectors[word] = word_vector\n\n    # Combine all word vectors into a single sparse vector\n    sparse_vector = np.array([word_vectors[word][i] for i, word in enumerate(words)])\n\n    return sparse_vector\n\n# Test the function\ntext = \"The quick brown fox jumps over the lazy dog.\"\nsparse_vector = create_sparse_vector(text)\nprint(sparse_vector.shape)  # Output: (1, 768)\nIn this example, we use a simple dictionary to store word vectors and then combine them into a single sparse vector. The i-th value in each word vector is set to 1.0 to represent the correct relationship between words.",
    "crumbs": [
      "Dense text embedding"
    ]
  },
  {
    "objectID": "001_basic.html#late-interaction-models-aka-colbert",
    "href": "001_basic.html#late-interaction-models-aka-colbert",
    "title": "Dense text embedding",
    "section": "2.4 Late interaction models (aka ColBERT)",
    "text": "2.4 Late interaction models (aka ColBERT)\n\n%%ai ollama:llama3.2\n\n\"Explain late interaction models (aka ColBERT), keep it simple. Demonstrate some example. Give some real-world example\"\n\n3 Late Interaction Models (ColBERT)\n\n3.0.1 What is Late Interaction?\nLate interaction models are a type of search model that improves the performance of early interaction methods, such as BM25.\n\n\n3.0.2 How Does it Work?\nImagine you’re searching for documents related to a query. Early interaction methods like BM25 use features extracted from the query and the document to rank them. Late interaction models refine these rankings by using additional information, such as the entire document or other relevant documents.\n\n\n3.1 ### Example\nLet’s say we have a search model that uses BM25 to rank documents for a query “computer science”. The top 5 ranked documents are:\n\nA paper on computer vision\nA blog post on machine learning\nA Wikipedia page on artificial intelligence\nA news article on data science\nA book review on software engineering\n\nLate interaction models like ColBERT can improve these rankings by considering additional features, such as:\n\nThe entire document text\nOther relevant documents in the corpus\n\n\n\n3.2 ### Real-World Examples\n\nE-commerce Search: Late interaction models can improve search results for e-commerce platforms by incorporating product descriptions, reviews, and other relevant information.\nKnowledge Graph Search: Late interaction models can be used to search knowledge graphs, which are databases of entities and their relationships.\nRecommendation Systems: Late interaction models can be used to recommend items based on user behavior and preferences.\n\n\n\n3.3 ### Code Example\n# Import necessary libraries\nimport numpy as np\n\n# Define a function to create a ColBERT model\ndef create_colbert_model(query, documents):\n    # Extract features from the query and documents using BM25\n    bm25_features = bm25_query_and_document(query, documents)\n\n    # Add additional features using late interaction\n    colbert_features = add_late_interaction(bm25_features, query, documents)\n\n    return colbert_features\n\n# Define a function to calculate BM25 features\ndef bm25_query_and_document(query, documents):\n    # Implement BM25 calculation here\n    pass\n\n# Define a function to add late interaction features\ndef add_late_interaction(features, query, documents):\n    # Implement late interaction calculation here\n    pass\nIn this example, we define a simple ColBERT model that uses BM25 as an early interaction method and adds additional features using late interaction. The bm25_query_and_document function calculates BM25 features, and the add_late_interaction function calculates late interaction features.\n\n\n\n\nfrom fastembed import LateInteractionTextEmbedding\n\nmodel = LateInteractionTextEmbedding(model_name=\"colbert-ir/colbertv2.0\")\nembeddings = list(model.embed(documents))\nembeddings\n\n[array([[-0.1351824 ,  0.12230334,  0.1269857 , ...,  0.17307524,\n          0.11274203,  0.02880633],\n        [-0.17495233,  0.08767531,  0.11352374, ...,  0.12433604,\n          0.15752925,  0.08118125],\n        [-0.10130584,  0.09613474,  0.13923067, ...,  0.12898032,\n          0.16839182,  0.09858395],\n        ...,\n        [-0.10270972,  0.01041561,  0.04440113, ...,  0.0550529 ,\n          0.08930317,  0.09720251],\n        [-0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [-0.15476122,  0.06961455,  0.10665789, ...,  0.15388842,\n          0.09050205,  0.00516431]], shape=(29, 128), dtype=float32),\n array([[ 0.12170535,  0.07871944,  0.12508287, ...,  0.08450251,\n          0.01834184, -0.01686618],\n        [-0.02659732, -0.12131035,  0.14012505, ..., -0.01885814,\n          0.01064609, -0.05982119],\n        [-0.03633325, -0.14667122,  0.14062028, ..., -0.052545  ,\n          0.00967532, -0.08844125],\n        ...,\n        [-0.        ,  0.        ,  0.        , ..., -0.        ,\n         -0.        ,  0.        ],\n        [-0.        , -0.        ,  0.        , ..., -0.        ,\n         -0.        , -0.        ],\n        [-0.        , -0.        ,  0.        , ..., -0.        ,\n          0.        , -0.        ]], shape=(29, 128), dtype=float32)]",
    "crumbs": [
      "Dense text embedding"
    ]
  },
  {
    "objectID": "001_basic.html#example-2",
    "href": "001_basic.html#example-2",
    "title": "Dense text embedding",
    "section": "3.1 ### Example",
    "text": "3.1 ### Example\nLet’s say we have a search model that uses BM25 to rank documents for a query “computer science”. The top 5 ranked documents are:\n\nA paper on computer vision\nA blog post on machine learning\nA Wikipedia page on artificial intelligence\nA news article on data science\nA book review on software engineering\n\nLate interaction models like ColBERT can improve these rankings by considering additional features, such as:\n\nThe entire document text\nOther relevant documents in the corpus",
    "crumbs": [
      "Dense text embedding"
    ]
  },
  {
    "objectID": "001_basic.html#real-world-examples-2",
    "href": "001_basic.html#real-world-examples-2",
    "title": "Dense text embedding",
    "section": "3.2 ### Real-World Examples",
    "text": "3.2 ### Real-World Examples\n\nE-commerce Search: Late interaction models can improve search results for e-commerce platforms by incorporating product descriptions, reviews, and other relevant information.\nKnowledge Graph Search: Late interaction models can be used to search knowledge graphs, which are databases of entities and their relationships.\nRecommendation Systems: Late interaction models can be used to recommend items based on user behavior and preferences.",
    "crumbs": [
      "Dense text embedding"
    ]
  },
  {
    "objectID": "001_basic.html#code-example-2",
    "href": "001_basic.html#code-example-2",
    "title": "Dense text embedding",
    "section": "3.3 ### Code Example",
    "text": "3.3 ### Code Example\n# Import necessary libraries\nimport numpy as np\n\n# Define a function to create a ColBERT model\ndef create_colbert_model(query, documents):\n    # Extract features from the query and documents using BM25\n    bm25_features = bm25_query_and_document(query, documents)\n\n    # Add additional features using late interaction\n    colbert_features = add_late_interaction(bm25_features, query, documents)\n\n    return colbert_features\n\n# Define a function to calculate BM25 features\ndef bm25_query_and_document(query, documents):\n    # Implement BM25 calculation here\n    pass\n\n# Define a function to add late interaction features\ndef add_late_interaction(features, query, documents):\n    # Implement late interaction calculation here\n    pass\nIn this example, we define a simple ColBERT model that uses BM25 as an early interaction method and adds additional features using late interaction. The bm25_query_and_document function calculates BM25 features, and the add_late_interaction function calculates late interaction features.",
    "crumbs": [
      "Dense text embedding"
    ]
  },
  {
    "objectID": "001_basic.html#image-embeddings",
    "href": "001_basic.html#image-embeddings",
    "title": "Dense text embedding",
    "section": "3.4 Image embeddings",
    "text": "3.4 Image embeddings\n\nimport numpy as np\nfrom fastembed import ImageEmbedding\n\nimages = [\"images/cat1.jpeg\", \"images/cat2.jpeg\", \"images/dog1.webp\"]\n\nmodel = ImageEmbedding(model_name=\"Qdrant/clip-ViT-B-32-vision\")\nembeddings = list(model.embed(images))\n\n\nfrom qdrant_client.local.distances import cosine_similarity\n\n\ncosine_similarity(np.array(embeddings), np.array(embeddings))\n\narray([[1.0000001 , 0.83723867, 0.61121917],\n       [0.83723867, 1.0000002 , 0.7055948 ],\n       [0.61121917, 0.7055948 , 1.0000001 ]], dtype=float32)",
    "crumbs": [
      "Dense text embedding"
    ]
  },
  {
    "objectID": "001_basic.html#late-interaction-multimodal-models-colpali",
    "href": "001_basic.html#late-interaction-multimodal-models-colpali",
    "title": "Dense text embedding",
    "section": "3.5 Late interaction multimodal models (ColPali)",
    "text": "3.5 Late interaction multimodal models (ColPali)\n\n%%ai\n\n\"Explain late interaction multimodal models (aka ColPali), keep it simple. Demonstrate some example. Give some real-world example\"\n\n4 Late Interaction Multimodal Models (ColPali)\n\n4.0.1 What is Late Interaction?\nLate interaction models are a type of search model that improves the performance of early interaction methods, such as BM25.\n\n\n4.0.2 How Does it Work?\nImagine you’re searching for documents related to a query. Early interaction methods like BM25 use features extracted from the query and the document to rank them. Late interaction models refine these rankings by using additional information, such as the entire document or other relevant documents.\n\n\n4.0.3 Multimodal Models\nMultimodal models extend late interaction models to incorporate multiple input modalities, such as text, images, audio, and video.\n\n\n4.1 ### Example\nLet’s say we have a search model that uses ColPali to rank documents for a query “computer science”. The input modalities are:\n\nText: A passage of text related to the query.\nImage: An image related to the query (e.g. a diagram of a computer chip).\nAudio: An audio file related to the query (e.g. a lecture on machine learning).\n\nThe ColPali model extracts features from each modality and combines them using late interaction. The final ranking is based on the weighted sum of these features.\n\n\n4.2 ### Real-World Examples\n\nImage Search with Text Description: Late interaction multimodal models can be used to search images based on a text description.\nSpeech Recognition with Visual Feedback: ColPali can be used in speech recognition systems that incorporate visual feedback, such as displaying the recognized text in real-time.\nMultimodal Question Answering: Late interaction multimodal models can be used to answer questions that require information from multiple input modalities (e.g. text, images, audio).\n\n\n\n4.3 ### Code Example\n# Import necessary libraries\nimport numpy as np\n\n# Define a function to create a ColPali model\ndef create_colpali_model(query, text, image, audio):\n    # Extract features from each modality using late interaction\n    text_features = extract_features(text, query)\n    image_features = extract_features(image, query)\n    audio_features = extract_features(audio, query)\n\n    # Combine features using weighted sum\n    combined_features = combine_features(text_features, image_features, audio_features)\n\n    return combined_features\n\n# Define a function to extract features from text using late interaction\ndef extract_features(text, query):\n    # Implement late interaction calculation here\n    pass\n\n# Define a function to combine features using weighted sum\ndef combine_features(text_features, image_features, audio_features):\n    # Implement weighted sum calculation here\n    pass\nIn this example, we define a simple ColPali model that uses late interaction to extract features from each modality and combines them using a weighted sum. The extract_features function calculates features for each modality, and the combine_features function calculates the final combined features.\n\n\n\n\nfrom fastembed import LateInteractionMultimodalEmbedding\n\ndoc_images = [\n    \"images/wiki_computer_science.png\",\n    \"images/wiki_technology.png\",\n    \"images/wiki_space.png\",\n]\n\nquery = \"what is tech\"\n\nmodel = LateInteractionMultimodalEmbedding(model_name=\"Qdrant/colpali-v1.3-fp16\")\ndoc_images_embeddings = list(model.embed_image(doc_images))\nquery_embedding = model.embed_text(query)\n\n\nfrom qdrant_client import models\nfrom qdrant_client.local.multi_distances import calculate_multi_distance\n\n# How to calculate distance?\n# from qdrant_client.local.sparse_distances import calculate_distance_sparse\n# calculate_multi_distance(\n#     query_embedding,\n#     doc_images_embeddings,\n#     # distance_type=models.MultiVectorComparator.MAX_SIM,\n#     distance_type=models.Distance.DOT,\n# )",
    "crumbs": [
      "Dense text embedding"
    ]
  },
  {
    "objectID": "001_basic.html#example-3",
    "href": "001_basic.html#example-3",
    "title": "Dense text embedding",
    "section": "4.1 ### Example",
    "text": "4.1 ### Example\nLet’s say we have a search model that uses ColPali to rank documents for a query “computer science”. The input modalities are:\n\nText: A passage of text related to the query.\nImage: An image related to the query (e.g. a diagram of a computer chip).\nAudio: An audio file related to the query (e.g. a lecture on machine learning).\n\nThe ColPali model extracts features from each modality and combines them using late interaction. The final ranking is based on the weighted sum of these features.",
    "crumbs": [
      "Dense text embedding"
    ]
  },
  {
    "objectID": "001_basic.html#real-world-examples-3",
    "href": "001_basic.html#real-world-examples-3",
    "title": "Dense text embedding",
    "section": "4.2 ### Real-World Examples",
    "text": "4.2 ### Real-World Examples\n\nImage Search with Text Description: Late interaction multimodal models can be used to search images based on a text description.\nSpeech Recognition with Visual Feedback: ColPali can be used in speech recognition systems that incorporate visual feedback, such as displaying the recognized text in real-time.\nMultimodal Question Answering: Late interaction multimodal models can be used to answer questions that require information from multiple input modalities (e.g. text, images, audio).",
    "crumbs": [
      "Dense text embedding"
    ]
  },
  {
    "objectID": "001_basic.html#code-example-3",
    "href": "001_basic.html#code-example-3",
    "title": "Dense text embedding",
    "section": "4.3 ### Code Example",
    "text": "4.3 ### Code Example\n# Import necessary libraries\nimport numpy as np\n\n# Define a function to create a ColPali model\ndef create_colpali_model(query, text, image, audio):\n    # Extract features from each modality using late interaction\n    text_features = extract_features(text, query)\n    image_features = extract_features(image, query)\n    audio_features = extract_features(audio, query)\n\n    # Combine features using weighted sum\n    combined_features = combine_features(text_features, image_features, audio_features)\n\n    return combined_features\n\n# Define a function to extract features from text using late interaction\ndef extract_features(text, query):\n    # Implement late interaction calculation here\n    pass\n\n# Define a function to combine features using weighted sum\ndef combine_features(text_features, image_features, audio_features):\n    # Implement weighted sum calculation here\n    pass\nIn this example, we define a simple ColPali model that uses late interaction to extract features from each modality and combines them using a weighted sum. The extract_features function calculates features for each modality, and the combine_features function calculates the final combined features.",
    "crumbs": [
      "Dense text embedding"
    ]
  },
  {
    "objectID": "001_basic.html#rerankers",
    "href": "001_basic.html#rerankers",
    "title": "Dense text embedding",
    "section": "4.4 Rerankers",
    "text": "4.4 Rerankers\n\nfrom fastembed.rerank.cross_encoder import TextCrossEncoder\n\nquery = \"Who is maintaining Qdrant?\"\ndocuments: list[str] = [\n    \"This is built to be faster and lighter than other embedding libraries e.g. Transformers, Sentence-Transformers, etc.\",\n    \"fastembed is supported by and maintained by Qdrant.\",\n]\nencoder = TextCrossEncoder(model_name=\"Xenova/ms-marco-MiniLM-L-6-v2\")\nscores = list(encoder.rerank(query, documents))\nscores",
    "crumbs": [
      "Dense text embedding"
    ]
  },
  {
    "objectID": "002_qdrant_client.html",
    "href": "002_qdrant_client.html",
    "title": "python applied machine learning",
    "section": "",
    "text": "from qdrant_client import QdrantClient\n\n\n# client = QdrantClient(\":memory:\")\n# or\nclient = QdrantClient(path=\"tmp/tmp.db\")\n# client.close()\n\n\ncat_facts = [\n    \"Cats have 32 muscles in each ear, allowing for their unique ability to rotate their ears independently.\",\n    \"A group of cats is called a 'clowder'.\",\n    \"Cats can't taste sweetness.\",\n    \"A cat's whiskers help them navigate in the dark.\",\n    \"Cats have three eyelids: upper, lower, and third (also known as nictitating membrane).\",\n    \"Cats spend 1/3 of their waking hours sleeping.\",\n    \"A cat's purr can be a sign of happiness or self-soothing.\",\n    \"Cats have unique nose prints, just like humans have fingerprints.\",\n    \"The world's largest domesticated cat was Muffin, who measured 48.5 inches long and weighed 46.3 pounds.\",\n    \"Cats can't see in complete darkness but their night vision is excellent due to a reflective layer in the back of their eyes called the tapetum lucidum.\",\n]\ndog_facts = [\n    \"A dog's sense of smell is up to 10,000 times more sensitive than a human's.\",\n    \"Dogs can hear sounds at frequencies as high as 40,000 Hz, while humans can only hear up to 20,000 Hz.\",\n    \"A group of dogs is called a 'pack'.\",\n    \"Dogs have three eyelids: upper, lower, and nictitating membrane (third eyelid).\",\n    \"Dogs can dream just like humans do, and their brain waves show similar patterns during sleep.\",\n    \"The world's smallest dog breed is the Chihuahua, with adults weighing as little as 2 pounds.\",\n    \"A dog's tail language is more complex than human body language, conveying emotions and intentions.\",\n    \"Dogs can be right- or left-pawed, just like humans are right- or left-handed.\",\n    \"The oldest known dog remains dates back to around 14,000 years ago, during the Late Pleistocene era.\",\n    \"Dogs have a unique nose print, just like humans have fingerprints.\",\n]\n\n\ndocs = cat_facts + dog_facts\nmetadata = [{\"source\": \"cats\"}] * len(cat_facts) + [{\"source\": \"dogs\"}] * len(dog_facts)\nids = list(range(len(docs)))\n\n\nclient.add(\n    collection_name=\"cats_and_dogs\",\n    documents=docs,\n    metadata=metadata,\n    ids=ids,\n)\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n\n\n\nsearch_result = client.query(\n    collection_name=\"cats_and_dogs\", query_text=\"what is the cat whisker for?\"\n)\nsearch_result\n\n[QueryResponse(id=3, embedding=None, sparse_embedding=None, metadata={'document': \"A cat's whiskers help them navigate in the dark.\", 'source': 'cats'}, document=\"A cat's whiskers help them navigate in the dark.\", score=0.8960122052835192),\n QueryResponse(id=1, embedding=None, sparse_embedding=None, metadata={'document': \"A group of cats is called a 'clowder'.\", 'source': 'cats'}, document=\"A group of cats is called a 'clowder'.\", score=0.8371795647410071),\n QueryResponse(id=6, embedding=None, sparse_embedding=None, metadata={'document': \"A cat's purr can be a sign of happiness or self-soothing.\", 'source': 'cats'}, document=\"A cat's purr can be a sign of happiness or self-soothing.\", score=0.8254998513657924),\n QueryResponse(id=4, embedding=None, sparse_embedding=None, metadata={'document': 'Cats have three eyelids: upper, lower, and third (also known as nictitating membrane).', 'source': 'cats'}, document='Cats have three eyelids: upper, lower, and third (also known as nictitating membrane).', score=0.8023456208085149),\n QueryResponse(id=7, embedding=None, sparse_embedding=None, metadata={'document': 'Cats have unique nose prints, just like humans have fingerprints.', 'source': 'cats'}, document='Cats have unique nose prints, just like humans have fingerprints.', score=0.7981833555293472),\n QueryResponse(id=0, embedding=None, sparse_embedding=None, metadata={'document': 'Cats have 32 muscles in each ear, allowing for their unique ability to rotate their ears independently.', 'source': 'cats'}, document='Cats have 32 muscles in each ear, allowing for their unique ability to rotate their ears independently.', score=0.7954061802722565),\n QueryResponse(id=2, embedding=None, sparse_embedding=None, metadata={'document': \"Cats can't taste sweetness.\", 'source': 'cats'}, document=\"Cats can't taste sweetness.\", score=0.7869827666131266),\n QueryResponse(id=5, embedding=None, sparse_embedding=None, metadata={'document': 'Cats spend 1/3 of their waking hours sleeping.', 'source': 'cats'}, document='Cats spend 1/3 of their waking hours sleeping.', score=0.780144426020518),\n QueryResponse(id=19, embedding=None, sparse_embedding=None, metadata={'document': 'Dogs have a unique nose print, just like humans have fingerprints.', 'source': 'dogs'}, document='Dogs have a unique nose print, just like humans have fingerprints.', score=0.7677822913415793),\n QueryResponse(id=12, embedding=None, sparse_embedding=None, metadata={'document': \"A group of dogs is called a 'pack'.\", 'source': 'dogs'}, document=\"A group of dogs is called a 'pack'.\", score=0.7629267110754824)]\n\n\n\nfrom qdrant_client.models import FieldCondition, Filter, MatchValue\n\nsearch_result = client.query(\n    collection_name=\"cats_and_dogs\",\n    query_text=\"what is the cat whisker for?\",\n    query_filter=Filter(\n        must=[FieldCondition(key=\"source\", match=MatchValue(value=\"cats\"))]\n    ),\n)\n\nfor res in search_result:\n    print(dict(doc=res.document, score=res.score))\n\n{'doc': \"A cat's whiskers help them navigate in the dark.\", 'score': 0.8960122052835192}\n{'doc': \"A group of cats is called a 'clowder'.\", 'score': 0.8371795647410071}\n{'doc': \"A cat's purr can be a sign of happiness or self-soothing.\", 'score': 0.8254998513657924}\n{'doc': 'Cats have three eyelids: upper, lower, and third (also known as nictitating membrane).', 'score': 0.8023456208085149}\n{'doc': 'Cats have unique nose prints, just like humans have fingerprints.', 'score': 0.7981833555293472}\n{'doc': 'Cats have 32 muscles in each ear, allowing for their unique ability to rotate their ears independently.', 'score': 0.7954061802722565}\n{'doc': \"Cats can't taste sweetness.\", 'score': 0.7869827666131266}\n{'doc': 'Cats spend 1/3 of their waking hours sleeping.', 'score': 0.780144426020518}\n{'doc': \"Cats can't see in complete darkness but their night vision is excellent due to a reflective layer in the back of their eyes called the tapetum lucidum.\", 'score': 0.7533777384436582}\n{'doc': \"The world's largest domesticated cat was Muffin, who measured 48.5 inches long and weighed 46.3 pounds.\", 'score': 0.7365538436735114}",
    "crumbs": [
      "002 Qdrant Client"
    ]
  },
  {
    "objectID": "004_production_ready.html",
    "href": "004_production_ready.html",
    "title": "1 Retrieval Augmented Generation (RAG) Application Planning",
    "section": "",
    "text": "%load_ext jupyter_ai\n\n\n%%ai ollama:llama3.2\n\n\"\"\"\nyou are an experienced AI engineer with 10 years of experience.\nyou are tasked to build a retrieval augmented generation, RAG application.\na vector store is an essential part for querying large amount of documents.\n\nplan before you act\n- suggest the metadata for the documents to embed\n- the metadata should aid in fast retrieval\n- explain each metadata fields\n- consider how to version the changes for the metadata when the document changes\n- consider how to support different embedding models\n- take into consideration of other edge cases\n- list down other edge cases too\n\"\"\"\n\n1 Retrieval Augmented Generation (RAG) Application Planning\n\n1.0.1 1. Suggested Metadata for Documents\nTo aid in fast retrieval, we suggest embedding the following metadata fields:\n\nDocument ID: A unique identifier for each document.\nCategory: The category or topic that the document belongs to.\nTextual Features: Relevant textual features such as word embeddings (e.g., Word2Vec, GloVe) and document frequency.\nMetadata Tags: Additional metadata tags such as author, date of creation, and keywords.\n\n\n\n1.0.2 2. Explanation of Metadata Fields\n\nDocument ID: A unique identifier for each document, which can be used to quickly identify the document in storage or when querying.\nCategory: The category or topic that the document belongs to. This information is useful for fast retrieval based on topic similarity.\nTextual Features: Relevant textual features such as word embeddings (e.g., Word2Vec, GloVe) and document frequency. These features provide semantic meaning to the text content of the document.\nMetadata Tags: Additional metadata tags such as author, date of creation, and keywords. These tags provide additional context about the document.\n\n\n\n1.0.3 3. Versioning Changes for Metadata when Documents Change\nTo version changes for metadata when documents change:\n\nCreate a separate field to track the version number of each document.\nIncrement the version number each time the metadata changes.\nStore the previous version number along with the updated metadata.\n\nExample:\n\n\n\n\n\n\n\n\n\n\nDocument ID\nVersion\nCategory\nTextual Features\nMetadata Tags\n\n\n\n\ndoc-1\n1.0\nTech\n[word embeddings]\nauthor: John, date: 2022-01-01\n\n\ndoc-1\n2.0\nTech\n[updated word embeddings]\nupdated tags: keywords\n\n\n\n\n\n1.0.4 4. Supporting Different Embedding Models\nTo support different embedding models:\n\nUse a modular architecture that allows the addition of new embedding models.\nImplement a standard interface for embedding models to ensure compatibility.\n\nExample:\n\nAdd new embedding model using a RESTful API or a message queue.\nUpdate the metadata field to use the new embedding model.\nRestart the application or re-run the embedding process.\n\n\n\n1.0.5 5. Edge Cases\nSome additional edge cases to consider:\n\nHandling duplicate documents: Implement measures to handle duplicate documents, such as using a unique identifier for each document or storing the most recent version of the metadata.\nDealing with sparse data: Handle sparse data by using techniques such as interpolation or imputation to estimate missing values.\nOptimizing storage and query performance: Optimize storage and query performance by indexing frequently used metadata fields.\n\nExample Edge Cases:\n\nDuplicate Documents: Duplicate documents can occur when multiple users submit the same document. To handle this, store a unique identifier for each document and use it to identify duplicates.\nSparse Data: Sparse data occurs when some documents have missing values in their metadata fields. To handle this, use interpolation or imputation techniques to estimate missing values.\n\nOther edge cases:\n\nHandling non-textual metadata\nDealing with large volumes of metadata updates\nOptimizing storage and query performance for large datasets",
    "crumbs": [
      "004 Production Ready"
    ]
  },
  {
    "objectID": "006_generate_colbert_multivectors.html",
    "href": "006_generate_colbert_multivectors.html",
    "title": "How to Generate ColBERT Multivectors with FastEmbed",
    "section": "",
    "text": "https://qdrant.tech/documentation/fastembed/fastembed-colbert/\nfrom fastembed import LateInteractionTextEmbedding\nLateInteractionTextEmbedding.list_supported_models()[0]\n\n{'model': 'colbert-ir/colbertv2.0',\n 'sources': {'hf': 'colbert-ir/colbertv2.0', 'url': None},\n 'model_file': 'model.onnx',\n 'description': 'Late interaction model',\n 'license': 'mit',\n 'size_in_GB': 0.44,\n 'additional_files': [],\n 'dim': 128,\n 'tasks': {}}\nembedding_model = LateInteractionTextEmbedding(\"colbert-ir/colbertv2.0\")",
    "crumbs": [
      "How to Generate ColBERT Multivectors with FastEmbed"
    ]
  },
  {
    "objectID": "006_generate_colbert_multivectors.html#embed-data",
    "href": "006_generate_colbert_multivectors.html#embed-data",
    "title": "How to Generate ColBERT Multivectors with FastEmbed",
    "section": "1 Embed data",
    "text": "1 Embed data\n\ndescriptions = [\n    \"In 1431, Jeanne d'Arc is placed on trial on charges of heresy. The ecclesiastical jurists attempt to force Jeanne to recant her claims of holy visions.\",\n    \"A film projectionist longs to be a detective, and puts his meagre skills to work when he is framed by a rival for stealing his girlfriend's father's pocketwatch.\",\n    \"A group of high-end professional thieves start to feel the heat from the LAPD when they unknowingly leave a clue at their latest heist.\",\n    \"A petty thief with an utter resemblance to a samurai warlord is hired as the lord's double. When the warlord later dies the thief is forced to take up arms in his place.\",\n    \"A young boy named Kubo must locate a magical suit of armour worn by his late father in order to defeat a vengeful spirit from the past.\",\n    \"A biopic detailing the 2 decades that Punjabi Sikh revolutionary Udham Singh spent planning the assassination of the man responsible for the Jallianwala Bagh massacre.\",\n    \"When a machine that allows therapists to enter their patients' dreams is stolen, all hell breaks loose. Only a young female therapist, Paprika, can stop it.\",\n    \"An ordinary word processor has the worst night of his life after he agrees to visit a girl in Soho whom he met that evening at a coffee shop.\",\n    \"A story that revolves around drug abuse in the affluent north Indian State of Punjab and how the youth there have succumbed to it en-masse resulting in a socio-economic decline.\",\n    \"A world-weary political journalist picks up the story of a woman's search for her son, who was taken away from her decades ago after she became pregnant and was forced to live in a convent.\",\n    \"Concurrent theatrical ending of the TV series Neon Genesis Evangelion (1995).\",\n    \"During World War II, a rebellious U.S. Army Major is assigned a dozen convicted murderers to train and lead them into a mass assassination mission of German officers.\",\n    \"The toys are mistakenly delivered to a day-care center instead of the attic right before Andy leaves for college, and it's up to Woody to convince the other toys that they weren't abandoned and to return home.\",\n    \"A soldier fighting aliens gets to relive the same day over and over again, the day restarting every time he dies.\",\n    \"After two male musicians witness a mob hit, they flee the state in an all-female band disguised as women, but further complications set in.\",\n    \"Exiled into the dangerous forest by her wicked stepmother, a princess is rescued by seven dwarf miners who make her part of their household.\",\n    \"A renegade reporter trailing a young runaway heiress for a big story joins her on a bus heading from Florida to New York, and they end up stuck with each other when the bus leaves them behind at one of the stops.\",\n    \"Story of 40-man Turkish task force who must defend a relay station.\",\n    \"Spinal Tap, one of England's loudest bands, is chronicled by film director Marty DiBergi on what proves to be a fateful tour.\",\n    \"Oskar, an overlooked and bullied boy, finds love and revenge through Eli, a beautiful but peculiar girl.\",\n]\n\n\ndescriptions_embeddings = list(embedding_model.embed(descriptions))\ndescriptions_embeddings[0].shape\n\n(48, 128)",
    "crumbs": [
      "How to Generate ColBERT Multivectors with FastEmbed"
    ]
  },
  {
    "objectID": "006_generate_colbert_multivectors.html#upload-embeddings-to-qdrant",
    "href": "006_generate_colbert_multivectors.html#upload-embeddings-to-qdrant",
    "title": "How to Generate ColBERT Multivectors with FastEmbed",
    "section": "2 Upload embeddings to Qdrant",
    "text": "2 Upload embeddings to Qdrant\n\nfrom qdrant_client import QdrantClient, models\n\nqdrant_client = QdrantClient(\":memory:\")\n\n\nqdrant_client.create_collection(\n    collection_name=\"movies\",\n    vectors_config=models.VectorParams(\n        size=128,  # Size of each vector produced by ColBERT\n        distance=models.Distance.COSINE,\n        multivector_config=models.MultiVectorConfig(\n            comparator=models.MultiVectorComparator.MAX_SIM\n        ),\n    ),\n)\n\nTrue\n\n\n\nmetadata = [\n    {\n        \"movie_name\": \"The Passion of Joan of Arc\",\n        \"movie_watch_time_min\": 114,\n        \"movie_description\": \"In 1431, Jeanne d'Arc is placed on trial on charges of heresy. The ecclesiastical jurists attempt to force Jeanne to recant her claims of holy visions.\",\n    },\n    {\n        \"movie_name\": \"Sherlock Jr.\",\n        \"movie_watch_time_min\": 45,\n        \"movie_description\": \"A film projectionist longs to be a detective, and puts his meagre skills to work when he is framed by a rival for stealing his girlfriend's father's pocketwatch.\",\n    },\n    {\n        \"movie_name\": \"Heat\",\n        \"movie_watch_time_min\": 170,\n        \"movie_description\": \"A group of high-end professional thieves start to feel the heat from the LAPD when they unknowingly leave a clue at their latest heist.\",\n    },\n    {\n        \"movie_name\": \"Kagemusha\",\n        \"movie_watch_time_min\": 162,\n        \"movie_description\": \"A petty thief with an utter resemblance to a samurai warlord is hired as the lord's double. When the warlord later dies the thief is forced to take up arms in his place.\",\n    },\n    {\n        \"movie_name\": \"Kubo and the Two Strings\",\n        \"movie_watch_time_min\": 101,\n        \"movie_description\": \"A young boy named Kubo must locate a magical suit of armour worn by his late father in order to defeat a vengeful spirit from the past.\",\n    },\n    {\n        \"movie_name\": \"Sardar Udham\",\n        \"movie_watch_time_min\": 164,\n        \"movie_description\": \"A biopic detailing the 2 decades that Punjabi Sikh revolutionary Udham Singh spent planning the assassination of the man responsible for the Jallianwala Bagh massacre.\",\n    },\n    {\n        \"movie_name\": \"Paprika\",\n        \"movie_watch_time_min\": 90,\n        \"movie_description\": \"When a machine that allows therapists to enter their patients' dreams is stolen, all hell breaks loose. Only a young female therapist, Paprika, can stop it.\",\n    },\n    {\n        \"movie_name\": \"After Hours\",\n        \"movie_watch_time_min\": 97,\n        \"movie_description\": \"An ordinary word processor has the worst night of his life after he agrees to visit a girl in Soho whom he met that evening at a coffee shop.\",\n    },\n    {\n        \"movie_name\": \"Udta Punjab\",\n        \"movie_watch_time_min\": 148,\n        \"movie_description\": \"A story that revolves around drug abuse in the affluent north Indian State of Punjab and how the youth there have succumbed to it en-masse resulting in a socio-economic decline.\",\n    },\n    {\n        \"movie_name\": \"Philomena\",\n        \"movie_watch_time_min\": 98,\n        \"movie_description\": \"A world-weary political journalist picks up the story of a woman's search for her son, who was taken away from her decades ago after she became pregnant and was forced to live in a convent.\",\n    },\n    {\n        \"movie_name\": \"Neon Genesis Evangelion: The End of Evangelion\",\n        \"movie_watch_time_min\": 87,\n        \"movie_description\": \"Concurrent theatrical ending of the TV series Neon Genesis Evangelion (1995).\",\n    },\n    {\n        \"movie_name\": \"The Dirty Dozen\",\n        \"movie_watch_time_min\": 150,\n        \"movie_description\": \"During World War II, a rebellious U.S. Army Major is assigned a dozen convicted murderers to train and lead them into a mass assassination mission of German officers.\",\n    },\n    {\n        \"movie_name\": \"Toy Story 3\",\n        \"movie_watch_time_min\": 103,\n        \"movie_description\": \"The toys are mistakenly delivered to a day-care center instead of the attic right before Andy leaves for college, and it's up to Woody to convince the other toys that they weren't abandoned and to return home.\",\n    },\n    {\n        \"movie_name\": \"Edge of Tomorrow\",\n        \"movie_watch_time_min\": 113,\n        \"movie_description\": \"A soldier fighting aliens gets to relive the same day over and over again, the day restarting every time he dies.\",\n    },\n    {\n        \"movie_name\": \"Some Like It Hot\",\n        \"movie_watch_time_min\": 121,\n        \"movie_description\": \"After two male musicians witness a mob hit, they flee the state in an all-female band disguised as women, but further complications set in.\",\n    },\n    {\n        \"movie_name\": \"Snow White and the Seven Dwarfs\",\n        \"movie_watch_time_min\": 83,\n        \"movie_description\": \"Exiled into the dangerous forest by her wicked stepmother, a princess is rescued by seven dwarf miners who make her part of their household.\",\n    },\n    {\n        \"movie_name\": \"It Happened One Night\",\n        \"movie_watch_time_min\": 105,\n        \"movie_description\": \"A renegade reporter trailing a young runaway heiress for a big story joins her on a bus heading from Florida to New York, and they end up stuck with each other when the bus leaves them behind at one of the stops.\",\n    },\n    {\n        \"movie_name\": \"Nefes: Vatan Sagolsun\",\n        \"movie_watch_time_min\": 128,\n        \"movie_description\": \"Story of 40-man Turkish task force who must defend a relay station.\",\n    },\n    {\n        \"movie_name\": \"This Is Spinal Tap\",\n        \"movie_watch_time_min\": 82,\n        \"movie_description\": \"Spinal Tap, one of England's loudest bands, is chronicled by film director Marty DiBergi on what proves to be a fateful tour.\",\n    },\n    {\n        \"movie_name\": \"Let the Right One In\",\n        \"movie_watch_time_min\": 114,\n        \"movie_description\": \"Oskar, an overlooked and bullied boy, finds love and revenge through Eli, a beautiful but peculiar girl.\",\n    },\n]\n\n\nqdrant_client.upload_points(\n    collection_name=\"movies\",\n    points=[\n        models.PointStruct(id=idx, payload=metadata[idx], vector=vector)\n        for idx, vector in enumerate(descriptions_embeddings)\n    ],\n)",
    "crumbs": [
      "How to Generate ColBERT Multivectors with FastEmbed"
    ]
  },
  {
    "objectID": "006_generate_colbert_multivectors.html#querying",
    "href": "006_generate_colbert_multivectors.html#querying",
    "title": "How to Generate ColBERT Multivectors with FastEmbed",
    "section": "3 Querying",
    "text": "3 Querying\n\nqdrant_client.query_points(\n    collection_name=\"movies\",\n    query=list(\n        embedding_model.query_embed(\n            \"A movie for kids with fantasy elements and wonders\"\n        )\n    )[0],\n    limit=1,\n    with_vectors=False,  # Returns vectors\n    with_payload=True,  # Returns metadata\n)\n\nQueryResponse(points=[ScoredPoint(id=4, version=0, score=12.06346668699247, payload={'movie_name': 'Kubo and the Two Strings', 'movie_watch_time_min': 101, 'movie_description': 'A young boy named Kubo must locate a magical suit of armour worn by his late father in order to defeat a vengeful spirit from the past.'}, vector=None, shard_key=None, order_value=None)])",
    "crumbs": [
      "How to Generate ColBERT Multivectors with FastEmbed"
    ]
  },
  {
    "objectID": "010_long_context.html",
    "href": "010_long_context.html",
    "title": "1 My Feline Friend",
    "section": "",
    "text": "%load_ext jupyter_ai\n%config AiMagics.default_language_model = \"ollama:llama3.2\"\n\n\n%%ai\n\n\"Generate a super long text of 1000 words about my cat\"\n\n1 My Feline Friend\n\n1.0.1 Introduction\nMy cat, Mr. Whiskers, has been a constant companion in my life for several years now. He is a sleek and agile feline with a shiny coat that comes in a variety of colors depending on the lighting conditions. Despite his refined appearance, Mr. Whiskers is a playful and mischievous creature who always keeps me entertained.\n\n\n1.0.2 Physical Characteristics\nMr. Whiskers is a medium-sized cat with a muscular build. He has a short, smooth coat that ranges in color from grey to black, with distinctive white markings on his paws and chest. His eyes are a bright green, and his whiskers (as his name suggests) are incredibly long and sensitive. Despite their impressive length, Mr. Whiskers’ whiskers rarely get tangled or caught in anything.\n\n\n1.0.3 Personality\nMr. Whiskers is an extremely affectionate cat who loves to be around people. He has a special fondness for human attention, and will often seek out cuddles and pets from me whenever he’s feeling particularly playful. However, despite his affectionate nature, Mr. Whiskers can also be quite demanding at times. If he doesn’t get the attention or playtime he wants, he’ll let me know about it in loud, insistent meows that can pierce through even the most concentrated work session.\n\n\n1.0.4 Playtime\nOne of my favorite things to do with Mr. Whiskers is engage him in a game of “hide-and-seek.” I’ll hide treats or toys around the house, and encourage Mr. Whiskers to find them for himself. He’s incredibly skilled at this game, using his powerful sense of smell to track down even the most elusive hiding spots.\n\n\n1.0.5 Sleeping Habits\nDespite his energetic personality, Mr. Whiskers is also a heavy sleeper. He’ll often spend hours curled up in a sunbeam or under my bed, snoozing peacefully as I go about my day. It’s not uncommon for me to walk by him without even noticing he’s there, only to have him suddenly wake up and pounce on my arm with a playful swat.\n\n\n1.0.6 Grooming Habits\nMr. Whiskers is extremely fastidious when it comes to his grooming habits. He spends hours each day licking and nuzzling his fur to keep it clean and well-coated. This means that he’s always looking immaculate, even after a particularly messy play session or three.\n\n\n1.0.7 Special Quirks\nOne of Mr. Whiskers’ most endearing quirks is his love of chasing invisible prey. He’ll spend hours stalking imaginary mice or birds around the house, pouncing on them with lightning-fast reflexes only to realize they’re not really there. It’s hilarious to watch him in action, and always leaves me laughing.\n\n\n1.0.8 Favorite Activities\nIn addition to playtime, Mr. Whiskers also enjoys a good scratch behind the ears or under the chin. He’ll sit for hours letting me rub his favorite spots, purring contentedly as I work out the knots and tangles in his fur. Another of his favorite activities is lounging on my lap while I watch TV or read a book.\n\n\n1.0.9 Impact on Daily Life\nDespite his small size, Mr. Whiskers has a significant impact on my daily life. He’s always there to brighten up my day with his antics and affectionate nature. Whether he’s chasing invisible prey or simply curled up in a ball at my feet, Mr. Whiskers is a constant source of comfort and companionship.\n\n\n1.0.10 Benefits of Owning a Cat\nOne of the many benefits of owning a cat like Mr. Whiskers is the sense of calm and relaxation that comes with having him around. Studies have shown that simply petting a cat can lower your blood pressure, heart rate, and stress levels, making it easier to unwind after a long day.\n\n\n1.0.11 Conclusion\nIn conclusion, my life would not be complete without Mr. Whiskers by my side. He’s more than just a pet – he’s a beloved companion, a source of comfort and joy, and an endless source of entertainment. Whether he’s chasing invisible prey or simply lounging in the sun, Mr. Whiskers is always up to something interesting.\n\n\n1.0.12 Final Thoughts\nAs I reflect on my time with Mr. Whiskers, I’m struck by his remarkable adaptability and resilience. Despite his many quirks and eccentricities, he remains a loyal and loving companion who brings endless joy and happiness into my life.\n\n\n\n\ndocument = \"\"\"My Feline Friend¶\nIntroduction\nMy cat, Mr. Whiskers, has been a constant companion in my life for several years now. He is a sleek and agile feline with a shiny coat that comes in a variety of colors depending on the lighting conditions. Despite his refined appearance, Mr. Whiskers is a playful and mischievous creature who always keeps me entertained.\n\nPhysical Characteristics\nMr. Whiskers is a medium-sized cat with a muscular build. He has a short, smooth coat that ranges in color from grey to black, with distinctive white markings on his paws and chest. His eyes are a bright green, and his whiskers (as his name suggests) are incredibly long and sensitive. Despite their impressive length, Mr. Whiskers' whiskers rarely get tangled or caught in anything.\n\nPersonality\nMr. Whiskers is an extremely affectionate cat who loves to be around people. He has a special fondness for human attention, and will often seek out cuddles and pets from me whenever he's feeling particularly playful. However, despite his affectionate nature, Mr. Whiskers can also be quite demanding at times. If he doesn't get the attention or playtime he wants, he'll let me know about it in loud, insistent meows that can pierce through even the most concentrated work session.\n\nPlaytime\nOne of my favorite things to do with Mr. Whiskers is engage him in a game of \"hide-and-seek.\" I'll hide treats or toys around the house, and encourage Mr. Whiskers to find them for himself. He's incredibly skilled at this game, using his powerful sense of smell to track down even the most elusive hiding spots.\n\nSleeping Habits\nDespite his energetic personality, Mr. Whiskers is also a heavy sleeper. He'll often spend hours curled up in a sunbeam or under my bed, snoozing peacefully as I go about my day. It's not uncommon for me to walk by him without even noticing he's there, only to have him suddenly wake up and pounce on my arm with a playful swat.\n\nGrooming Habits\nMr. Whiskers is extremely fastidious when it comes to his grooming habits. He spends hours each day licking and nuzzling his fur to keep it clean and well-coated. This means that he's always looking immaculate, even after a particularly messy play session or three.\n\nSpecial Quirks\nOne of Mr. Whiskers' most endearing quirks is his love of chasing invisible prey. He'll spend hours stalking imaginary mice or birds around the house, pouncing on them with lightning-fast reflexes only to realize they're not really there. It's hilarious to watch him in action, and always leaves me laughing.\n\nFavorite Activities\nIn addition to playtime, Mr. Whiskers also enjoys a good scratch behind the ears or under the chin. He'll sit for hours letting me rub his favorite spots, purring contentedly as I work out the knots and tangles in his fur. Another of his favorite activities is lounging on my lap while I watch TV or read a book.\n\nImpact on Daily Life\nDespite his small size, Mr. Whiskers has a significant impact on my daily life. He's always there to brighten up my day with his antics and affectionate nature. Whether he's chasing invisible prey or simply curled up in a ball at my feet, Mr. Whiskers is a constant source of comfort and companionship.\n\nBenefits of Owning a Cat\nOne of the many benefits of owning a cat like Mr. Whiskers is the sense of calm and relaxation that comes with having him around. Studies have shown that simply petting a cat can lower your blood pressure, heart rate, and stress levels, making it easier to unwind after a long day.\n\nConclusion\nIn conclusion, my life would not be complete without Mr. Whiskers by my side. He's more than just a pet – he's a beloved companion, a source of comfort and joy, and an endless source of entertainment. Whether he's chasing invisible prey or simply lounging in the sun, Mr. Whiskers is always up to something interesting.\n\nFinal Thoughts\nAs I reflect on my time with Mr. Whiskers, I'm struck by his remarkable adaptability and resilience. Despite his many quirks and eccentricities, he remains a loyal and loving companion who brings endless joy and happiness into my life.\"\"\"\n\n\nlen(document)\n\n4084\n\n\n\n# needle in a haystack\nniah = \"Mr. Whiskers has a dog friend named Mr. Husky\"\ndocuments = [niah + document, document + niah, document, niah]\n\n\nfrom fastembed import TextEmbedding\n\nembedding_model = TextEmbedding()\nembeddings = list(embedding_model.embed(documents))\n\n\nimport numpy as np\nfrom qdrant_client.local.distances import cosine_similarity\n\n\nqueries = [\"Who is Mr. Husky?\"]\n\nquery_embeddings = list(embedding_model.query_embed(queries))\ncosine_similarity(np.array(query_embeddings), np.array(embeddings))\n\narray([[0.5864911 , 0.53275806, 0.53275806, 0.71727777]], dtype=float32)\n\n\nWe can observe that the 2nd and 3rd document has the same similarity scores. This is because the document is too long is truncated before passing into the embedding model.",
    "crumbs": [
      "010 Long Context"
    ]
  },
  {
    "objectID": "ar101_002_multilingual_and_multimodal.html",
    "href": "ar101_002_multilingual_and_multimodal.html",
    "title": "Advanced Retrieval 101 Multilingual and Multimodal Search with LlamaIndex",
    "section": "",
    "text": "https://qdrant.tech/documentation/multimodal-search/",
    "crumbs": [
      "Advanced Retrieval 101 Multilingual and Multimodal Search with LlamaIndex"
    ]
  },
  {
    "objectID": "ar101_002_multilingual_and_multimodal.html#vectorize-data",
    "href": "ar101_002_multilingual_and_multimodal.html#vectorize-data",
    "title": "Advanced Retrieval 101 Multilingual and Multimodal Search with LlamaIndex",
    "section": "1 Vectorize Data",
    "text": "1 Vectorize Data\n\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\n\nmodel = HuggingFaceEmbedding(\n    model_name=\"llamaindex/vdr-2b-multi-v1\",\n    device=\"mps\",  # \"mps\" for mac, \"cuda\" for nvidia GPUs, \"cpu\"\n    trust_remote_code=True,\n)\n\n\ndocuments = [\n    {\n        \"caption\": \"An image about plane emergency safety\",\n        \"image\": \"images/place_emergency.jpg\",\n    },\n    {\n        \"caption\": \"An image about airplane components.\",\n        \"image\": \"images/airplane_parts.png\",\n    },\n    {\n        \"caption\": \"An image about COVID safety restrictions.\",\n        \"image\": \"images/coronavirus-safety.jpg\",\n    },\n    {\n        \"caption\": \"A confidential image about UFO sightings\",\n        \"image\": \"images/ufo_sightings.jpeg\",\n    },\n    {\n        \"caption\": \"An image about US stock market news\",\n        \"image\": \"images/us_stock_market_news.png\",\n    },\n]\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[19], line 3\n      1 from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n----&gt; 3 model = HuggingFaceEmbedding(\n      4     model_name=\"llamaindex/vdr-2b-multi-v1\",\n      5     device=\"cpu\",  # \"mps\" for mac, \"cuda\" for nvidia GPUs, \"cpu\"\n      6     trust_remote_code=True,\n      7 )\n     10 documents = [\n     11     {\n     12         \"caption\": \"An image about plane emergency safety\",\n   (...)     30     },\n     31 ]\n\nFile ~/Documents/go/python-fastembed/.venv/lib/python3.12/site-packages/llama_index/embeddings/huggingface/base.py:155, in HuggingFaceEmbedding.__init__(self, model_name, tokenizer_name, pooling, max_length, query_instruction, text_instruction, normalize, model, tokenizer, embed_batch_size, cache_folder, trust_remote_code, device, callback_manager, parallel_process, target_devices, **model_kwargs)\n    152 if model_name is None:\n    153     raise ValueError(\"The `model_name` argument must be provided.\")\n--&gt; 155 model = SentenceTransformer(\n    156     model_name,\n    157     device=device,\n    158     cache_folder=cache_folder,\n    159     trust_remote_code=trust_remote_code,\n    160     prompts={\n    161         \"query\": query_instruction\n    162         or get_query_instruct_for_model_name(model_name),\n    163         \"text\": text_instruction\n    164         or get_text_instruct_for_model_name(model_name),\n    165     },\n    166     **model_kwargs,\n    167 )\n    168 if max_length:\n    169     model.max_seq_length = max_length\n\nFile ~/Documents/go/python-fastembed/.venv/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:308, in SentenceTransformer.__init__(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\n    299         model_name_or_path = __MODEL_HUB_ORGANIZATION__ + \"/\" + model_name_or_path\n    301 if is_sentence_transformer_model(\n    302     model_name_or_path,\n    303     token,\n   (...)    306     local_files_only=local_files_only,\n    307 ):\n--&gt; 308     modules, self.module_kwargs = self._load_sbert_model(\n    309         model_name_or_path,\n    310         token=token,\n    311         cache_folder=cache_folder,\n    312         revision=revision,\n    313         trust_remote_code=trust_remote_code,\n    314         local_files_only=local_files_only,\n    315         model_kwargs=model_kwargs,\n    316         tokenizer_kwargs=tokenizer_kwargs,\n    317         config_kwargs=config_kwargs,\n    318     )\n    319 else:\n    320     modules = self._load_auto_model(\n    321         model_name_or_path,\n    322         token=token,\n   (...)    329         config_kwargs=config_kwargs,\n    330     )\n\nFile ~/Documents/go/python-fastembed/.venv/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:1739, in SentenceTransformer._load_sbert_model(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs)\n   1736 # Try to initialize the module with a lot of kwargs, but only if the module supports them\n   1737 # Otherwise we fall back to the load method\n   1738 try:\n-&gt; 1739     module = module_class(model_name_or_path, cache_dir=cache_folder, backend=self.backend, **kwargs)\n   1740 except TypeError:\n   1741     module = module_class.load(model_name_or_path)\n\nFile ~/.cache/huggingface/modules/transformers_modules/llamaindex/vdr-2b-multi-v1/2c4e54c8db4071cc61fc3c62f4490124e40c37db/custom_st.py:65, in Transformer.__init__(self, model_name_or_path, processor_name_or_path, max_pixels, min_pixels, dimension, max_seq_length, model_args, processor_args, tokenizer_args, config_args, cache_dir, backend, **kwargs)\n     58 self.model = Qwen2VLForConditionalGeneration.from_pretrained(\n     59     model_name_or_path,\n     60     cache_dir=cache_dir,\n     61     **model_kwargs\n     62 ).eval()\n     64 # Initialize processor\n---&gt; 65 self.processor = AutoProcessor.from_pretrained(\n     66     processor_name_or_path or model_name_or_path,\n     67     **processor_kwargs\n     68 )\n     70 # Set padding sides\n     71 self.model.padding_side = \"left\"\n\nFile ~/Documents/go/python-fastembed/.venv/lib/python3.12/site-packages/transformers/models/auto/processing_auto.py:345, in AutoProcessor.from_pretrained(cls, pretrained_model_name_or_path, **kwargs)\n    341     return processor_class.from_pretrained(\n    342         pretrained_model_name_or_path, trust_remote_code=trust_remote_code, **kwargs\n    343     )\n    344 elif processor_class is not None:\n--&gt; 345     return processor_class.from_pretrained(\n    346         pretrained_model_name_or_path, trust_remote_code=trust_remote_code, **kwargs\n    347     )\n    348 # Last try: we use the PROCESSOR_MAPPING.\n    349 elif type(config) in PROCESSOR_MAPPING:\n\nFile ~/Documents/go/python-fastembed/.venv/lib/python3.12/site-packages/transformers/processing_utils.py:1070, in ProcessorMixin.from_pretrained(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\n   1067 if token is not None:\n   1068     kwargs[\"token\"] = token\n-&gt; 1070 args = cls._get_arguments_from_pretrained(pretrained_model_name_or_path, **kwargs)\n   1071 processor_dict, kwargs = cls.get_processor_dict(pretrained_model_name_or_path, **kwargs)\n   1072 processor_dict.update({k: v for k, v in kwargs.items() if k in processor_dict.keys()})\n\nFile ~/Documents/go/python-fastembed/.venv/lib/python3.12/site-packages/transformers/processing_utils.py:1134, in ProcessorMixin._get_arguments_from_pretrained(cls, pretrained_model_name_or_path, **kwargs)\n   1131     else:\n   1132         attribute_class = cls.get_possibly_dynamic_module(class_name)\n-&gt; 1134     args.append(attribute_class.from_pretrained(pretrained_model_name_or_path, **kwargs))\n   1135 return args\n\nFile ~/Documents/go/python-fastembed/.venv/lib/python3.12/site-packages/transformers/models/auto/image_processing_auto.py:557, in AutoImageProcessor.from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs)\n    555     return image_processor_class.from_dict(config_dict, **kwargs)\n    556 elif image_processor_class is not None:\n--&gt; 557     return image_processor_class.from_dict(config_dict, **kwargs)\n    558 # Last try: we use the IMAGE_PROCESSOR_MAPPING.\n    559 elif type(config) in IMAGE_PROCESSOR_MAPPING:\n\nFile ~/Documents/go/python-fastembed/.venv/lib/python3.12/site-packages/transformers/image_processing_base.py:423, in ImageProcessingMixin.from_dict(cls, image_processor_dict, **kwargs)\n    420 if \"crop_size\" in kwargs and \"crop_size\" in image_processor_dict:\n    421     image_processor_dict[\"crop_size\"] = kwargs.pop(\"crop_size\")\n--&gt; 423 image_processor = cls(**image_processor_dict)\n    425 # Update image_processor with kwargs if needed\n    426 to_remove = []\n\nFile ~/Documents/go/python-fastembed/.venv/lib/python3.12/site-packages/transformers/models/qwen2_vl/image_processing_qwen2_vl.py:144, in Qwen2VLImageProcessor.__init__(self, do_resize, size, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std, do_convert_rgb, min_pixels, max_pixels, patch_size, temporal_patch_size, merge_size, **kwargs)\n    142 super().__init__(**kwargs)\n    143 if size is not None and (\"shortest_edge\" not in size or \"longest_edge\" not in size):\n--&gt; 144     raise ValueError(\"size must contain 'shortest_edge' and 'longest_edge' keys.\")\n    145 else:\n    146     size = {\"shortest_edge\": 56 * 56, \"longest_edge\": 28 * 28 * 1280}\n\nValueError: size must contain 'shortest_edge' and 'longest_edge' keys.\n\n\n\n\ntext_embeddings = model.get_text_embedding_batch([doc[\"caption\"] for doc in documents])\nimage_embeddings = model.get_image_embedding_batch([doc[\"image\"] for doc in documents])\n\n\nfrom qdrant_client import QdrantClient, models\n\nclient = QdrantClient(\":memory:\")\nif not client.collection_exists(\"llama-multi\"):\n    client.create_collection(\n        collection_name=\"llama-multi\",\n        vectors_config={\n            \"image\": models.VectorParams(\n                size=len(image_embeddings[0]), distance=models.Distance.COSINE\n            ),\n            \"text\": models.VectorParams(\n                size=len(text_embeddings[0]), distance=models.Distance.COSINE\n            ),\n        },\n    )\n\n\nclient.upload_points(\n    collection_name=\"llama-multi\",\n    points=[\n        models.PointStruct(\n            id=idx,\n            vector={\n                \"text\": text_embeddings[idx],\n                \"image\": image_embeddings[idx],\n            },\n            payload=doc,\n        )\n        for idx, doc in enumerate(documents)\n    ],\n)",
    "crumbs": [
      "Advanced Retrieval 101 Multilingual and Multimodal Search with LlamaIndex"
    ]
  },
  {
    "objectID": "ar101_002_multilingual_and_multimodal.html#search",
    "href": "ar101_002_multilingual_and_multimodal.html#search",
    "title": "Advanced Retrieval 101 Multilingual and Multimodal Search with LlamaIndex",
    "section": "2 Search",
    "text": "2 Search\n\n2.1 Text-to-Image\n\nfrom PIL import Image\n\nfind_image = model.get_query_embedding(\"Bullish on US Equity\")\n\nImage.open(\n    client.query_points(\n        collection_name=\"llama-multi\",\n        query=find_image,\n        using=\"image\",\n        with_payload=[\"image\"],\n        limit=1,\n    )\n    .points[0]\n    .payload[\"image\"]\n)\n\n\n\n2.2 Multilingual Search\n\nImage.open(\n    client.query_points(\n        collection_name=COLLECTION_NAME,\n        # German: Tell me about the mysterious object\n        query=model.get_query_embedding(\"erzähl mir von dem mysteriösen Objekt\"),\n        using=\"image\",\n        with_payload=[\"image\"],\n        limit=1,\n    )\n    .points[0]\n    .payload[\"image\"]\n)",
    "crumbs": [
      "Advanced Retrieval 101 Multilingual and Multimodal Search with LlamaIndex"
    ]
  },
  {
    "objectID": "ss101_001_basic.html",
    "href": "ss101_001_basic.html",
    "title": "Semantic Search 101: Build your first Semantic Search Engine in 5 minutes",
    "section": "",
    "text": "https://qdrant.tech/documentation/beginner-tutorials/search-beginners/\nfrom fastembed import TextEmbedding\nfrom qdrant_client import QdrantClient, models\n\n# [model[\"model\"] for model in TextEmbedding.list_supported_models()]\nembedding_model = TextEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\nmodel = [\n    model\n    for model in TextEmbedding.list_supported_models()\n    if \"all-MiniLM\" in model[\"model\"]\n][0]\nmodel\n\n{'model': 'sentence-transformers/all-MiniLM-L6-v2',\n 'dim': 384,\n 'description': 'Text embeddings, Unimodal (text), English, 256 input tokens truncation, Prefixes for queries/documents: not necessary, 2021 year.',\n 'license': 'apache-2.0',\n 'size_in_GB': 0.09,\n 'sources': {'url': 'https://storage.googleapis.com/qdrant-fastembed/sentence-transformers-all-MiniLM-L6-v2.tar.gz',\n  'hf': 'qdrant/all-MiniLM-L6-v2-onnx'},\n 'model_file': 'model.onnx'}",
    "crumbs": [
      "Semantic Search 101: Build your first Semantic Search Engine in 5 minutes"
    ]
  },
  {
    "objectID": "ss101_001_basic.html#add-the-dataset",
    "href": "ss101_001_basic.html#add-the-dataset",
    "title": "Semantic Search 101: Build your first Semantic Search Engine in 5 minutes",
    "section": "1 Add the dataset",
    "text": "1 Add the dataset\n\ndocuments = [\n    {\n        \"name\": \"The Time Machine\",\n        \"description\": \"A man travels through time and witnesses the evolution of humanity.\",\n        \"author\": \"H.G. Wells\",\n        \"year\": 1895,\n    },\n    {\n        \"name\": \"Ender's Game\",\n        \"description\": \"A young boy is trained to become a military leader in a war against an alien race.\",\n        \"author\": \"Orson Scott Card\",\n        \"year\": 1985,\n    },\n    {\n        \"name\": \"Brave New World\",\n        \"description\": \"A dystopian society where people are genetically engineered and conditioned to conform to a strict social hierarchy.\",\n        \"author\": \"Aldous Huxley\",\n        \"year\": 1932,\n    },\n    {\n        \"name\": \"The Hitchhiker's Guide to the Galaxy\",\n        \"description\": \"A comedic science fiction series following the misadventures of an unwitting human and his alien friend.\",\n        \"author\": \"Douglas Adams\",\n        \"year\": 1979,\n    },\n    {\n        \"name\": \"Dune\",\n        \"description\": \"A desert planet is the site of political intrigue and power struggles.\",\n        \"author\": \"Frank Herbert\",\n        \"year\": 1965,\n    },\n    {\n        \"name\": \"Foundation\",\n        \"description\": \"A mathematician develops a science to predict the future of humanity and works to save civilization from collapse.\",\n        \"author\": \"Isaac Asimov\",\n        \"year\": 1951,\n    },\n    {\n        \"name\": \"Snow Crash\",\n        \"description\": \"A futuristic world where the internet has evolved into a virtual reality metaverse.\",\n        \"author\": \"Neal Stephenson\",\n        \"year\": 1992,\n    },\n    {\n        \"name\": \"Neuromancer\",\n        \"description\": \"A hacker is hired to pull off a near-impossible hack and gets pulled into a web of intrigue.\",\n        \"author\": \"William Gibson\",\n        \"year\": 1984,\n    },\n    {\n        \"name\": \"The War of the Worlds\",\n        \"description\": \"A Martian invasion of Earth throws humanity into chaos.\",\n        \"author\": \"H.G. Wells\",\n        \"year\": 1898,\n    },\n    {\n        \"name\": \"The Hunger Games\",\n        \"description\": \"A dystopian society where teenagers are forced to fight to the death in a televised spectacle.\",\n        \"author\": \"Suzanne Collins\",\n        \"year\": 2008,\n    },\n    {\n        \"name\": \"The Andromeda Strain\",\n        \"description\": \"A deadly virus from outer space threatens to wipe out humanity.\",\n        \"author\": \"Michael Crichton\",\n        \"year\": 1969,\n    },\n    {\n        \"name\": \"The Left Hand of Darkness\",\n        \"description\": \"A human ambassador is sent to a planet where the inhabitants are genderless and can change gender at will.\",\n        \"author\": \"Ursula K. Le Guin\",\n        \"year\": 1969,\n    },\n    {\n        \"name\": \"The Three-Body Problem\",\n        \"description\": \"Humans encounter an alien civilization that lives in a dying system.\",\n        \"author\": \"Liu Cixin\",\n        \"year\": 2008,\n    },\n]\n\n\nembeddings = list(\n    embedding_model.embed([document[\"description\"] for document in documents])\n)",
    "crumbs": [
      "Semantic Search 101: Build your first Semantic Search Engine in 5 minutes"
    ]
  },
  {
    "objectID": "ss101_001_basic.html#create-a-collection",
    "href": "ss101_001_basic.html#create-a-collection",
    "title": "Semantic Search 101: Build your first Semantic Search Engine in 5 minutes",
    "section": "2 Create a collection",
    "text": "2 Create a collection\n\nclient = QdrantClient(\":memory:\")\nclient.create_collection(\n    collection_name=\"my_books\",\n    vectors_config=models.VectorParams(\n        size=model[\"dim\"], distance=models.Distance.COSINE\n    ),\n)\n\nTrue",
    "crumbs": [
      "Semantic Search 101: Build your first Semantic Search Engine in 5 minutes"
    ]
  },
  {
    "objectID": "ss101_001_basic.html#upload-data-to-collection",
    "href": "ss101_001_basic.html#upload-data-to-collection",
    "title": "Semantic Search 101: Build your first Semantic Search Engine in 5 minutes",
    "section": "3 Upload data to collection",
    "text": "3 Upload data to collection\n\nclient.upload_points(\n    collection_name=\"my_books\",\n    points=[\n        models.PointStruct(id=idx, vector=embeddings[idx], payload=doc)\n        for idx, doc in enumerate(documents)\n    ],\n)",
    "crumbs": [
      "Semantic Search 101: Build your first Semantic Search Engine in 5 minutes"
    ]
  },
  {
    "objectID": "ss101_001_basic.html#query",
    "href": "ss101_001_basic.html#query",
    "title": "Semantic Search 101: Build your first Semantic Search Engine in 5 minutes",
    "section": "4 Query",
    "text": "4 Query\n\nhits = client.query_points(\n    collection_name=\"my_books\",\n    query=list(embedding_model.embed(\"alien invasion\"))[0],\n    limit=3,\n).points\n\nfor hit in hits:\n    print(hit.payload, \"score:\", hit.score)\n\n{'name': 'The War of the Worlds', 'description': 'A Martian invasion of Earth throws humanity into chaos.', 'author': 'H.G. Wells', 'year': 1898} score: 0.5700932336027419\n{'name': \"The Hitchhiker's Guide to the Galaxy\", 'description': 'A comedic science fiction series following the misadventures of an unwitting human and his alien friend.', 'author': 'Douglas Adams', 'year': 1979} score: 0.5040469745831534\n{'name': 'The Three-Body Problem', 'description': 'Humans encounter an alien civilization that lives in a dying system.', 'author': 'Liu Cixin', 'year': 2008} score: 0.4590294008142361",
    "crumbs": [
      "Semantic Search 101: Build your first Semantic Search Engine in 5 minutes"
    ]
  },
  {
    "objectID": "ss101_001_basic.html#narrow-down-the-query",
    "href": "ss101_001_basic.html#narrow-down-the-query",
    "title": "Semantic Search 101: Build your first Semantic Search Engine in 5 minutes",
    "section": "5 Narrow down the query",
    "text": "5 Narrow down the query\n\nhits = client.query_points(\n    collection_name=\"my_books\",\n    query=list(embedding_model.embed(\"alien invasion\"))[0],\n    query_filter=models.Filter(\n        must=[models.FieldCondition(key=\"year\", range=models.Range(gte=2000))]\n    ),\n    limit=1,\n).points\n\nfor hit in hits:\n    print(hit.payload, \"score:\", hit.score)\n\n{'name': 'The Three-Body Problem', 'description': 'Humans encounter an alien civilization that lives in a dying system.', 'author': 'Liu Cixin', 'year': 2008} score: 0.4590294008142361",
    "crumbs": [
      "Semantic Search 101: Build your first Semantic Search Engine in 5 minutes"
    ]
  },
  {
    "objectID": "ss101_003_hybrid_search.html",
    "href": "ss101_003_hybrid_search.html",
    "title": "Semantic Search 101: Build a Hybrid Search Service with FastEmbed and Qdrant",
    "section": "",
    "text": "https://qdrant.tech/documentation/beginner-tutorials/hybrid-search-fastembed/\n\n# Import client library\nfrom qdrant_client import QdrantClient, models\n\nclient = QdrantClient(path=\"tmp/startups\")\n\n\n---------------------------------------------------------------------------\nBlockingIOError                           Traceback (most recent call last)\nFile ~/Documents/go/python-fastembed/.venv/lib/python3.12/site-packages/portalocker/portalocker.py:118, in lock(file_, flags)\n    117 try:\n--&gt; 118     LOCKER(file_, flags)\n    119 except OSError as exc_value:\n    120     # Python can use one of several different exception classes to\n    121     # represent timeout (most likely is BlockingIOError and IOError),\n   (...)    126     # inherit) and check the errno (which should be EACCESS or EAGAIN\n    127     # according to the spec).\n\nBlockingIOError: [Errno 35] Resource temporarily unavailable\n\nThe above exception was the direct cause of the following exception:\n\nAlreadyLocked                             Traceback (most recent call last)\nFile ~/Documents/go/python-fastembed/.venv/lib/python3.12/site-packages/qdrant_client/local/qdrant_local.py:134, in QdrantLocal._load(self)\n    133 try:\n--&gt; 134     portalocker.lock(\n    135         self._flock_file,\n    136         portalocker.LockFlags.EXCLUSIVE | portalocker.LockFlags.NON_BLOCKING,\n    137     )\n    138 except portalocker.exceptions.LockException:\n\nFile ~/Documents/go/python-fastembed/.venv/lib/python3.12/site-packages/portalocker/portalocker.py:131, in lock(file_, flags)\n    128 if exc_value.errno in (errno.EACCES, errno.EAGAIN):\n    129     # A timeout exception, wrap this so the outer code knows to try\n    130     # again (if it wants to).\n--&gt; 131     raise exceptions.AlreadyLocked(\n    132         exc_value,\n    133         fh=file_,\n    134     ) from exc_value\n    135 else:\n    136     # Something else went wrong; don't wrap this so we stop\n    137     # immediately.\n\nAlreadyLocked: [Errno 35] Resource temporarily unavailable\n\nDuring handling of the above exception, another exception occurred:\n\nRuntimeError                              Traceback (most recent call last)\nCell In[20], line 4\n      1 # Import client library\n      2 from qdrant_client import QdrantClient,models\n----&gt; 4 client = QdrantClient(path=\"tmp/startups\")\n\nFile ~/Documents/go/python-fastembed/.venv/lib/python3.12/site-packages/qdrant_client/qdrant_client.py:130, in QdrantClient.__init__(self, location, url, port, grpc_port, prefer_grpc, https, api_key, prefix, timeout, host, path, force_disable_check_same_thread, grpc_options, auth_token_provider, cloud_inference, check_compatibility, **kwargs)\n    125     self._client = QdrantLocal(\n    126         location=location,\n    127         force_disable_check_same_thread=force_disable_check_same_thread,\n    128     )\n    129 elif path is not None:\n--&gt; 130     self._client = QdrantLocal(\n    131         location=path,\n    132         force_disable_check_same_thread=force_disable_check_same_thread,\n    133     )\n    134 else:\n    135     if location is not None and url is None:\n\nFile ~/Documents/go/python-fastembed/.venv/lib/python3.12/site-packages/qdrant_client/local/qdrant_local.py:67, in QdrantLocal.__init__(self, location, force_disable_check_same_thread)\n     65 self.aliases: dict[str, str] = {}\n     66 self._flock_file: Optional[TextIOWrapper] = None\n---&gt; 67 self._load()\n     68 self._closed: bool = False\n\nFile ~/Documents/go/python-fastembed/.venv/lib/python3.12/site-packages/qdrant_client/local/qdrant_local.py:139, in QdrantLocal._load(self)\n    134     portalocker.lock(\n    135         self._flock_file,\n    136         portalocker.LockFlags.EXCLUSIVE | portalocker.LockFlags.NON_BLOCKING,\n    137     )\n    138 except portalocker.exceptions.LockException:\n--&gt; 139     raise RuntimeError(\n    140         f\"Storage folder {self.location} is already accessed by another instance of Qdrant client.\"\n    141         f\" If you require concurrent access, use Qdrant server instead.\"\n    142     )\n\nRuntimeError: Storage folder tmp/startups is already accessed by another instance of Qdrant client. If you require concurrent access, use Qdrant server instead.\n\n\n\n\nclient.set_model(\"sentence-transformers/all-MiniLM-L6-v2\")\n\n# comment this line to use dense vectors only\nclient.set_sparse_model(\"prithivida/Splade_PP_en_v1\")\n\n\nvectors_config = client.get_fastembed_vector_params()\nvectors_config\n\n{'fast-all-minilm-l6-v2': VectorParams(size=384, distance=&lt;Distance.COSINE: 'Cosine'&gt;, hnsw_config=None, quantization_config=None, on_disk=None, datatype=None, multivector_config=None)}\n\n\n\nsparse_vectors_config = client.get_fastembed_sparse_vector_params()\nsparse_vectors_config\n\n{'fast-sparse-splade_pp_en_v1': SparseVectorParams(index=SparseIndexParams(full_scan_threshold=None, on_disk=None, datatype=None), modifier=None)}\n\n\n\nif not client.collection_exists(\"startups\"):\n    client.create_collection(\n        collection_name=\"startups\",\n        vectors_config=vectors_config,\n        # comment this line to use dense vectors only\n        sparse_vectors_config=sparse_vectors_config,\n    )\n\n\nimport json\n\npayload_path = \"startups_demo.json\"\nmetadata = []\ndocuments = []\n\nwith open(payload_path) as fd:\n    for line in fd:\n        obj = json.loads(line)\n        documents.append(obj.pop(\"description\"))\n        metadata.append(obj)\n\n\ndocuments = documents[:1000]\nmetadata = metadata[:1000]\n\n\nfrom tqdm.notebook import tqdm\n\n\nclient.add(\n    collection_name=\"startups\",\n    documents=documents,\n    metadata=metadata,\n    # parallel=0,  # Use all available CPU cores to encode data.\n    # Requires wrapping code into if __name__ == '__main__' block\n    ids=tqdm(range(len(documents))),\n)\n\n\ndef search(text: str, query_filter=None):\n    search_result = client.query(\n        collection_name=\"startups\",\n        query_text=text,\n        query_filter=query_filter,  # If you don't want any filters for now\n        limit=5,  # 5 the closest results\n    )\n    # `search_result` contains found vector ids with similarity scores\n    # along with the stored payload\n\n    # Select and return metadata\n    metadata = [hit.metadata for hit in search_result]\n    return metadata\n\n\nsearch(\"robotics\")\n\n[{'document': 'Creative materials for better brand awarness \\nRoboToaster main goal is to assist companies and individuals create brand awareness. Whether you need engaging design, quality video, or amazing events. RoboToaster LLC, strives to create compelling and consistent stories through visual marketing tactics to give ...',\n  'name': 'RoboToaster',\n  'images': 'https://d1qb2nb5cznatu.cloudfront.net/startups/i/128687-71dfa1846b8ebc0da88e1fe933d51c73-thumb_jpg.jpg?buster=1349796491',\n  'alt': 'RoboToaster -  video advertising events brand marketing',\n  'link': 'http://RoboToaster.co',\n  'city': 'Chicago'},\n {'document': \"A new way to trade.\\nDesignByRobots' Trading App automates a critical part of the stock trading process-- making the leap from an incomprehensible number of possible directions to take your strategy development process to a manageable set of back-tested strategies to select from. ...\",\n  'name': 'DesignByRobots',\n  'images': 'https://d1qb2nb5cznatu.cloudfront.net/startups/i/25679-e154adc712fd35fbfeb3b93087d20b8b-thumb_jpg.jpg?buster=1317835125',\n  'alt': 'DesignByRobots -  software trading',\n  'link': 'http://designbyrobots.com/',\n  'city': 'Chicago'},\n {'document': 'Projection immersion (aka virtual reality) beyond #Oculus for Brick & Mortar \\nOur mission is to make immersive technology as purposeful as possible. In order to do this, we employ cost efficient systems in developing vital solutions.\\nWe are developing a core tech + platform (software) for user locomotion in 3D virtual environments. Initial ...',\n  'name': 'Semiautomatic Semiotics',\n  'images': 'https://d1qb2nb5cznatu.cloudfront.net/startups/i/1302-f3947d958b2d41eda43f69185abfd0d5-thumb_jpg.jpg?buster=1315770608',\n  'alt': 'Semiautomatic Semiotics  -  digital signage virtual reality San Jose',\n  'link': 'http://semiautomatic3d.com',\n  'city': 'Chicago'},\n {'document': 'Gesture Recognition Platform\\nWe at Rithmio have pushed the boundaries in the gesture recognition market by providing advanced gesture recognition as a platform. Our software libraries are built to run on any platform, including wearables, smartphones, or any other connected motion sensing ...',\n  'name': 'Rithmio',\n  'images': 'https://d1qb2nb5cznatu.cloudfront.net/startups/i/505509-8455465d442dc9cf2112f2caa4b29771-thumb_jpg.jpg?buster=1412717451',\n  'alt': 'Rithmio -  fitness human computer interaction Wearables',\n  'link': 'http://rithmio.com',\n  'city': 'Chicago'},\n {'document': 'Your apps. Simplified.\\nCloudbot is a mobile and web application that is an efficient solution to having your personal data and relationships scattered around on different services.\\nPeople rely on different applications to access the little bits of their lives saved in the cloud. ...',\n  'name': 'Cloudbot',\n  'images': 'https://d1qb2nb5cznatu.cloudfront.net/startups/i/1873-a83644b8a2d18d4b91742faf5c4eab7b-thumb_jpg.jpg?buster=1315741829',\n  'alt': 'Cloudbot -  mobile messaging social media platforms development platforms',\n  'link': 'http://cloudbot.com',\n  'city': 'Chicago'}]\n\n\n\nquery_filter = models.Filter(\n    must=[models.FieldCondition(key=\"city\", match=models.MatchValue(value=\"Chicago\"))]\n)\nsearch(\"robotic\", query_filter)\n\n[{'document': 'Creative materials for better brand awarness \\nRoboToaster main goal is to assist companies and individuals create brand awareness. Whether you need engaging design, quality video, or amazing events. RoboToaster LLC, strives to create compelling and consistent stories through visual marketing tactics to give ...',\n  'name': 'RoboToaster',\n  'images': 'https://d1qb2nb5cznatu.cloudfront.net/startups/i/128687-71dfa1846b8ebc0da88e1fe933d51c73-thumb_jpg.jpg?buster=1349796491',\n  'alt': 'RoboToaster -  video advertising events brand marketing',\n  'link': 'http://RoboToaster.co',\n  'city': 'Chicago'},\n {'document': \"A new way to trade.\\nDesignByRobots' Trading App automates a critical part of the stock trading process-- making the leap from an incomprehensible number of possible directions to take your strategy development process to a manageable set of back-tested strategies to select from. ...\",\n  'name': 'DesignByRobots',\n  'images': 'https://d1qb2nb5cznatu.cloudfront.net/startups/i/25679-e154adc712fd35fbfeb3b93087d20b8b-thumb_jpg.jpg?buster=1317835125',\n  'alt': 'DesignByRobots -  software trading',\n  'link': 'http://designbyrobots.com/',\n  'city': 'Chicago'},\n {'document': 'Projection immersion (aka virtual reality) beyond #Oculus for Brick & Mortar \\nOur mission is to make immersive technology as purposeful as possible. In order to do this, we employ cost efficient systems in developing vital solutions.\\nWe are developing a core tech + platform (software) for user locomotion in 3D virtual environments. Initial ...',\n  'name': 'Semiautomatic Semiotics',\n  'images': 'https://d1qb2nb5cznatu.cloudfront.net/startups/i/1302-f3947d958b2d41eda43f69185abfd0d5-thumb_jpg.jpg?buster=1315770608',\n  'alt': 'Semiautomatic Semiotics  -  digital signage virtual reality San Jose',\n  'link': 'http://semiautomatic3d.com',\n  'city': 'Chicago'},\n {'document': 'Virtual reality operating system\\nWe believe that the adoption of VR technology will represent a sea change in how people interact with computers and collaborate with each other.\\nAnarchist is a Chicago-based software startup building a virtual reality operating system and developer toolkit. Smart ...',\n  'name': 'Anarchist',\n  'images': 'https://d1qb2nb5cznatu.cloudfront.net/startups/i/470881-ad62c25b4e82d5cd7f4561911e5a7493-thumb_jpg.jpg?buster=1408821642',\n  'alt': 'Anarchist -  productivity software developer tools digital entertainment virtual reality',\n  'link': 'http://anarchist.com',\n  'city': 'Chicago'},\n {'document': 'Your apps. Simplified.\\nCloudbot is a mobile and web application that is an efficient solution to having your personal data and relationships scattered around on different services.\\nPeople rely on different applications to access the little bits of their lives saved in the cloud. ...',\n  'name': 'Cloudbot',\n  'images': 'https://d1qb2nb5cznatu.cloudfront.net/startups/i/1873-a83644b8a2d18d4b91742faf5c4eab7b-thumb_jpg.jpg?buster=1315741829',\n  'alt': 'Cloudbot -  mobile messaging social media platforms development platforms',\n  'link': 'http://cloudbot.com',\n  'city': 'Chicago'}]\n\n\n\nsearch(\"finance\")\n\n[{'document': 'Financial Education for Young Professionals\\nSilver Step educates new financial consumers on the basics of wealth management, and lets them discover service providers that fit their needs.\\nThink of us as Yelp for financial services, with an educational component.',\n  'name': 'Silver Step',\n  'images': 'https://d1qb2nb5cznatu.cloudfront.net/startups/i/31271-7023073ac555ea3a430a4b8dbdbcb615-thumb_jpg.jpg?buster=1325694652',\n  'alt': 'Silver Step -  financial services education',\n  'link': 'http://www.thesilverstep.com',\n  'city': 'Chicago'},\n {'document': 'Changing the way you borrow with safer, faster, better financial products\\nAvant is changing the way consumers borrow money. Utilizing advanced algorithms and machine-learning capabilities, the company offers a unique and highly customized approach to the personal loan process. The combination of technology, analytics and customer service ...',\n  'name': 'Avant',\n  'images': 'https://d1qb2nb5cznatu.cloudfront.net/startups/i/168565-45d9c198ede0987843d850c5a963f084-thumb_jpg.jpg?buster=1434042946',\n  'alt': 'Avant -  analytics machine learning big data web development',\n  'link': 'https://www.avant.com',\n  'city': 'Chicago'},\n {'document': 'Technologically-enhanced financial mentoring for teens\\nMoneythink (MT) targets urban 11th and 12th graders. These students have dreams, but between age 17 and 20, they face many financial decisions that can make or break their future: getting/keeping a job, using a paycheck, credit cards, paying for college. To prepare ...',\n  'name': 'Moneythink',\n  'images': 'https://d1qb2nb5cznatu.cloudfront.net/startups/i/282361-0ae5f20b6d9a7433190bf03064b22902-thumb_jpg.jpg?buster=1382098658',\n  'alt': 'Moneythink -  financial services education ventures for good nonprofits',\n  'link': 'http://moneythink.org',\n  'city': 'Chicago'},\n {'document': 'The Private Network for Investments',\n  'name': 'Fundology',\n  'images': 'https://d1qb2nb5cznatu.cloudfront.net/startups/i/175739-0cbc4996113b629ab83dd3c528eedf8f-thumb_jpg.jpg?buster=1366819766',\n  'alt': 'Fundology -  financial services investment management finance',\n  'link': 'http://www.fundology.com',\n  'city': 'Chicago'},\n {'document': 'Personal Financial Execution (TM) system\\nSpendbot is a Personal Financial Execution solution to create a spending plan, optimize it, and stick to it so as to align spending with informed values. We combine cloud-based data visualization, algorithms, expert systems, and partner integrations to deliver ...',\n  'name': 'Spendbot',\n  'images': 'https://d1qb2nb5cznatu.cloudfront.net/startups/i/247454-ffa9f5098f7664641acc300d26590c9a-thumb_jpg.jpg?buster=1376078533',\n  'alt': 'Spendbot -  lead generation ventures for good personal finance health and wellness',\n  'link': 'http://www.spendbot.com',\n  'city': 'Chicago'}]",
    "crumbs": [
      "Semantic Search 101: Build a Hybrid Search Service with FastEmbed and Qdrant"
    ]
  }
]