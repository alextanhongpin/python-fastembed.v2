[
  {
    "objectID": "ss101_004_measure_and_improve_retrieval_quality.html",
    "href": "ss101_004_measure_and_improve_retrieval_quality.html",
    "title": "Semantic Search 101: Measure and Improve Retrieval Quality in Semantic Search",
    "section": "",
    "text": "https://qdrant.tech/documentation/beginner-tutorials/retrieval-quality/\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\n    \"Qdrant/arxiv-titles-instructorxl-embeddings\", split=\"train\", streaming=True\n)\n\n\n\n\n\ndataset_iterator = iter(dataset)\ntrain_dataset = [next(dataset_iterator) for _ in range(6000)]\ntest_dataset = [next(dataset_iterator) for _ in range(100)]\n\n\nfrom qdrant_client import QdrantClient, models\n\nclient = QdrantClient(path=\"tmp/arxiv-titles\")\nif not client.collection_exists(\"arxiv-titles-instructorxl-embeddings\"):\n    client.create_collection(\n        collection_name=\"arxiv-titles-instructorxl-embeddings\",\n        vectors_config=models.VectorParams(\n            size=768,  # Size of the embeddings generated by InstructorXL model\n            distance=models.Distance.COSINE,\n        ),\n    )\n\n\nclient.upload_points(  # upload_points is available as of qdrant-client v1.7.1\n    collection_name=\"arxiv-titles-instructorxl-embeddings\",\n    points=[\n        models.PointStruct(\n            id=item[\"id\"],\n            vector=item[\"vector\"],\n            payload=item,\n        )\n        for item in train_dataset\n    ],\n)\n\nwhile True:\n    collection_info = client.get_collection(\n        collection_name=\"arxiv-titles-instructorxl-embeddings\"\n    )\n    if collection_info.status == models.CollectionStatus.GREEN:\n        # Collection status is green, which means the indexing is finished\n        break\n\n\ndef avg_precision_at_k(k: int):\n    precisions = []\n    for item in test_dataset:\n        ann_result = client.query_points(\n            collection_name=\"arxiv-titles-instructorxl-embeddings\",\n            query=item[\"vector\"],\n            limit=k,\n        ).points\n\n        knn_result = client.query_points(\n            collection_name=\"arxiv-titles-instructorxl-embeddings\",\n            query=item[\"vector\"],\n            limit=k,\n            search_params=models.SearchParams(\n                exact=True,  # Turns on the exact search mode\n            ),\n        ).points\n\n        # We can calculate the precision@k by comparing the ids of the search results\n        ann_ids = set(item.id for item in ann_result)\n        knn_ids = set(item.id for item in knn_result)\n        precision = len(ann_ids.intersection(knn_ids)) / k\n        precisions.append(precision)\n\n    return sum(precisions) / len(precisions)\n\n\nprint(f\"avg(precision@5) = {avg_precision_at_k(k=5)}\")\n\navg(precision@5) = 1.0\n\n\n\nclient.update_collection(\n    collection_name=\"arxiv-titles-instructorxl-embeddings\",\n    hnsw_config=models.HnswConfigDiff(\n        m=32,  # Increase the number of edges per node from the default 16 to 32\n        ef_construct=200,  # Increase the number of neighbours from the default 100 to 200\n    ),\n)\n\nwhile True:\n    collection_info = client.get_collection(\n        collection_name=\"arxiv-titles-instructorxl-embeddings\"\n    )\n    if collection_info.status == models.CollectionStatus.GREEN:\n        # Collection status is green, which means the indexing is finished\n        break\n\n\nprint(f\"avg(precision@5) = {avg_precision_at_k(k=5)}\")\n\navg(precision@5) = 1.0",
    "crumbs": [
      "Semantic Search 101: Measure and Improve Retrieval Quality in Semantic Search"
    ]
  },
  {
    "objectID": "ss101_002_build_neural_search.html",
    "href": "ss101_002_build_neural_search.html",
    "title": "Semantic Search 101: Build a Neural Search Service",
    "section": "",
    "text": "https://qdrant.tech/documentation/beginner-tutorials/neural-search/\n!curl https://storage.googleapis.com/generall-shared-data/startups_demo.json -O startups_demo.json\n\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 21.1M  100 21.1M    0     0  4564k      0  0:00:04  0:00:04 --:--:-- 5832k\ncurl: (6) Could not resolve host: startups_demo.json\n!head -n 1 startups_demo.json\n\n{\"name\":\"SaferCodes\",\"images\":\"https:\\/\\/safer.codes\\/img\\/brand\\/logo-icon.png\",\"alt\":\"SaferCodes Logo QR codes generator system forms for COVID-19\",\"description\":\"QR codes systems for COVID-19.\\nSimple tools for bars, restaurants, offices, and other small proximity businesses.\",\"link\":\"https:\\/\\/safer.codes\",\"city\":\"Chicago\"}\nimport json\n\nimport numpy as np\nimport pandas as pd\nfrom fastembed import TextEmbedding\nfrom tqdm.notebook import tqdm\nmodel_name = \"sentence-transformers/all-MiniLM-L6-v2\"\nfor model in TextEmbedding.list_supported_models():\n    if model[\"model\"] == model_name:\n        break\n\nmodel\n\n{'model': 'sentence-transformers/all-MiniLM-L6-v2',\n 'dim': 384,\n 'description': 'Text embeddings, Unimodal (text), English, 256 input tokens truncation, Prefixes for queries/documents: not necessary, 2021 year.',\n 'license': 'apache-2.0',\n 'size_in_GB': 0.09,\n 'sources': {'url': 'https://storage.googleapis.com/qdrant-fastembed/sentence-transformers-all-MiniLM-L6-v2.tar.gz',\n  'hf': 'qdrant/all-MiniLM-L6-v2-onnx'},\n 'model_file': 'model.onnx'}\nembedding_model = TextEmbedding(model_name=model_name)",
    "crumbs": [
      "Semantic Search 101: Build a Neural Search Service"
    ]
  },
  {
    "objectID": "ss101_002_build_neural_search.html#prepare-sample-dataset",
    "href": "ss101_002_build_neural_search.html#prepare-sample-dataset",
    "title": "Semantic Search 101: Build a Neural Search Service",
    "section": "1 Prepare sample dataset",
    "text": "1 Prepare sample dataset\n\ndf = pd.read_json(\"startups_demo.json\", lines=True)\ndf.head()\n\n\n\n\n\n\n\n\nname\nimages\nalt\ndescription\nlink\ncity\n\n\n\n\n0\nSaferCodes\nhttps://safer.codes/img/brand/logo-icon.png\nSaferCodes Logo QR codes generator system form...\nQR codes systems for COVID-19.\\nSimple tools f...\nhttps://safer.codes\nChicago\n\n\n1\nHuman Practice\nhttps://d1qb2nb5cznatu.cloudfront.net/startups...\nHuman Practice - health care information tech...\nPoint-of-care word of mouth\\nPreferral is a mo...\nhttp://humanpractice.com\nChicago\n\n\n2\nStyleSeek\nhttps://d1qb2nb5cznatu.cloudfront.net/startups...\nStyleSeek - e-commerce fashion mass customiza...\nPersonalized e-commerce for lifestyle products...\nhttp://styleseek.com\nChicago\n\n\n3\nScout\nhttps://d1qb2nb5cznatu.cloudfront.net/startups...\nScout - security consumer electronics interne...\nHassle-free Home Security\\nScout is a self-ins...\nhttp://www.scoutalarm.com\nChicago\n\n\n4\nInvitation codes\nhttps://invitation.codes/img/inv-brand-fb3.png\nInvitation App - Share referral codes community\nThe referral community\\nInvitation App is a so...\nhttps://invitation.codes\nChicago\n\n\n\n\n\n\n\n\nvectors = list(\n    embedding_model.embed([row.alt + \". \" + row.description for row in df.itertuples()])\n)\n\n\nnp.save(\"startup_vectors.npy\", vectors, allow_pickle=False)",
    "crumbs": [
      "Semantic Search 101: Build a Neural Search Service"
    ]
  },
  {
    "objectID": "ss101_002_build_neural_search.html#upload-data-to-qdrant",
    "href": "ss101_002_build_neural_search.html#upload-data-to-qdrant",
    "title": "Semantic Search 101: Build a Neural Search Service",
    "section": "2 Upload data to qdrant",
    "text": "2 Upload data to qdrant\n\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.models import Distance, VectorParams\n\n\nclient = QdrantClient(\":memory:\")\n\n\nif not client.collection_exists(\"startups\"):\n    client.create_collection(\n        collection_name=\"startups\",\n        vectors_config=VectorParams(size=384, distance=Distance.COSINE),\n    )\n\n\nfd = open(\"startups_demo.json\")\n\n# Payload is now an iterator over startup data\npayload = map(json.loads, fd)\n\n# Load all vectors into memory, numpy array works as iterable afor itself.\n# Other options would be to use Mmap, if you don't want to load all data into RAM.\nvectors = np.load(\"startup_vectors.npy\")\n\n\nclient.upload_collection(\n    collection_name=\"startups\",\n    vectors=vectors,\n    payload=payload,\n    ids=None,  # Vector ids will be assigned automatically\n    batch_size=256,  # How many vectors will be uploaded in a single request?\n)\n\n/var/folders/v5/8v9k6wcn65jbbct8spl3wwsh0000gn/T/ipykernel_59718/2314715236.py:1: UserWarning: Local mode is not recommended for collections with more than 20,000 points. Current collection contains 40474 points. Consider using Qdrant in Docker or Qdrant Cloud for better performance with large datasets.\n  client.upload_collection(",
    "crumbs": [
      "Semantic Search 101: Build a Neural Search Service"
    ]
  },
  {
    "objectID": "ss101_002_build_neural_search.html#build-the-search-api",
    "href": "ss101_002_build_neural_search.html#build-the-search-api",
    "title": "Semantic Search 101: Build a Neural Search Service",
    "section": "3 Build the search API",
    "text": "3 Build the search API\n\ndef search(text: str):\n    # Convert text query into vector.\n    vector = list(embedding_model.embed(text))[0]\n\n    # Use `vector` to search for closest vectors in the collection.\n    search_result = client.query_points(\n        collection_name=\"startups\", query=vector, query_filter=None, limit=5\n    ).points\n\n    payloads = [hit.payload for hit in search_result]\n    return payloads\n\n\nsearch(\"robotic ai\")\n\n[{'name': 'Orchid Robotics',\n  'images': 'https://d1qb2nb5cznatu.cloudfront.net/startups/i/281423-bd01451f06bd2888ebf239580d502263-thumb_jpg.jpg?buster=1392329500',\n  'alt': 'Orchid Robotics -  robotics machine learning artificial intelligence industrial automation',\n  'description': \"Advanced Software for Robots\\nGoogle's machine learning techniques applied to robotics.\",\n  'link': 'http://www.orchidrobotics.com',\n  'city': 'Boston'},\n {'name': 'Robotbase',\n  'images': 'https://d1qb2nb5cznatu.cloudfront.net/startups/i/190707-d6f705aa7d803faa6bd5c33c281698b4-thumb_jpg.jpg?buster=1420224362',\n  'alt': 'Robotbase -  robotics artificial intelligence internet of things hardware + software',\n  'description': \"The World's First Artificial Intelligence Personal Robot\\nORDER NOW ON KICKSTARTER\\nhttps://www.kickstarter.com/projects/403524037/personal-robot\\nMeet the world’s first Artificial Intelligence Personal Robot. \\xa0\\nShe's a friend, a multi-talented personal assistant, an awesome photographer, a reliable security guard, a ...\",\n  'link': 'http://www.robotbase.com',\n  'city': 'New York'},\n {'name': 'Robots Alive - Simplified Robotics',\n  'images': 'https://d1qb2nb5cznatu.cloudfront.net/startups/i/477280-85f224c8ed6429bd9b65b33506aaa34e-thumb_jpg.jpg?buster=1409540159',\n  'alt': 'Robots Alive - Simplified Robotics -  manufacturing robotics industrial automation fmcg',\n  'description': 'Simplistic Industrial Robots for the SME\\nWe are a Bangalore based robotics technology firm working on simple and easy to use industrial robots for the SME\\nOur robots are equipped with intelligent software which allows the SME machine operator to program the application on the robot in a few minutes\\nHence ...',\n  'link': 'http://www.robots-alive.com',\n  'city': 'Bangalore'},\n {'name': 'Robotic Wares',\n  'images': 'https://d1qb2nb5cznatu.cloudfront.net/startups/i/484731-0aedee32a15b36c57132b952cbf15506-thumb_jpg.jpg?buster=1410386454',\n  'alt': 'Robotic Wares - ',\n  'description': 'Get the Latest News, Analysis, Opinion & Multimedia from the world of Business & Finance.',\n  'link': 'http://www.livemint.com/',\n  'city': 'New Delhi'},\n {'name': 'Neurala',\n  'images': 'https://d1qb2nb5cznatu.cloudfront.net/startups/i/136219-e6edc37bb162e0dc5ecb7629435010e0-thumb_jpg.jpg?buster=1352511607',\n  'alt': 'Neurala -  robotics artificial intelligence video conferencing software',\n  'description': 'Building brains for bots.\\nThe Neurala for telepresence software provides robots with learning, intelligence, and autonomy. Neurala frees you from driving a robot and instead allows you to focus on communicating and interacting with someone.\\nA robot preloaded with Neurala’s software will ...',\n  'link': 'http://neurala.com',\n  'city': 'Boston'}]\n\n\n\nsearch(\"financial\")\n\n[{'name': 'Finomial',\n  'images': 'https://d1qb2nb5cznatu.cloudfront.net/startups/i/65946-437a75bc2055ce89a40402860c2a53b6-thumb_jpg.jpg?buster=1408376313',\n  'alt': 'Finomial -  finance',\n  'description': '',\n  'link': 'http://finomial.com',\n  'city': 'New York'},\n {'name': 'U.S. Fiduciary',\n  'images': 'https://angel.co/images/shared/nopic_startup.png',\n  'alt': 'U.S. Fiduciary -  finance',\n  'description': '',\n  'link': 'http://www.usfiduciary.com',\n  'city': 'Houston'},\n {'name': 'American Express',\n  'images': 'https://d1qb2nb5cznatu.cloudfront.net/startups/i/42490-41d65934240d652086fde74873a443fa-thumb_jpg.jpg?buster=1326850941',\n  'alt': 'American Express -  finance',\n  'description': '',\n  'link': 'https://www.americanexpress.com',\n  'city': 'New York'},\n {'name': 'Paymentus',\n  'images': 'https://d1qb2nb5cznatu.cloudfront.net/startups/i/63392-f817eedec057e834b69dacb82c5c1f38-thumb_jpg.jpg?buster=1408893681',\n  'alt': 'Paymentus -  finance',\n  'description': '',\n  'link': 'http://www.paymentus.com',\n  'city': 'Atlanta'},\n {'name': 'FinancialAsk',\n  'images': 'https://d1qb2nb5cznatu.cloudfront.net/startups/i/367328-7805129c5017b6bd7fb575d17276c23a-thumb_jpg.jpg?buster=1417054915',\n  'alt': 'FinancialAsk -  personal finance finance technology',\n  'description': 'A financial advice marketplace\\nFinancialAsk makes financial advice easy to get via apps and the web. \\xa0It connects qualified financial advisers with consumers in a transparent marketplace that makes financial advices accessible and affordable.\\nUsers simply ask a financial question which is answered ...',\n  'link': 'http://www.andrewlai.org',\n  'city': 'Melbourne'}]",
    "crumbs": [
      "Semantic Search 101: Build a Neural Search Service"
    ]
  },
  {
    "objectID": "glossary.html",
    "href": "glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "%load_ext jupyter_ai\n%config AiMagics.default_language_model = \"ollama:llama3.2\"\n\n\n%%ai\n\n\"what is text embedding\"\n\n1 # Text Embedding\nText embedding is a technique used to represent text data as numerical vectors, allowing for efficient and effective processing of natural language inputs.\n\n1.0.1 What’s the Problem?\nTraditional approaches to text analysis involve converting text into numerical representations using techniques such as bag-of-words or term-frequency, inverse-document-frequency (TF-IDF). However, these methods have limitations:\n\nHigh dimensionality: Text data has a vast number of features (words), resulting in high-dimensional vectors that are computationally expensive to process and store.\nLack of semantic meaning: These representations do not capture the underlying semantic relationships between words.\n\n\n\n1.0.2 Solution: Embeddings\nText embeddings aim to learn a lower-dimensional, dense representation of text data that captures its semantic meaning. This is achieved through complex machine learning models, such as:\n\nWord2Vec\nGloVe\nBERT (Bidirectional Encoder Representations from Transformers)\n\nThese models generate vector representations for words or phrases in the language, which can be used for various NLP tasks, including:\n\nText classification\nSentiment analysis\nInformation retrieval\n\n\n\n1.0.3 Types of Embeddings\nThere are two main types of embeddings:\n\nFixed-length embeddings: These are obtained using word2vec or glove models and have a fixed vector length.\nContext-dependent embeddings: These are generated by BERT and other transformer-based models, which consider the context in which a word is used to generate its representation.\n\n\n\n1.0.4 Benefits\nText embeddings offer several benefits:\n\nEfficient processing: Embeddings can be processed using efficient algorithms and data structures, making them suitable for large-scale applications.\nImproved performance: Embeddings capture semantic relationships between words, leading to improved performance on NLP tasks.\nFlexibility: Embeddings can be used as input features for machine learning models or as a standalone representation.\n\n\n\n\n\n%%ai\n\n\"how is sparse text embedding different from dense text embedding? keep it simple\"\n\n2 # Sparse vs Dense Text Embeddings\nText embeddings are representations of text data as numerical vectors. The main difference between sparse and dense embeddings lies in their dimensionality.\n\n2.0.1 Dense Text Embeddings\n\nFixed: Every word or phrase has a fixed, non-zero value in the embedding vector.\nHigher-dimensional: Typically have a higher number of dimensions to capture more nuanced semantic relationships.\nMore computationally expensive: Require more memory and computational resources to store and process.\n\nExample:\n Dense Embedding: [0.1, 0.2, 0.3, 0.4, ...]\n\n\n2.0.2 Sparse Text Embeddings\n\nVariable: Only non-zero values are stored in the embedding vector.\nLower-dimensional: Have fewer dimensions compared to dense embeddings, reducing computational costs and storage requirements.\nLess accurate for complex relationships: May not capture as nuanced semantic relationships due to reduced dimensionality.\n\nExample:\n Sparse Embedding: [0.1, 0.2]\nIn summary, sparse text embeddings are more efficient in terms of memory and computation but may sacrifice some accuracy in capturing complex semantic relationships.\n\n\n\n\n%%ai\n\n\"how is sparse text embedding different from bm25 search?\"\n\n3 # Sparse Text Embedding vs BM25 Search\nBM25 (Bayesian Multiplicative Weighted) is a widely used algorithm for information retrieval and text search, while sparse text embeddings represent text data as numerical vectors.\n\n3.0.1 Key Differences\n\nPurpose:\n\nBM25: Designed for ranking and retrieving documents based on relevance to a query.\nSparse Text Embeddings: Used for NLP tasks such as text classification, sentiment analysis, and information retrieval.\n\nDimensionality:\n\nBM25: Treats each word as a feature, resulting in a high-dimensional vector space.\nSparse Text Embeddings: Typically have a much lower dimensionality (e.g., 128-512) compared to the number of unique words in a corpus.\n\nWeighting and Scoring:\n\nBM25: Assigns weights to each word based on its importance, relevance, and rarity in the document.\nSparse Text Embeddings: Uses learned representations (e.g., word2vec) to capture semantic meaning, often without explicit weighting or scoring.\n\nQuery Representation:\n\nBM25: Typically represents a query as a bag-of-words, using term frequencies and inverse documents frequencies.\nSparse Text Embeddings: Often uses a dense representation of the query (e.g., dense vector) to compute similarity scores.\n\n\n\n\n3.0.2 Example Workflow\nBM25 Search:\n\nPreprocess text data (tokenization, stemming, etc.)\nCompute BM25 weights for each word in a document\nRank documents based on their relevance to the query\n\nSparse Text Embeddings:\n\nPreprocess text data (tokenization, stemming, etc.)\nLearn dense vector representations for words using techniques like word2vec\nUse these vectors as input features for NLP tasks or compute similarity scores",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "e_001_data_ingestion.html",
    "href": "e_001_data_ingestion.html",
    "title": "Essentials: Data Ingestion for Beginners",
    "section": "",
    "text": "https://qdrant.tech/documentation/data-ingestion-beginners/\n\nfrom fastembed import ImageEmbedding, TextEmbedding\nfrom PIL import Image\nfrom qdrant_client import QdrantClient, models\n\n\nimage_model = ImageEmbedding(model_name=\"Qdrant/clip-ViT-B-32-vision\")\ntext_model = TextEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n\n\nclient = QdrantClient(\":memory:\")\nclient.create_collection(\n    collection_name=\"products-data\",\n    vectors_config={\n        \"text_embedding\": models.VectorParams(\n            size=384,\n            distance=models.Distance.COSINE,\n        ),\n        \"image_embedding\": models.VectorParams(\n            size=512, distance=models.Distance.COSINE\n        ),\n    },\n)\n\nTrue\n\n\n\nfrom langchain_community.document_loaders import DirectoryLoader\n\n\npdf_loader = DirectoryLoader(\"data\", glob=\"**/*.pdf\", show_progress=False)\npdfs = pdf_loader.load()\n\n\nunique_id = 1\n\nfor idx, doc in enumerate(pdfs):\n    unique_id += 1\n    source = doc.metadata[\"source\"]\n    content = doc.page_content\n    embedding = list(text_model.embed(content))[0]\n    client.upload_points(\n        collection_name=\"products-data\",\n        points=[\n            models.PointStruct(\n                id=unique_id,\n                vector={\"text_embedding\": embedding},\n                payload={\"review\": content, \"source\": source},\n            )\n        ],\n    )\n\n\nimage_loader = DirectoryLoader(\n    \"images\",\n    glob=[\"**/*.png\", \"**/*.jpg\"],\n    show_progress=False,\n)\nimages = image_loader.load()\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\n\n\nfor idx, doc in enumerate(images):\n    unique_id += 1\n    source = doc.metadata[\"source\"]\n    image = Image.open(source)\n    content = doc.page_content\n\n    text_embedding = list(text_model.embed(content))[0]\n    image_embedding = list(image_model.embed(image))[0]\n    client.upload_points(\n        collection_name=\"products-data\",\n        points=[\n            models.PointStruct(\n                id=unique_id,\n                vector={\n                    \"text_embedding\": embedding,\n                    \"image_embedding\": image_embedding,\n                },\n                payload={\"review\": content, \"source\": source},\n            )\n        ],\n    )\n\n\nhits = client.query_points(\n    collection_name=\"products-data\",\n    query=list(text_model.query_embed(\"Singapore Johor\"))[0],\n    using=\"text_embedding\",\n)\n\n\nhits.points[0].payload[\"source\"]\n\n'data/Singapore winners.pdf'\n\n\n\nhits = client.query_points(\n    collection_name=\"products-data\",\n    query=list(image_model.embed(Image.open(\"images/wiki_computer_science.png\")))[0],\n    using=\"image_embedding\",\n)\n\n\nhits.points[0].payload[\"source\"]\n\n'images/wiki_computer_science.png'",
    "crumbs": [
      "Essentials: Data Ingestion for Beginners"
    ]
  },
  {
    "objectID": "ar101_002_multilingual_and_multimodal.html",
    "href": "ar101_002_multilingual_and_multimodal.html",
    "title": "Advanced Retrieval 101 Multilingual and Multimodal Search with LlamaIndex",
    "section": "",
    "text": "https://qdrant.tech/documentation/multimodal-search/",
    "crumbs": [
      "Advanced Retrieval 101 Multilingual and Multimodal Search with LlamaIndex"
    ]
  },
  {
    "objectID": "ar101_002_multilingual_and_multimodal.html#vectorize-data",
    "href": "ar101_002_multilingual_and_multimodal.html#vectorize-data",
    "title": "Advanced Retrieval 101 Multilingual and Multimodal Search with LlamaIndex",
    "section": "1 Vectorize Data",
    "text": "1 Vectorize Data\n\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\n\nmodel = HuggingFaceEmbedding(\n    model_name=\"llamaindex/vdr-2b-multi-v1\",\n    device=\"mps\",  # \"mps\" for mac, \"cuda\" for nvidia GPUs, \"cpu\"\n    trust_remote_code=True,\n)\n\n\ndocuments = [\n    {\n        \"caption\": \"An image about plane emergency safety\",\n        \"image\": \"images/place_emergency.jpg\",\n    },\n    {\n        \"caption\": \"An image about airplane components.\",\n        \"image\": \"images/airplane_parts.png\",\n    },\n    {\n        \"caption\": \"An image about COVID safety restrictions.\",\n        \"image\": \"images/coronavirus-safety.jpg\",\n    },\n    {\n        \"caption\": \"A confidential image about UFO sightings\",\n        \"image\": \"images/ufo_sightings.jpeg\",\n    },\n    {\n        \"caption\": \"An image about US stock market news\",\n        \"image\": \"images/us_stock_market_news.png\",\n    },\n]\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[19], line 3\n      1 from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n----&gt; 3 model = HuggingFaceEmbedding(\n      4     model_name=\"llamaindex/vdr-2b-multi-v1\",\n      5     device=\"cpu\",  # \"mps\" for mac, \"cuda\" for nvidia GPUs, \"cpu\"\n      6     trust_remote_code=True,\n      7 )\n     10 documents = [\n     11     {\n     12         \"caption\": \"An image about plane emergency safety\",\n   (...)     30     },\n     31 ]\n\nFile ~/Documents/go/python-fastembed/.venv/lib/python3.12/site-packages/llama_index/embeddings/huggingface/base.py:155, in HuggingFaceEmbedding.__init__(self, model_name, tokenizer_name, pooling, max_length, query_instruction, text_instruction, normalize, model, tokenizer, embed_batch_size, cache_folder, trust_remote_code, device, callback_manager, parallel_process, target_devices, **model_kwargs)\n    152 if model_name is None:\n    153     raise ValueError(\"The `model_name` argument must be provided.\")\n--&gt; 155 model = SentenceTransformer(\n    156     model_name,\n    157     device=device,\n    158     cache_folder=cache_folder,\n    159     trust_remote_code=trust_remote_code,\n    160     prompts={\n    161         \"query\": query_instruction\n    162         or get_query_instruct_for_model_name(model_name),\n    163         \"text\": text_instruction\n    164         or get_text_instruct_for_model_name(model_name),\n    165     },\n    166     **model_kwargs,\n    167 )\n    168 if max_length:\n    169     model.max_seq_length = max_length\n\nFile ~/Documents/go/python-fastembed/.venv/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:308, in SentenceTransformer.__init__(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\n    299         model_name_or_path = __MODEL_HUB_ORGANIZATION__ + \"/\" + model_name_or_path\n    301 if is_sentence_transformer_model(\n    302     model_name_or_path,\n    303     token,\n   (...)    306     local_files_only=local_files_only,\n    307 ):\n--&gt; 308     modules, self.module_kwargs = self._load_sbert_model(\n    309         model_name_or_path,\n    310         token=token,\n    311         cache_folder=cache_folder,\n    312         revision=revision,\n    313         trust_remote_code=trust_remote_code,\n    314         local_files_only=local_files_only,\n    315         model_kwargs=model_kwargs,\n    316         tokenizer_kwargs=tokenizer_kwargs,\n    317         config_kwargs=config_kwargs,\n    318     )\n    319 else:\n    320     modules = self._load_auto_model(\n    321         model_name_or_path,\n    322         token=token,\n   (...)    329         config_kwargs=config_kwargs,\n    330     )\n\nFile ~/Documents/go/python-fastembed/.venv/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:1739, in SentenceTransformer._load_sbert_model(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs)\n   1736 # Try to initialize the module with a lot of kwargs, but only if the module supports them\n   1737 # Otherwise we fall back to the load method\n   1738 try:\n-&gt; 1739     module = module_class(model_name_or_path, cache_dir=cache_folder, backend=self.backend, **kwargs)\n   1740 except TypeError:\n   1741     module = module_class.load(model_name_or_path)\n\nFile ~/.cache/huggingface/modules/transformers_modules/llamaindex/vdr-2b-multi-v1/2c4e54c8db4071cc61fc3c62f4490124e40c37db/custom_st.py:65, in Transformer.__init__(self, model_name_or_path, processor_name_or_path, max_pixels, min_pixels, dimension, max_seq_length, model_args, processor_args, tokenizer_args, config_args, cache_dir, backend, **kwargs)\n     58 self.model = Qwen2VLForConditionalGeneration.from_pretrained(\n     59     model_name_or_path,\n     60     cache_dir=cache_dir,\n     61     **model_kwargs\n     62 ).eval()\n     64 # Initialize processor\n---&gt; 65 self.processor = AutoProcessor.from_pretrained(\n     66     processor_name_or_path or model_name_or_path,\n     67     **processor_kwargs\n     68 )\n     70 # Set padding sides\n     71 self.model.padding_side = \"left\"\n\nFile ~/Documents/go/python-fastembed/.venv/lib/python3.12/site-packages/transformers/models/auto/processing_auto.py:345, in AutoProcessor.from_pretrained(cls, pretrained_model_name_or_path, **kwargs)\n    341     return processor_class.from_pretrained(\n    342         pretrained_model_name_or_path, trust_remote_code=trust_remote_code, **kwargs\n    343     )\n    344 elif processor_class is not None:\n--&gt; 345     return processor_class.from_pretrained(\n    346         pretrained_model_name_or_path, trust_remote_code=trust_remote_code, **kwargs\n    347     )\n    348 # Last try: we use the PROCESSOR_MAPPING.\n    349 elif type(config) in PROCESSOR_MAPPING:\n\nFile ~/Documents/go/python-fastembed/.venv/lib/python3.12/site-packages/transformers/processing_utils.py:1070, in ProcessorMixin.from_pretrained(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\n   1067 if token is not None:\n   1068     kwargs[\"token\"] = token\n-&gt; 1070 args = cls._get_arguments_from_pretrained(pretrained_model_name_or_path, **kwargs)\n   1071 processor_dict, kwargs = cls.get_processor_dict(pretrained_model_name_or_path, **kwargs)\n   1072 processor_dict.update({k: v for k, v in kwargs.items() if k in processor_dict.keys()})\n\nFile ~/Documents/go/python-fastembed/.venv/lib/python3.12/site-packages/transformers/processing_utils.py:1134, in ProcessorMixin._get_arguments_from_pretrained(cls, pretrained_model_name_or_path, **kwargs)\n   1131     else:\n   1132         attribute_class = cls.get_possibly_dynamic_module(class_name)\n-&gt; 1134     args.append(attribute_class.from_pretrained(pretrained_model_name_or_path, **kwargs))\n   1135 return args\n\nFile ~/Documents/go/python-fastembed/.venv/lib/python3.12/site-packages/transformers/models/auto/image_processing_auto.py:557, in AutoImageProcessor.from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs)\n    555     return image_processor_class.from_dict(config_dict, **kwargs)\n    556 elif image_processor_class is not None:\n--&gt; 557     return image_processor_class.from_dict(config_dict, **kwargs)\n    558 # Last try: we use the IMAGE_PROCESSOR_MAPPING.\n    559 elif type(config) in IMAGE_PROCESSOR_MAPPING:\n\nFile ~/Documents/go/python-fastembed/.venv/lib/python3.12/site-packages/transformers/image_processing_base.py:423, in ImageProcessingMixin.from_dict(cls, image_processor_dict, **kwargs)\n    420 if \"crop_size\" in kwargs and \"crop_size\" in image_processor_dict:\n    421     image_processor_dict[\"crop_size\"] = kwargs.pop(\"crop_size\")\n--&gt; 423 image_processor = cls(**image_processor_dict)\n    425 # Update image_processor with kwargs if needed\n    426 to_remove = []\n\nFile ~/Documents/go/python-fastembed/.venv/lib/python3.12/site-packages/transformers/models/qwen2_vl/image_processing_qwen2_vl.py:144, in Qwen2VLImageProcessor.__init__(self, do_resize, size, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std, do_convert_rgb, min_pixels, max_pixels, patch_size, temporal_patch_size, merge_size, **kwargs)\n    142 super().__init__(**kwargs)\n    143 if size is not None and (\"shortest_edge\" not in size or \"longest_edge\" not in size):\n--&gt; 144     raise ValueError(\"size must contain 'shortest_edge' and 'longest_edge' keys.\")\n    145 else:\n    146     size = {\"shortest_edge\": 56 * 56, \"longest_edge\": 28 * 28 * 1280}\n\nValueError: size must contain 'shortest_edge' and 'longest_edge' keys.\n\n\n\n\ntext_embeddings = model.get_text_embedding_batch([doc[\"caption\"] for doc in documents])\nimage_embeddings = model.get_image_embedding_batch([doc[\"image\"] for doc in documents])\n\n\nfrom qdrant_client import QdrantClient, models\n\nclient = QdrantClient(\":memory:\")\nif not client.collection_exists(\"llama-multi\"):\n    client.create_collection(\n        collection_name=\"llama-multi\",\n        vectors_config={\n            \"image\": models.VectorParams(\n                size=len(image_embeddings[0]), distance=models.Distance.COSINE\n            ),\n            \"text\": models.VectorParams(\n                size=len(text_embeddings[0]), distance=models.Distance.COSINE\n            ),\n        },\n    )\n\n\nclient.upload_points(\n    collection_name=\"llama-multi\",\n    points=[\n        models.PointStruct(\n            id=idx,\n            vector={\n                \"text\": text_embeddings[idx],\n                \"image\": image_embeddings[idx],\n            },\n            payload=doc,\n        )\n        for idx, doc in enumerate(documents)\n    ],\n)",
    "crumbs": [
      "Advanced Retrieval 101 Multilingual and Multimodal Search with LlamaIndex"
    ]
  },
  {
    "objectID": "ar101_002_multilingual_and_multimodal.html#search",
    "href": "ar101_002_multilingual_and_multimodal.html#search",
    "title": "Advanced Retrieval 101 Multilingual and Multimodal Search with LlamaIndex",
    "section": "2 Search",
    "text": "2 Search\n\n2.1 Text-to-Image\n\nfrom PIL import Image\n\nfind_image = model.get_query_embedding(\"Bullish on US Equity\")\n\nImage.open(\n    client.query_points(\n        collection_name=\"llama-multi\",\n        query=find_image,\n        using=\"image\",\n        with_payload=[\"image\"],\n        limit=1,\n    )\n    .points[0]\n    .payload[\"image\"]\n)\n\n\n\n2.2 Multilingual Search\n\nImage.open(\n    client.query_points(\n        collection_name=COLLECTION_NAME,\n        # German: Tell me about the mysterious object\n        query=model.get_query_embedding(\"erzähl mir von dem mysteriösen Objekt\"),\n        using=\"image\",\n        with_payload=[\"image\"],\n        limit=1,\n    )\n    .points[0]\n    .payload[\"image\"]\n)",
    "crumbs": [
      "Advanced Retrieval 101 Multilingual and Multimodal Search with LlamaIndex"
    ]
  },
  {
    "objectID": "010_long_context.html",
    "href": "010_long_context.html",
    "title": "1 My Feline Friend",
    "section": "",
    "text": "%load_ext jupyter_ai\n%config AiMagics.default_language_model = \"ollama:llama3.2\"\n%%ai\n\n\"Generate a super long text of 1000 words about my cat\"\n\n1 My Feline Friend\n\n1.0.1 Introduction\nMy cat, Mr. Whiskers, has been a constant companion in my life for several years now. He is a sleek and agile feline with a shiny coat that comes in a variety of colors depending on the lighting conditions. Despite his refined appearance, Mr. Whiskers is a playful and mischievous creature who always keeps me entertained.\n\n\n1.0.2 Physical Characteristics\nMr. Whiskers is a medium-sized cat with a muscular build. He has a short, smooth coat that ranges in color from grey to black, with distinctive white markings on his paws and chest. His eyes are a bright green, and his whiskers (as his name suggests) are incredibly long and sensitive. Despite their impressive length, Mr. Whiskers’ whiskers rarely get tangled or caught in anything.\n\n\n1.0.3 Personality\nMr. Whiskers is an extremely affectionate cat who loves to be around people. He has a special fondness for human attention, and will often seek out cuddles and pets from me whenever he’s feeling particularly playful. However, despite his affectionate nature, Mr. Whiskers can also be quite demanding at times. If he doesn’t get the attention or playtime he wants, he’ll let me know about it in loud, insistent meows that can pierce through even the most concentrated work session.\n\n\n1.0.4 Playtime\nOne of my favorite things to do with Mr. Whiskers is engage him in a game of “hide-and-seek.” I’ll hide treats or toys around the house, and encourage Mr. Whiskers to find them for himself. He’s incredibly skilled at this game, using his powerful sense of smell to track down even the most elusive hiding spots.\n\n\n1.0.5 Sleeping Habits\nDespite his energetic personality, Mr. Whiskers is also a heavy sleeper. He’ll often spend hours curled up in a sunbeam or under my bed, snoozing peacefully as I go about my day. It’s not uncommon for me to walk by him without even noticing he’s there, only to have him suddenly wake up and pounce on my arm with a playful swat.\n\n\n1.0.6 Grooming Habits\nMr. Whiskers is extremely fastidious when it comes to his grooming habits. He spends hours each day licking and nuzzling his fur to keep it clean and well-coated. This means that he’s always looking immaculate, even after a particularly messy play session or three.\n\n\n1.0.7 Special Quirks\nOne of Mr. Whiskers’ most endearing quirks is his love of chasing invisible prey. He’ll spend hours stalking imaginary mice or birds around the house, pouncing on them with lightning-fast reflexes only to realize they’re not really there. It’s hilarious to watch him in action, and always leaves me laughing.\n\n\n1.0.8 Favorite Activities\nIn addition to playtime, Mr. Whiskers also enjoys a good scratch behind the ears or under the chin. He’ll sit for hours letting me rub his favorite spots, purring contentedly as I work out the knots and tangles in his fur. Another of his favorite activities is lounging on my lap while I watch TV or read a book.\n\n\n1.0.9 Impact on Daily Life\nDespite his small size, Mr. Whiskers has a significant impact on my daily life. He’s always there to brighten up my day with his antics and affectionate nature. Whether he’s chasing invisible prey or simply curled up in a ball at my feet, Mr. Whiskers is a constant source of comfort and companionship.\n\n\n1.0.10 Benefits of Owning a Cat\nOne of the many benefits of owning a cat like Mr. Whiskers is the sense of calm and relaxation that comes with having him around. Studies have shown that simply petting a cat can lower your blood pressure, heart rate, and stress levels, making it easier to unwind after a long day.\n\n\n1.0.11 Conclusion\nIn conclusion, my life would not be complete without Mr. Whiskers by my side. He’s more than just a pet – he’s a beloved companion, a source of comfort and joy, and an endless source of entertainment. Whether he’s chasing invisible prey or simply lounging in the sun, Mr. Whiskers is always up to something interesting.\n\n\n1.0.12 Final Thoughts\nAs I reflect on my time with Mr. Whiskers, I’m struck by his remarkable adaptability and resilience. Despite his many quirks and eccentricities, he remains a loyal and loving companion who brings endless joy and happiness into my life.\ndocument = \"\"\"My Feline Friend¶\nIntroduction\nMy cat, Mr. Whiskers, has been a constant companion in my life for several years now. He is a sleek and agile feline with a shiny coat that comes in a variety of colors depending on the lighting conditions. Despite his refined appearance, Mr. Whiskers is a playful and mischievous creature who always keeps me entertained.\n\nPhysical Characteristics\nMr. Whiskers is a medium-sized cat with a muscular build. He has a short, smooth coat that ranges in color from grey to black, with distinctive white markings on his paws and chest. His eyes are a bright green, and his whiskers (as his name suggests) are incredibly long and sensitive. Despite their impressive length, Mr. Whiskers' whiskers rarely get tangled or caught in anything.\n\nPersonality\nMr. Whiskers is an extremely affectionate cat who loves to be around people. He has a special fondness for human attention, and will often seek out cuddles and pets from me whenever he's feeling particularly playful. However, despite his affectionate nature, Mr. Whiskers can also be quite demanding at times. If he doesn't get the attention or playtime he wants, he'll let me know about it in loud, insistent meows that can pierce through even the most concentrated work session.\n\nPlaytime\nOne of my favorite things to do with Mr. Whiskers is engage him in a game of \"hide-and-seek.\" I'll hide treats or toys around the house, and encourage Mr. Whiskers to find them for himself. He's incredibly skilled at this game, using his powerful sense of smell to track down even the most elusive hiding spots.\n\nSleeping Habits\nDespite his energetic personality, Mr. Whiskers is also a heavy sleeper. He'll often spend hours curled up in a sunbeam or under my bed, snoozing peacefully as I go about my day. It's not uncommon for me to walk by him without even noticing he's there, only to have him suddenly wake up and pounce on my arm with a playful swat.\n\nGrooming Habits\nMr. Whiskers is extremely fastidious when it comes to his grooming habits. He spends hours each day licking and nuzzling his fur to keep it clean and well-coated. This means that he's always looking immaculate, even after a particularly messy play session or three.\n\nSpecial Quirks\nOne of Mr. Whiskers' most endearing quirks is his love of chasing invisible prey. He'll spend hours stalking imaginary mice or birds around the house, pouncing on them with lightning-fast reflexes only to realize they're not really there. It's hilarious to watch him in action, and always leaves me laughing.\n\nFavorite Activities\nIn addition to playtime, Mr. Whiskers also enjoys a good scratch behind the ears or under the chin. He'll sit for hours letting me rub his favorite spots, purring contentedly as I work out the knots and tangles in his fur. Another of his favorite activities is lounging on my lap while I watch TV or read a book.\n\nImpact on Daily Life\nDespite his small size, Mr. Whiskers has a significant impact on my daily life. He's always there to brighten up my day with his antics and affectionate nature. Whether he's chasing invisible prey or simply curled up in a ball at my feet, Mr. Whiskers is a constant source of comfort and companionship.\n\nBenefits of Owning a Cat\nOne of the many benefits of owning a cat like Mr. Whiskers is the sense of calm and relaxation that comes with having him around. Studies have shown that simply petting a cat can lower your blood pressure, heart rate, and stress levels, making it easier to unwind after a long day.\n\nConclusion\nIn conclusion, my life would not be complete without Mr. Whiskers by my side. He's more than just a pet – he's a beloved companion, a source of comfort and joy, and an endless source of entertainment. Whether he's chasing invisible prey or simply lounging in the sun, Mr. Whiskers is always up to something interesting.\n\nFinal Thoughts\nAs I reflect on my time with Mr. Whiskers, I'm struck by his remarkable adaptability and resilience. Despite his many quirks and eccentricities, he remains a loyal and loving companion who brings endless joy and happiness into my life.\"\"\"\nlen(document)\n\n4084\n# needle in a haystack\nniah = \"Mr. Whiskers has a dog friend named Mr. Husky\"\ndocuments = [niah + document, document + niah, document, niah]\nfrom fastembed import TextEmbedding\n\nembedding_model = TextEmbedding()\nembeddings = list(embedding_model.embed(documents))\nimport numpy as np\nfrom qdrant_client.local.distances import cosine_similarity\nqueries = [\"Who is Mr. Husky?\"]\n\nquery_embeddings = list(embedding_model.query_embed(queries))\ncosine_similarity(np.array(query_embeddings), np.array(embeddings))\n\narray([[0.5864911 , 0.53275806, 0.53275806, 0.71727777]], dtype=float32)\nWe can observe that the 2nd and 3rd document has the same similarity scores. This is because the document is too long is truncated before passing into the embedding model.",
    "crumbs": [
      "Splitting text"
    ]
  },
  {
    "objectID": "010_long_context.html#splitting-text",
    "href": "010_long_context.html#splitting-text",
    "title": "1 My Feline Friend",
    "section": "1.1 Splitting text",
    "text": "1.1 Splitting text\n\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nniah = \"Singapore is beautiful\"\nqueries = [\"Tell me about Singapore\"]\nquery_embeddings = list(embedding_model.embed(queries))\n\n# We can test different chunk size to see if the last content got \"cut-off\".\nfor chunk_size in [240, 360, 480, 540, 1080, 2160, 4320, 8640]:\n    recursive_splitter = RecursiveCharacterTextSplitter(\n        separators=[\"\\n\\n\", \"\\n\", r\"(?&lt;=[.?!])\\s+\"],\n        keep_separator=False,\n        is_separator_regex=True,\n        chunk_size=chunk_size,\n        chunk_overlap=36,\n    )\n    docs = recursive_splitter.create_documents([document])\n    text = docs[0].page_content\n    embedding = list(embedding_model.embed([text, text + niah, niah]))\n    base_score, suffix_score, niah_score = cosine_similarity(\n        np.array(query_embeddings), np.array(embedding)\n    )[0]\n    if base_score &lt; suffix_score:\n        print(chunk_size, len(text), base_score, suffix_score)\n\n240 30 0.5020638 0.5304578\n360 353 0.44394454 0.5407039\n480 353 0.44394454 0.5407039\n540 353 0.44394454 0.5407039\n1080 763 0.4186567 0.51292914\n2160 1925 0.43664977 0.47356755\n\n\n\nfor model in TextEmbedding.list_supported_models():\n    if model[\"model\"] == embedding_model.model_name:\n        print(model)\n        break\n\n{'model': 'BAAI/bge-small-en-v1.5', 'dim': 384, 'description': 'Text embeddings, Unimodal (text), English, 512 input tokens truncation, Prefixes for queries/documents: not so necessary, 2023 year.', 'license': 'mit', 'size_in_GB': 0.067, 'sources': {'hf': 'qdrant/bge-small-en-v1.5-onnx-q'}, 'model_file': 'model_optimized.onnx'}",
    "crumbs": [
      "Splitting text"
    ]
  },
  {
    "objectID": "007_reranking_fastembed.html",
    "href": "007_reranking_fastembed.html",
    "title": "Reranking with fastembed",
    "section": "",
    "text": "https://qdrant.tech/documentation/fastembed/fastembed-rerankers/\nfrom fastembed import TextEmbedding\nfrom fastembed.rerank.cross_encoder import TextCrossEncoder\n[model[\"model\"] for model in TextCrossEncoder.list_supported_models()]\n\n['Xenova/ms-marco-MiniLM-L-6-v2',\n 'Xenova/ms-marco-MiniLM-L-12-v2',\n 'BAAI/bge-reranker-base',\n 'jinaai/jina-reranker-v1-tiny-en',\n 'jinaai/jina-reranker-v1-turbo-en',\n 'jinaai/jina-reranker-v2-base-multilingual']\ndense_embedding_model = TextEmbedding(\"sentence-transformers/all-MiniLM-L6-v2\")\nreranker = TextCrossEncoder(model_name=\"jinaai/jina-reranker-v2-base-multilingual\")",
    "crumbs": [
      "Reranking with fastembed"
    ]
  },
  {
    "objectID": "007_reranking_fastembed.html#embed-and-index-data-for-the-first-stage-retrieval",
    "href": "007_reranking_fastembed.html#embed-and-index-data-for-the-first-stage-retrieval",
    "title": "Reranking with fastembed",
    "section": "1 Embed and index data for the first-stage retrieval",
    "text": "1 Embed and index data for the first-stage retrieval\n\ndescriptions = [\n    \"In 1431, Jeanne d'Arc is placed on trial on charges of heresy. The ecclesiastical jurists attempt to force Jeanne to recant her claims of holy visions.\",\n    \"A film projectionist longs to be a detective, and puts his meagre skills to work when he is framed by a rival for stealing his girlfriend's father's pocketwatch.\",\n    \"A group of high-end professional thieves start to feel the heat from the LAPD when they unknowingly leave a clue at their latest heist.\",\n    \"A petty thief with an utter resemblance to a samurai warlord is hired as the lord's double. When the warlord later dies the thief is forced to take up arms in his place.\",\n    \"A young boy named Kubo must locate a magical suit of armour worn by his late father in order to defeat a vengeful spirit from the past.\",\n    \"A biopic detailing the 2 decades that Punjabi Sikh revolutionary Udham Singh spent planning the assassination of the man responsible for the Jallianwala Bagh massacre.\",\n    \"When a machine that allows therapists to enter their patients' dreams is stolen, all hell breaks loose. Only a young female therapist, Paprika, can stop it.\",\n    \"An ordinary word processor has the worst night of his life after he agrees to visit a girl in Soho whom he met that evening at a coffee shop.\",\n    \"A story that revolves around drug abuse in the affluent north Indian State of Punjab and how the youth there have succumbed to it en-masse resulting in a socio-economic decline.\",\n    \"A world-weary political journalist picks up the story of a woman's search for her son, who was taken away from her decades ago after she became pregnant and was forced to live in a convent.\",\n    \"Concurrent theatrical ending of the TV series Neon Genesis Evangelion (1995).\",\n    \"During World War II, a rebellious U.S. Army Major is assigned a dozen convicted murderers to train and lead them into a mass assassination mission of German officers.\",\n    \"The toys are mistakenly delivered to a day-care center instead of the attic right before Andy leaves for college, and it's up to Woody to convince the other toys that they weren't abandoned and to return home.\",\n    \"A soldier fighting aliens gets to relive the same day over and over again, the day restarting every time he dies.\",\n    \"After two male musicians witness a mob hit, they flee the state in an all-female band disguised as women, but further complications set in.\",\n    \"Exiled into the dangerous forest by her wicked stepmother, a princess is rescued by seven dwarf miners who make her part of their household.\",\n    \"A renegade reporter trailing a young runaway heiress for a big story joins her on a bus heading from Florida to New York, and they end up stuck with each other when the bus leaves them behind at one of the stops.\",\n    \"Story of 40-man Turkish task force who must defend a relay station.\",\n    \"Spinal Tap, one of England's loudest bands, is chronicled by film director Marty DiBergi on what proves to be a fateful tour.\",\n    \"Oskar, an overlooked and bullied boy, finds love and revenge through Eli, a beautiful but peculiar girl.\",\n]\n\n\ndescriptions_embeddings = list(dense_embedding_model.embed(descriptions))\nlen(descriptions_embeddings[0])\n\n384\n\n\n\nfrom qdrant_client import QdrantClient, models\n\nqdrant_client = QdrantClient(\":memory:\")  # Qdrant is running from RAM.\n\n\nqdrant_client.create_collection(\n    collection_name=\"movies\",\n    vectors_config={\n        \"embedding\": models.VectorParams(\n            size=384, distance=models.Distance.COSINE  # Size of all-MiniLM-L6-v2\n        )\n    },\n)\n\nTrue\n\n\n\nqdrant_client.upload_points(\n    collection_name=\"movies\",\n    points=[\n        models.PointStruct(\n            id=idx, payload={\"description\": description}, vector={\"embedding\": vector}\n        )\n        for idx, (description, vector) in enumerate(\n            zip(descriptions, descriptions_embeddings)\n        )\n    ],\n)",
    "crumbs": [
      "Reranking with fastembed"
    ]
  },
  {
    "objectID": "007_reranking_fastembed.html#first-stage-retrieval",
    "href": "007_reranking_fastembed.html#first-stage-retrieval",
    "title": "Reranking with fastembed",
    "section": "2 First-stage retrieval",
    "text": "2 First-stage retrieval\n\nquery = \"A story about a strong historically significant female figure.\"\nquery_embedded = list(dense_embedding_model.query_embed(query))[0]\n\ninitial_retrieval = qdrant_client.query_points(\n    collection_name=\"movies\",\n    using=\"embedding\",\n    query=query_embedded,\n    with_payload=True,\n    limit=10,\n)\n\n\ndescription_hits = []\nfor i, hit in enumerate(initial_retrieval.points):\n    print(f'Result number {i+1} is \"{hit.payload['description']}\"')\n    description_hits.append(hit.payload[\"description\"])\n\nResult number 1 is \"A world-weary political journalist picks up the story of a woman's search for her son, who was taken away from her decades ago after she became pregnant and was forced to live in a convent.\"\nResult number 2 is \"Exiled into the dangerous forest by her wicked stepmother, a princess is rescued by seven dwarf miners who make her part of their household.\"\nResult number 3 is \"Oskar, an overlooked and bullied boy, finds love and revenge through Eli, a beautiful but peculiar girl.\"\nResult number 4 is \"A renegade reporter trailing a young runaway heiress for a big story joins her on a bus heading from Florida to New York, and they end up stuck with each other when the bus leaves them behind at one of the stops.\"\nResult number 5 is \"A story that revolves around drug abuse in the affluent north Indian State of Punjab and how the youth there have succumbed to it en-masse resulting in a socio-economic decline.\"\nResult number 6 is \"After two male musicians witness a mob hit, they flee the state in an all-female band disguised as women, but further complications set in.\"\nResult number 7 is \"When a machine that allows therapists to enter their patients' dreams is stolen, all hell breaks loose. Only a young female therapist, Paprika, can stop it.\"\nResult number 8 is \"An ordinary word processor has the worst night of his life after he agrees to visit a girl in Soho whom he met that evening at a coffee shop.\"\nResult number 9 is \"A biopic detailing the 2 decades that Punjabi Sikh revolutionary Udham Singh spent planning the assassination of the man responsible for the Jallianwala Bagh massacre.\"\nResult number 10 is \"In 1431, Jeanne d'Arc is placed on trial on charges of heresy. The ecclesiastical jurists attempt to force Jeanne to recant her claims of holy visions.\"\n\n\n\nnew_scores = list(reranker.rerank(query, description_hits))\nnew_scores\n\n[-1.7871119976043701,\n -1.1165943145751953,\n -1.2816978693008423,\n -1.8776179552078247,\n -2.721012830734253,\n -2.2455098628997803,\n -2.380913257598877,\n -2.8468973636627197,\n -2.8494515419006348,\n -0.6084094047546387]\n\n\n\nranking = [(i, score) for i, score in enumerate(new_scores)]\nranking.sort(key=lambda x: x[1], reverse=True)\n\nfor i, rank in enumerate(ranking):\n    print(f'Reranked result number {i+1} is \"{description_hits[rank[0]]}\"')\n\nReranked result number 1 is \"In 1431, Jeanne d'Arc is placed on trial on charges of heresy. The ecclesiastical jurists attempt to force Jeanne to recant her claims of holy visions.\"\nReranked result number 2 is \"Exiled into the dangerous forest by her wicked stepmother, a princess is rescued by seven dwarf miners who make her part of their household.\"\nReranked result number 3 is \"Oskar, an overlooked and bullied boy, finds love and revenge through Eli, a beautiful but peculiar girl.\"\nReranked result number 4 is \"A world-weary political journalist picks up the story of a woman's search for her son, who was taken away from her decades ago after she became pregnant and was forced to live in a convent.\"\nReranked result number 5 is \"A renegade reporter trailing a young runaway heiress for a big story joins her on a bus heading from Florida to New York, and they end up stuck with each other when the bus leaves them behind at one of the stops.\"\nReranked result number 6 is \"After two male musicians witness a mob hit, they flee the state in an all-female band disguised as women, but further complications set in.\"\nReranked result number 7 is \"When a machine that allows therapists to enter their patients' dreams is stolen, all hell breaks loose. Only a young female therapist, Paprika, can stop it.\"\nReranked result number 8 is \"A story that revolves around drug abuse in the affluent north Indian State of Punjab and how the youth there have succumbed to it en-masse resulting in a socio-economic decline.\"\nReranked result number 9 is \"An ordinary word processor has the worst night of his life after he agrees to visit a girl in Soho whom he met that evening at a coffee shop.\"\nReranked result number 10 is \"A biopic detailing the 2 decades that Punjabi Sikh revolutionary Udham Singh spent planning the assassination of the man responsible for the Jallianwala Bagh massacre.\"",
    "crumbs": [
      "Reranking with fastembed"
    ]
  },
  {
    "objectID": "005_splade.html",
    "href": "005_splade.html",
    "title": "How to generate Sparse Vectors with SPLADE",
    "section": "",
    "text": "https://qdrant.tech/documentation/fastembed/fastembed-splade/\nfrom fastembed import SparseEmbedding, SparseTextEmbedding\nmodels = SparseTextEmbedding.list_supported_models()\nmodels[0]\n\n{'model': 'prithivida/Splade_PP_en_v1',\n 'sources': {'hf': 'Qdrant/SPLADE_PP_en_v1', 'url': None},\n 'model_file': 'model.onnx',\n 'description': 'Independent Implementation of SPLADE++ Model for English.',\n 'license': 'apache-2.0',\n 'size_in_GB': 0.532,\n 'additional_files': [],\n 'requires_idf': None,\n 'vocab_size': 30522}\nmodel_name = \"prithivida/Splade_PP_en_v1\"\nmodel = SparseTextEmbedding(model_name=model_name)",
    "crumbs": [
      "How to generate Sparse Vectors with SPLADE"
    ]
  },
  {
    "objectID": "005_splade.html#embed-data",
    "href": "005_splade.html#embed-data",
    "title": "How to generate Sparse Vectors with SPLADE",
    "section": "1 Embed data",
    "text": "1 Embed data\n\ndocuments: list[str] = [\n    \"Chandrayaan-3 is India's third lunar mission\",\n    \"It aimed to land a rover on the Moon's surface - joining the US, China and Russia\",\n    \"The mission is a follow-up to Chandrayaan-2, which had partial success\",\n    \"Chandrayaan-3 will be launched by the Indian Space Research Organisation (ISRO)\",\n    \"The estimated cost of the mission is around $35 million\",\n    \"It will carry instruments to study the lunar surface and atmosphere\",\n    \"Chandrayaan-3 landed on the Moon's surface on 23rd August 2023\",\n    \"It consists of a lander named Vikram and a rover named Pragyan similar to Chandrayaan-2. Its propulsion module would act like an orbiter.\",\n    \"The propulsion module carries the lander and rover configuration until the spacecraft is in a 100-kilometre (62 mi) lunar orbit\",\n    \"The mission used GSLV Mk III rocket for its launch\",\n    \"Chandrayaan-3 was launched from the Satish Dhawan Space Centre in Sriharikota\",\n    \"Chandrayaan-3 was launched earlier in the year 2023\",\n]\n\n\nsparse_embeddings_list: list[SparseEmbedding] = list(\n    model.embed(documents, batch_size=6)\n)",
    "crumbs": [
      "How to generate Sparse Vectors with SPLADE"
    ]
  },
  {
    "objectID": "005_splade.html#retrieve-embedding",
    "href": "005_splade.html#retrieve-embedding",
    "title": "How to generate Sparse Vectors with SPLADE",
    "section": "2 Retrieve Embedding",
    "text": "2 Retrieve Embedding\n\nindex = 0\nsparse_embeddings_list[index]\n\nSparseEmbedding(values=array([0.05297276, 0.01963477, 0.3645905 , 1.38508415, 0.7177667 ,\n       0.12668137, 0.46230468, 0.44676718, 0.26896986, 1.01519763,\n       1.56553161, 0.29411644, 1.53102267, 0.59785521, 1.10018086,\n       0.02078829, 0.09955899, 0.44248503, 0.09748027, 1.53519893,\n       1.36765647, 0.15741006, 0.49882478, 0.38628468, 0.76612252,\n       1.2580502 , 0.39058524, 0.27236614, 0.45152271, 0.48261923,\n       0.26085106, 1.35912812, 0.70710599, 1.71639597]), indices=array([ 1010,  1011,  1016,  1017,  2001,  2018,  2034,  2093,  2117,\n        2319,  2353,  2509,  2634,  2686,  2796,  2817,  2922,  2959,\n        3003,  3148,  3260,  3390,  3462,  3523,  3822,  4231,  4316,\n        4774,  5590,  5871,  6416, 11926, 12076, 16469]))",
    "crumbs": [
      "How to generate Sparse Vectors with SPLADE"
    ]
  },
  {
    "objectID": "005_splade.html#examine-weights",
    "href": "005_splade.html#examine-weights",
    "title": "How to generate Sparse Vectors with SPLADE",
    "section": "3 Examine weights",
    "text": "3 Examine weights\n\nfor i in range(5):\n    print(\n        f\"Token at index {sparse_embeddings_list[0].indices[i]} has weight {sparse_embeddings_list[0].values[i]}\"\n    )\n\nToken at index 1010 has weight 0.05297275632619858\nToken at index 1011 has weight 0.01963476650416851\nToken at index 1016 has weight 0.36459049582481384\nToken at index 1017 has weight 1.3850841522216797\nToken at index 2001 has weight 0.7177667021751404",
    "crumbs": [
      "How to generate Sparse Vectors with SPLADE"
    ]
  },
  {
    "objectID": "005_splade.html#analyze-results",
    "href": "005_splade.html#analyze-results",
    "title": "How to generate Sparse Vectors with SPLADE",
    "section": "4 Analyze results",
    "text": "4 Analyze results\n\nimport json\n\nfrom tokenizers import Tokenizer\n\ntokenizer = Tokenizer.from_pretrained(\n    SparseTextEmbedding.list_supported_models()[0][\"sources\"][\"hf\"]\n)\n\n\n\n\n\ndef get_tokens_and_weights(sparse_embedding, tokenizer):\n    token_weight_dict = {}\n    for i in range(len(sparse_embedding.indices)):\n        token = tokenizer.decode([sparse_embedding.indices[i]])\n        weight = sparse_embedding.values[i]\n        token_weight_dict[token] = weight\n\n    # Sort the dictionary by weights.\n    token_weight_dict = dict(\n        sorted(token_weight_dict.items(), key=lambda item: item[1], reverse=True)\n    )\n    return token_weight_dict\n\n\nprint(\n    json.dumps(\n        get_tokens_and_weights(sparse_embeddings_list[index], tokenizer), indent=4\n    )\n)\n\n{\n    \"chandra\": 1.7163959741592407,\n    \"third\": 1.565531611442566,\n    \"##ya\": 1.5351989269256592,\n    \"india\": 1.5310226678848267,\n    \"3\": 1.3850841522216797,\n    \"mission\": 1.3676564693450928,\n    \"lunar\": 1.3591281175613403,\n    \"moon\": 1.2580502033233643,\n    \"indian\": 1.1001808643341064,\n    \"##an\": 1.0151976346969604,\n    \"3rd\": 0.7661225199699402,\n    \"was\": 0.7177667021751404,\n    \"spacecraft\": 0.7071059942245483,\n    \"space\": 0.5978552103042603,\n    \"flight\": 0.4988247752189636,\n    \"satellite\": 0.4826192259788513,\n    \"first\": 0.4623046815395355,\n    \"expedition\": 0.45152270793914795,\n    \"three\": 0.4467671811580658,\n    \"fourth\": 0.4424850344657898,\n    \"vehicle\": 0.3905852437019348,\n    \"iii\": 0.3862846791744232,\n    \"2\": 0.36459049582481384,\n    \"##3\": 0.29411643743515015,\n    \"planet\": 0.27236613631248474,\n    \"second\": 0.268969863653183,\n    \"missions\": 0.26085105538368225,\n    \"launched\": 0.15741005539894104,\n    \"had\": 0.1266813725233078,\n    \"largest\": 0.0995589941740036,\n    \"leader\": 0.09748027473688126,\n    \",\": 0.05297275632619858,\n    \"study\": 0.02078828774392605,\n    \"-\": 0.01963476650416851\n}",
    "crumbs": [
      "How to generate Sparse Vectors with SPLADE"
    ]
  },
  {
    "objectID": "003_fastembed_text_embeddings.html",
    "href": "003_fastembed_text_embeddings.html",
    "title": "How to generate Text Embeddings with FastEmbed",
    "section": "",
    "text": "from fastembed import TextEmbedding\ndocuments: [str] = [\n    \"FastEmbed is lighter than Transformers & Sentence-Transformers.\",\n    \"FastEmbed is supported by and maintained by Qdrant.\",\n    \"I have a cat\",\n]\nembedding_model = TextEmbedding()\nembedding_model.model_name\n\n'BAAI/bge-small-en-v1.5'\nembeddings_generator = embedding_model.embed(documents)\nembeddings_list = list(embeddings_generator)\nembeddings_list[0][:10]\n\narray([-0.09479211,  0.01008398, -0.03087804,  0.02379127,  0.00236447,\n        0.00065356, -0.08248352,  0.00084713,  0.03719218,  0.01438666],\n      dtype=float32)",
    "crumbs": [
      "How to generate Text Embeddings with FastEmbed"
    ]
  },
  {
    "objectID": "003_fastembed_text_embeddings.html#calculating-similarity",
    "href": "003_fastembed_text_embeddings.html#calculating-similarity",
    "title": "How to generate Text Embeddings with FastEmbed",
    "section": "1 Calculating similarity",
    "text": "1 Calculating similarity\n\nimport numpy as np\nfrom qdrant_client import models\nfrom qdrant_client.local.distances import (\n    calculate_distance,\n    cosine_similarity,\n    dot_product,\n    euclidean_distance,\n    manhattan_distance,\n)\n\n\ndistance_types = [\n    cosine_similarity,\n    dot_product,\n    euclidean_distance,\n    manhattan_distance,\n]\n\n\nembeddings = np.array(embeddings_list)\n\nfor distance_type in distance_types:\n    print(distance_type.__name__, distance_type(embeddings[:1], embeddings[1:]))\n\ncosine_similarity [[0.6716511 0.4618737]]\ndot_product [[0.6716511 0.4618737]]\neuclidean_distance [[0.810369 1.037426]]\nmanhattan_distance [[12.591358 16.071075]]\n\n\n\nfor distance_type in list(models.Distance):\n    print(\n        distance_type, calculate_distance(embeddings[:1], embeddings[1:], distance_type)\n    )\n\nCosine [[0.6716511 0.4618737]]\nEuclid [[0.810369 1.037426]]\nDot [[0.6716511 0.4618737]]\nManhattan [[12.591358 16.071075]]",
    "crumbs": [
      "How to generate Text Embeddings with FastEmbed"
    ]
  },
  {
    "objectID": "003_fastembed_text_embeddings.html#finding-similarity",
    "href": "003_fastembed_text_embeddings.html#finding-similarity",
    "title": "How to generate Text Embeddings with FastEmbed",
    "section": "2 Finding similarity",
    "text": "2 Finding similarity\n\nquery = \"I need a pet\"\n\nembeddings_generator = embedding_model.embed(query)\nembeddings_list = list(embeddings_generator)\n\n\nscores = calculate_distance(\n    np.array(embeddings_list), embeddings, distance_type=models.Distance.COSINE\n)\nscores\n\narray([[0.433192  , 0.39380783, 0.77847326]], dtype=float32)\n\n\n\nrecommendations = sorted(zip(documents, scores[0]), key=lambda x: x[1], reverse=True)\nfor doc, score in recommendations:\n    print(doc, score)\n\nI have a cat 0.77847326\nFastEmbed is lighter than Transformers & Sentence-Transformers. 0.433192\nFastEmbed is supported by and maintained by Qdrant. 0.39380783",
    "crumbs": [
      "How to generate Text Embeddings with FastEmbed"
    ]
  },
  {
    "objectID": "001_basic.html",
    "href": "001_basic.html",
    "title": "Text Embedding",
    "section": "",
    "text": "In this notebook, we will learn how to generate text embeddings using fastembed library.\n\n%load_ext jupyter_ai\n%config AiMagics.default_language_model = \"ollama:llama3.2\"\n\nThe jupyter_ai extension is already loaded. To reload it, use:\n  %reload_ext jupyter_ai\n\n\n\n%%ai\n\n\"what is text embedding? keep it short and simple\"\n\n\nText embedding is a technique to represent words or phrases as numerical vectors, called embeddings, that capture their semantic meaning.\nKey Features:\n\nNumerical Representation: Words are converted into dense, high-dimensional vectors.\nSimilarity Measures: Distance between word vectors can be used to measure semantic similarity.\nContextual Understanding: Embeddings can capture relationships between words in a sentence or document.\n\nExample: Word “dog” might be represented as an embedding that is closer to the word “cat” than “house”, even though they are not semantically similar.\n\n\n\n# \"Create a wordcloud for the topic text embedding\"\n# \"Create a simple syllabus for learning the following topic text embedding\"",
    "crumbs": [
      "Text Embedding"
    ]
  },
  {
    "objectID": "001_basic.html#goal",
    "href": "001_basic.html#goal",
    "title": "Text Embedding",
    "section": "",
    "text": "In this notebook, we will learn how to generate text embeddings using fastembed library.\n\n%load_ext jupyter_ai\n%config AiMagics.default_language_model = \"ollama:llama3.2\"\n\nThe jupyter_ai extension is already loaded. To reload it, use:\n  %reload_ext jupyter_ai\n\n\n\n%%ai\n\n\"what is text embedding? keep it short and simple\"\n\n\nText embedding is a technique to represent words or phrases as numerical vectors, called embeddings, that capture their semantic meaning.\nKey Features:\n\nNumerical Representation: Words are converted into dense, high-dimensional vectors.\nSimilarity Measures: Distance between word vectors can be used to measure semantic similarity.\nContextual Understanding: Embeddings can capture relationships between words in a sentence or document.\n\nExample: Word “dog” might be represented as an embedding that is closer to the word “cat” than “house”, even though they are not semantically similar.\n\n\n\n# \"Create a wordcloud for the topic text embedding\"\n# \"Create a simple syllabus for learning the following topic text embedding\"",
    "crumbs": [
      "Text Embedding"
    ]
  },
  {
    "objectID": "001_basic.html#dense-text-embedding",
    "href": "001_basic.html#dense-text-embedding",
    "title": "Text Embedding",
    "section": "1.1 Dense text embedding",
    "text": "1.1 Dense text embedding\n\n%%ai\n\n\"Explain dense text embedding, keep it simple. Demonstrate some example. Give some real-world example\"\n\n1.2 What are Dense Text Embeddings?\nDense text embeddings are a way to convert text into numerical representations that capture its meaning and context.\n\n1.2.1 How Do They Work?\n\nVectorize Words: Turn words into vectors (numbers) that represent their meaning.\nLearned Through Data: The model learns from a massive amount of text data and generates these vectors based on the relationships between words.\n\n\n\n1.2.2 Example:\n\n“Dog” -&gt; [0.8, 0.2, 0.9]\n“Cat” -&gt; [0.7, 0.3, 0.8]\n“Car” -&gt; [0.1, 0.9, 0.2]\n“Apple” -&gt; [0.3, 0.5, 0.7]\n\nNotice how similar words are closer together.\n\n\n1.2.3 Real-World Examples:\n\nSearch Engines: Convert search queries into embeddings to find relevant documents.\nRecommendation Systems: Use embeddings to suggest movies or songs based on user preferences.\nSentiment Analysis: Analyze text to determine its sentiment (positive, negative, neutral).\nChatbots & Conversational AI: Embeddings help chatbots understand user input and respond accordingly.\n\n\n\n1.2.4 Key Benefits:\n\nCapture complex relationships between words\nEfficiently represent large amounts of text data\nEnable applications like search, recommendation, and sentiment analysis\n\n\n\n\n\nfrom fastembed import TextEmbedding\n\n# Example list of documents\ndocuments: list[str] = [\n    \"This is built to be faster and lighter than other embedding libraries e.g. Transformers, Sentence-Transformers, etc.\",\n    \"fastembed is supported by and maintained by Qdrant.\",\n]\n\n# This will trigger the model download and initialization\nembedding_model = TextEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\nprint(f\"The model {embedding_model.model_name} is ready to use.\")\n\nembeddings_generator = embedding_model.embed(documents)  # reminder this is a generator\nembeddings_list = list(embedding_model.embed(documents))\n# you can also convert the generator to a list, and that to a numpy array\nlen(embeddings_list[0])  # Vector of 384 dimensions\n\nThe model BAAI/bge-small-en-v1.5 is ready to use.\n\n\n384\n\n\n\nlist(filter(lambda x: not x.startswith(\"__\"), dir(embedding_model)))\n\n['EMBEDDINGS_REGISTRY',\n 'METADATA_FILE',\n '_get_model_description',\n '_list_supported_models',\n '_local_files_only',\n 'add_custom_model',\n 'cache_dir',\n 'decompress_to_cache',\n 'download_file_from_gcs',\n 'download_files_from_huggingface',\n 'download_model',\n 'embed',\n 'list_supported_models',\n 'model',\n 'model_name',\n 'passage_embed',\n 'query_embed',\n 'retrieve_model_gcs',\n 'threads']\n\n\n\nembedding_model.model_name\n\n'BAAI/bge-small-en-v1.5'",
    "crumbs": [
      "Text Embedding"
    ]
  },
  {
    "objectID": "001_basic.html#what-are-dense-text-embeddings",
    "href": "001_basic.html#what-are-dense-text-embeddings",
    "title": "Text Embedding",
    "section": "1.2 What are Dense Text Embeddings?",
    "text": "1.2 What are Dense Text Embeddings?\nDense text embeddings are a way to convert text into numerical representations that capture its meaning and context.\n\n1.2.1 How Do They Work?\n\nVectorize Words: Turn words into vectors (numbers) that represent their meaning.\nLearned Through Data: The model learns from a massive amount of text data and generates these vectors based on the relationships between words.\n\n\n\n1.2.2 Example:\n\n“Dog” -&gt; [0.8, 0.2, 0.9]\n“Cat” -&gt; [0.7, 0.3, 0.8]\n“Car” -&gt; [0.1, 0.9, 0.2]\n“Apple” -&gt; [0.3, 0.5, 0.7]\n\nNotice how similar words are closer together.\n\n\n1.2.3 Real-World Examples:\n\nSearch Engines: Convert search queries into embeddings to find relevant documents.\nRecommendation Systems: Use embeddings to suggest movies or songs based on user preferences.\nSentiment Analysis: Analyze text to determine its sentiment (positive, negative, neutral).\nChatbots & Conversational AI: Embeddings help chatbots understand user input and respond accordingly.\n\n\n\n1.2.4 Key Benefits:\n\nCapture complex relationships between words\nEfficiently represent large amounts of text data\nEnable applications like search, recommendation, and sentiment analysis",
    "crumbs": [
      "Text Embedding"
    ]
  },
  {
    "objectID": "001_basic.html#sparse-text-embeddings",
    "href": "001_basic.html#sparse-text-embeddings",
    "title": "Text Embedding",
    "section": "1.3 Sparse text embeddings",
    "text": "1.3 Sparse text embeddings\n\n%%ai\n\n\"Explain sparse text embedding, keep it simple. Demonstrate some example. Give some real-world example\"\n\n2 Sparse Text Embeddings\n\n2.0.1 What is a Sparse Vector?\nA sparse vector is a numerical representation of a piece of text where most values are 0.\n\n\n2.0.2 How Does it Work?\nImagine you’re looking at a word and thinking of related words, like “apple” and “fruit”. A sparse vector would have a few non-zero values representing the relationships between these words, but most values would be 0 (representing unrelated words).\n\n\n2.1 ### Example\nLet’s say we have a piece of text: “The quick brown fox jumps over the lazy dog.”\nWe can represent this as two sparse vectors:\n\nVector A: [0.1, 0, 0, ...] (related to the word “quick”)\nVector B: [0, 0.4, 0, ...] (related to the word “brown”)\n\nOur sparse vector would have only non-zero values representing the relationships between these two words.\n\n\n2.2 ### Real-World Examples\n\nText Classification: Sparse embeddings can be used as input features for text classification models, such as sentiment analysis or spam detection.\nTopic Modeling: Sparse embeddings can help identify topics in large collections of text by capturing the underlying relationships between words.\nWord Embeddings: Sparse embeddings can be used to represent individual words, like Word2Vec or GloVe.\n\n\n\n2.3 ### Code Example\n# Import necessary libraries\nimport numpy as np\n\n# Define a function to create a simple sparse vector representation\ndef create_sparse_vector(text):\n    # Split the text into words\n    words = text.split()\n\n    # Create a dictionary to store word vectors\n    word_vectors = {}\n\n    # Add each word to the dictionary\n    for i, word in enumerate(words):\n        # Use a random numerical value as the vector for simplicity\n        word_vector = np.random.rand(768)\n        word_vector[i] = 1.0  # Set the correct value\n        word_vectors[word] = word_vector\n\n    # Combine all word vectors into a single sparse vector\n    sparse_vector = np.array([word_vectors[word][i] for i, word in enumerate(words)])\n\n    return sparse_vector\n\n# Test the function\ntext = \"The quick brown fox jumps over the lazy dog.\"\nsparse_vector = create_sparse_vector(text)\nprint(sparse_vector.shape)  # Output: (1, 768)\nIn this example, we use a simple dictionary to store word vectors and then combine them into a single sparse vector. The i-th value in each word vector is set to 1.0 to represent the correct relationship between words.\n\n\n\n\nfrom fastembed import SparseTextEmbedding\n\nmodel = SparseTextEmbedding(model_name=\"prithivida/Splade_PP_en_v1\")\nembeddings = list(model.embed(documents))\nembeddings\n\n[SparseEmbedding(values=array([0.46793732, 0.34634435, 0.82014424, 0.45307532, 0.98732066,\n        0.80176616, 0.2087955 , 0.07078066, 0.15851103, 0.07413071,\n        0.34253079, 0.88557774, 0.13234277, 0.23698376, 0.07734038,\n        0.20083414, 1.3942709 , 0.57856292, 0.75639009, 0.12872015,\n        0.12940496, 1.21411681, 0.3960413 , 0.38100156, 0.85480541,\n        0.23132324, 0.61133695, 0.34899744, 0.15025412, 0.1130122 ,\n        0.15241024, 0.36152679, 0.13700481, 0.7303589 , 1.39194822,\n        0.04954698, 0.49473077, 0.30635571, 0.06034151, 1.13118982,\n        0.01341425, 0.02633621, 0.10710741, 1.03937888, 0.05903498,\n        0.33036089, 0.0278459 , 0.04743589, 1.68689609, 0.62101287,\n        1.86998868, 0.71478194, 0.08071101, 1.26968515, 0.05093801,\n        0.09553559, 1.57417607, 0.18500556, 0.0425379 , 0.24046306,\n        1.08656394, 0.72864759, 0.1876028 , 0.85070795, 0.16575399,\n        0.23869337, 0.52304912, 0.90775394, 0.02330356, 0.12363458,\n        0.37557927, 1.93465626, 0.5360083 , 0.08284581, 0.39607322,\n        0.13179989]), indices=array([ 2022,  2060,  2084,  2138,  2328,  2422,  2488,  2544,  2640,\n         2653,  2742,  2793,  2828,  2881,  3033,  3074,  3075,  3082,\n         3177,  3274,  3430,  3435,  3642,  3800,  3857,  3989,  4007,\n         4127,  4248,  4289,  4294,  4385,  4406,  4489,  4667,  4773,\n         5056,  5080,  5371,  5514,  5672,  6028,  6082,  6251,  6254,\n         6994,  7705,  7831,  7861,  7915,  8270,  8860,  8875,  8957,\n         9121,  9262,  9442, 10472, 10763, 10899, 10938, 11746, 11892,\n        11907, 12430, 12692, 13507, 13850, 15106, 16473, 19059, 19081,\n        21331, 23561, 23924, 27014])),\n SparseEmbedding(values=array([0.08703687, 2.10778594, 0.01475082, 0.16455774, 0.09053489,\n        0.04780621, 0.89654833, 1.0727092 , 0.05152059, 0.19544077,\n        1.18726742, 0.09147657, 0.53395849, 0.36316222, 0.05458989,\n        1.05834925, 0.21000545, 0.2174563 , 0.4657135 , 0.16001791,\n        2.58359981, 1.51888502, 0.42704242, 0.85715246, 0.22741754,\n        0.1889631 , 0.04392261, 0.03791322, 0.5099448 , 1.10458422,\n        0.20403962, 0.97265482, 0.0194856 , 0.86876655, 0.10075891,\n        2.00031805, 0.24204996, 0.07565179, 0.52923071, 0.48402771,\n        0.19994174, 1.99733865, 0.42608464, 0.18558736, 0.15080665,\n        0.13985491, 0.54798013, 0.16576864, 0.19238883, 0.57402122,\n        2.63415504]), indices=array([ 1010,  1053,  2003,  2009,  2025,  2040,  2102,  2194,  2284,\n         2291,  2490,  2497,  2544,  2562,  2572,  2793,  2897,  2974,\n         3079,  3274,  3435,  3569,  3954,  4007,  4289,  4316,  4322,\n         4434,  5080,  5224,  5371,  5441,  5527,  6032,  6153,  6633,\n         7506,  7621,  7751,  7861,  8241,  8270,  8498,  9319,  9722,\n        10472, 12494, 13666, 16350, 20486, 24914]))]",
    "crumbs": [
      "Text Embedding"
    ]
  },
  {
    "objectID": "001_basic.html#example-1",
    "href": "001_basic.html#example-1",
    "title": "Text Embedding",
    "section": "2.1 ### Example",
    "text": "2.1 ### Example\nLet’s say we have a piece of text: “The quick brown fox jumps over the lazy dog.”\nWe can represent this as two sparse vectors:\n\nVector A: [0.1, 0, 0, ...] (related to the word “quick”)\nVector B: [0, 0.4, 0, ...] (related to the word “brown”)\n\nOur sparse vector would have only non-zero values representing the relationships between these two words.",
    "crumbs": [
      "Text Embedding"
    ]
  },
  {
    "objectID": "001_basic.html#real-world-examples-1",
    "href": "001_basic.html#real-world-examples-1",
    "title": "Text Embedding",
    "section": "2.2 ### Real-World Examples",
    "text": "2.2 ### Real-World Examples\n\nText Classification: Sparse embeddings can be used as input features for text classification models, such as sentiment analysis or spam detection.\nTopic Modeling: Sparse embeddings can help identify topics in large collections of text by capturing the underlying relationships between words.\nWord Embeddings: Sparse embeddings can be used to represent individual words, like Word2Vec or GloVe.",
    "crumbs": [
      "Text Embedding"
    ]
  },
  {
    "objectID": "001_basic.html#code-example",
    "href": "001_basic.html#code-example",
    "title": "Text Embedding",
    "section": "2.3 ### Code Example",
    "text": "2.3 ### Code Example\n# Import necessary libraries\nimport numpy as np\n\n# Define a function to create a simple sparse vector representation\ndef create_sparse_vector(text):\n    # Split the text into words\n    words = text.split()\n\n    # Create a dictionary to store word vectors\n    word_vectors = {}\n\n    # Add each word to the dictionary\n    for i, word in enumerate(words):\n        # Use a random numerical value as the vector for simplicity\n        word_vector = np.random.rand(768)\n        word_vector[i] = 1.0  # Set the correct value\n        word_vectors[word] = word_vector\n\n    # Combine all word vectors into a single sparse vector\n    sparse_vector = np.array([word_vectors[word][i] for i, word in enumerate(words)])\n\n    return sparse_vector\n\n# Test the function\ntext = \"The quick brown fox jumps over the lazy dog.\"\nsparse_vector = create_sparse_vector(text)\nprint(sparse_vector.shape)  # Output: (1, 768)\nIn this example, we use a simple dictionary to store word vectors and then combine them into a single sparse vector. The i-th value in each word vector is set to 1.0 to represent the correct relationship between words.",
    "crumbs": [
      "Text Embedding"
    ]
  },
  {
    "objectID": "001_basic.html#late-interaction-models-aka-colbert",
    "href": "001_basic.html#late-interaction-models-aka-colbert",
    "title": "Text Embedding",
    "section": "2.4 Late interaction models (aka ColBERT)",
    "text": "2.4 Late interaction models (aka ColBERT)\n\n%%ai ollama:llama3.2\n\n\"Explain late interaction models (aka ColBERT), keep it simple. Demonstrate some example. Give some real-world example\"\n\n3 Late Interaction Models (ColBERT)\n\n3.0.1 What is Late Interaction?\nLate interaction models are a type of search model that improves the performance of early interaction methods, such as BM25.\n\n\n3.0.2 How Does it Work?\nImagine you’re searching for documents related to a query. Early interaction methods like BM25 use features extracted from the query and the document to rank them. Late interaction models refine these rankings by using additional information, such as the entire document or other relevant documents.\n\n\n3.1 ### Example\nLet’s say we have a search model that uses BM25 to rank documents for a query “computer science”. The top 5 ranked documents are:\n\nA paper on computer vision\nA blog post on machine learning\nA Wikipedia page on artificial intelligence\nA news article on data science\nA book review on software engineering\n\nLate interaction models like ColBERT can improve these rankings by considering additional features, such as:\n\nThe entire document text\nOther relevant documents in the corpus\n\n\n\n3.2 ### Real-World Examples\n\nE-commerce Search: Late interaction models can improve search results for e-commerce platforms by incorporating product descriptions, reviews, and other relevant information.\nKnowledge Graph Search: Late interaction models can be used to search knowledge graphs, which are databases of entities and their relationships.\nRecommendation Systems: Late interaction models can be used to recommend items based on user behavior and preferences.\n\n\n\n3.3 ### Code Example\n# Import necessary libraries\nimport numpy as np\n\n# Define a function to create a ColBERT model\ndef create_colbert_model(query, documents):\n    # Extract features from the query and documents using BM25\n    bm25_features = bm25_query_and_document(query, documents)\n\n    # Add additional features using late interaction\n    colbert_features = add_late_interaction(bm25_features, query, documents)\n\n    return colbert_features\n\n# Define a function to calculate BM25 features\ndef bm25_query_and_document(query, documents):\n    # Implement BM25 calculation here\n    pass\n\n# Define a function to add late interaction features\ndef add_late_interaction(features, query, documents):\n    # Implement late interaction calculation here\n    pass\nIn this example, we define a simple ColBERT model that uses BM25 as an early interaction method and adds additional features using late interaction. The bm25_query_and_document function calculates BM25 features, and the add_late_interaction function calculates late interaction features.\n\n\n\n\nfrom fastembed import LateInteractionTextEmbedding\n\nmodel = LateInteractionTextEmbedding(model_name=\"colbert-ir/colbertv2.0\")\nembeddings = list(model.embed(documents))\nembeddings\n\n[array([[-0.1351824 ,  0.12230334,  0.1269857 , ...,  0.17307524,\n          0.11274203,  0.02880633],\n        [-0.17495233,  0.08767531,  0.11352374, ...,  0.12433604,\n          0.15752925,  0.08118125],\n        [-0.10130584,  0.09613474,  0.13923067, ...,  0.12898032,\n          0.16839182,  0.09858395],\n        ...,\n        [-0.10270972,  0.01041561,  0.04440113, ...,  0.0550529 ,\n          0.08930317,  0.09720251],\n        [-0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [-0.15476122,  0.06961455,  0.10665789, ...,  0.15388842,\n          0.09050205,  0.00516431]], shape=(29, 128), dtype=float32),\n array([[ 0.12170535,  0.07871944,  0.12508287, ...,  0.08450251,\n          0.01834184, -0.01686618],\n        [-0.02659732, -0.12131035,  0.14012505, ..., -0.01885814,\n          0.01064609, -0.05982119],\n        [-0.03633325, -0.14667122,  0.14062028, ..., -0.052545  ,\n          0.00967532, -0.08844125],\n        ...,\n        [-0.        ,  0.        ,  0.        , ..., -0.        ,\n         -0.        ,  0.        ],\n        [-0.        , -0.        ,  0.        , ..., -0.        ,\n         -0.        , -0.        ],\n        [-0.        , -0.        ,  0.        , ..., -0.        ,\n          0.        , -0.        ]], shape=(29, 128), dtype=float32)]",
    "crumbs": [
      "Text Embedding"
    ]
  },
  {
    "objectID": "001_basic.html#example-2",
    "href": "001_basic.html#example-2",
    "title": "Text Embedding",
    "section": "3.1 ### Example",
    "text": "3.1 ### Example\nLet’s say we have a search model that uses BM25 to rank documents for a query “computer science”. The top 5 ranked documents are:\n\nA paper on computer vision\nA blog post on machine learning\nA Wikipedia page on artificial intelligence\nA news article on data science\nA book review on software engineering\n\nLate interaction models like ColBERT can improve these rankings by considering additional features, such as:\n\nThe entire document text\nOther relevant documents in the corpus",
    "crumbs": [
      "Text Embedding"
    ]
  },
  {
    "objectID": "001_basic.html#real-world-examples-2",
    "href": "001_basic.html#real-world-examples-2",
    "title": "Text Embedding",
    "section": "3.2 ### Real-World Examples",
    "text": "3.2 ### Real-World Examples\n\nE-commerce Search: Late interaction models can improve search results for e-commerce platforms by incorporating product descriptions, reviews, and other relevant information.\nKnowledge Graph Search: Late interaction models can be used to search knowledge graphs, which are databases of entities and their relationships.\nRecommendation Systems: Late interaction models can be used to recommend items based on user behavior and preferences.",
    "crumbs": [
      "Text Embedding"
    ]
  },
  {
    "objectID": "001_basic.html#code-example-1",
    "href": "001_basic.html#code-example-1",
    "title": "Text Embedding",
    "section": "3.3 ### Code Example",
    "text": "3.3 ### Code Example\n# Import necessary libraries\nimport numpy as np\n\n# Define a function to create a ColBERT model\ndef create_colbert_model(query, documents):\n    # Extract features from the query and documents using BM25\n    bm25_features = bm25_query_and_document(query, documents)\n\n    # Add additional features using late interaction\n    colbert_features = add_late_interaction(bm25_features, query, documents)\n\n    return colbert_features\n\n# Define a function to calculate BM25 features\ndef bm25_query_and_document(query, documents):\n    # Implement BM25 calculation here\n    pass\n\n# Define a function to add late interaction features\ndef add_late_interaction(features, query, documents):\n    # Implement late interaction calculation here\n    pass\nIn this example, we define a simple ColBERT model that uses BM25 as an early interaction method and adds additional features using late interaction. The bm25_query_and_document function calculates BM25 features, and the add_late_interaction function calculates late interaction features.",
    "crumbs": [
      "Text Embedding"
    ]
  },
  {
    "objectID": "001_basic.html#image-embeddings",
    "href": "001_basic.html#image-embeddings",
    "title": "Text Embedding",
    "section": "3.4 Image embeddings",
    "text": "3.4 Image embeddings\n\nimport numpy as np\nfrom fastembed import ImageEmbedding\n\nimages = [\"images/cat1.jpeg\", \"images/cat2.jpeg\", \"images/dog1.webp\"]\n\nmodel = ImageEmbedding(model_name=\"Qdrant/clip-ViT-B-32-vision\")\nembeddings = list(model.embed(images))\n\n\nfrom qdrant_client.local.distances import cosine_similarity\n\n\ncosine_similarity(np.array(embeddings), np.array(embeddings))\n\narray([[1.0000001 , 0.83723867, 0.61121917],\n       [0.83723867, 1.0000002 , 0.7055948 ],\n       [0.61121917, 0.7055948 , 1.0000001 ]], dtype=float32)",
    "crumbs": [
      "Text Embedding"
    ]
  },
  {
    "objectID": "001_basic.html#late-interaction-multimodal-models-colpali",
    "href": "001_basic.html#late-interaction-multimodal-models-colpali",
    "title": "Text Embedding",
    "section": "3.5 Late interaction multimodal models (ColPali)",
    "text": "3.5 Late interaction multimodal models (ColPali)\n\n%%ai\n\n\"Explain late interaction multimodal models (aka ColPali), keep it simple. Demonstrate some example. Give some real-world example\"\n\n4 Late Interaction Multimodal Models (ColPali)\n\n4.0.1 What is Late Interaction?\nLate interaction models are a type of search model that improves the performance of early interaction methods, such as BM25.\n\n\n4.0.2 How Does it Work?\nImagine you’re searching for documents related to a query. Early interaction methods like BM25 use features extracted from the query and the document to rank them. Late interaction models refine these rankings by using additional information, such as the entire document or other relevant documents.\n\n\n4.0.3 Multimodal Models\nMultimodal models extend late interaction models to incorporate multiple input modalities, such as text, images, audio, and video.\n\n\n4.1 ### Example\nLet’s say we have a search model that uses ColPali to rank documents for a query “computer science”. The input modalities are:\n\nText: A passage of text related to the query.\nImage: An image related to the query (e.g. a diagram of a computer chip).\nAudio: An audio file related to the query (e.g. a lecture on machine learning).\n\nThe ColPali model extracts features from each modality and combines them using late interaction. The final ranking is based on the weighted sum of these features.\n\n\n4.2 ### Real-World Examples\n\nImage Search with Text Description: Late interaction multimodal models can be used to search images based on a text description.\nSpeech Recognition with Visual Feedback: ColPali can be used in speech recognition systems that incorporate visual feedback, such as displaying the recognized text in real-time.\nMultimodal Question Answering: Late interaction multimodal models can be used to answer questions that require information from multiple input modalities (e.g. text, images, audio).\n\n\n\n4.3 ### Code Example\n# Import necessary libraries\nimport numpy as np\n\n# Define a function to create a ColPali model\ndef create_colpali_model(query, text, image, audio):\n    # Extract features from each modality using late interaction\n    text_features = extract_features(text, query)\n    image_features = extract_features(image, query)\n    audio_features = extract_features(audio, query)\n\n    # Combine features using weighted sum\n    combined_features = combine_features(text_features, image_features, audio_features)\n\n    return combined_features\n\n# Define a function to extract features from text using late interaction\ndef extract_features(text, query):\n    # Implement late interaction calculation here\n    pass\n\n# Define a function to combine features using weighted sum\ndef combine_features(text_features, image_features, audio_features):\n    # Implement weighted sum calculation here\n    pass\nIn this example, we define a simple ColPali model that uses late interaction to extract features from each modality and combines them using a weighted sum. The extract_features function calculates features for each modality, and the combine_features function calculates the final combined features.\n\n\n\n\nfrom fastembed import LateInteractionMultimodalEmbedding\n\ndoc_images = [\n    \"images/wiki_computer_science.png\",\n    \"images/wiki_technology.png\",\n    \"images/wiki_space.png\",\n]\n\nquery = \"what is tech\"\n\nmodel = LateInteractionMultimodalEmbedding(model_name=\"Qdrant/colpali-v1.3-fp16\")\ndoc_images_embeddings = list(model.embed_image(doc_images))\nquery_embedding = model.embed_text(query)\n\n\nfrom qdrant_client import models\nfrom qdrant_client.local.multi_distances import calculate_multi_distance\n\n# How to calculate distance?\n# from qdrant_client.local.sparse_distances import calculate_distance_sparse\n# calculate_multi_distance(\n#     query_embedding,\n#     doc_images_embeddings,\n#     # distance_type=models.MultiVectorComparator.MAX_SIM,\n#     distance_type=models.Distance.DOT,\n# )",
    "crumbs": [
      "Text Embedding"
    ]
  },
  {
    "objectID": "001_basic.html#example-3",
    "href": "001_basic.html#example-3",
    "title": "Text Embedding",
    "section": "4.1 ### Example",
    "text": "4.1 ### Example\nLet’s say we have a search model that uses ColPali to rank documents for a query “computer science”. The input modalities are:\n\nText: A passage of text related to the query.\nImage: An image related to the query (e.g. a diagram of a computer chip).\nAudio: An audio file related to the query (e.g. a lecture on machine learning).\n\nThe ColPali model extracts features from each modality and combines them using late interaction. The final ranking is based on the weighted sum of these features.",
    "crumbs": [
      "Text Embedding"
    ]
  },
  {
    "objectID": "001_basic.html#real-world-examples-3",
    "href": "001_basic.html#real-world-examples-3",
    "title": "Text Embedding",
    "section": "4.2 ### Real-World Examples",
    "text": "4.2 ### Real-World Examples\n\nImage Search with Text Description: Late interaction multimodal models can be used to search images based on a text description.\nSpeech Recognition with Visual Feedback: ColPali can be used in speech recognition systems that incorporate visual feedback, such as displaying the recognized text in real-time.\nMultimodal Question Answering: Late interaction multimodal models can be used to answer questions that require information from multiple input modalities (e.g. text, images, audio).",
    "crumbs": [
      "Text Embedding"
    ]
  },
  {
    "objectID": "001_basic.html#code-example-2",
    "href": "001_basic.html#code-example-2",
    "title": "Text Embedding",
    "section": "4.3 ### Code Example",
    "text": "4.3 ### Code Example\n# Import necessary libraries\nimport numpy as np\n\n# Define a function to create a ColPali model\ndef create_colpali_model(query, text, image, audio):\n    # Extract features from each modality using late interaction\n    text_features = extract_features(text, query)\n    image_features = extract_features(image, query)\n    audio_features = extract_features(audio, query)\n\n    # Combine features using weighted sum\n    combined_features = combine_features(text_features, image_features, audio_features)\n\n    return combined_features\n\n# Define a function to extract features from text using late interaction\ndef extract_features(text, query):\n    # Implement late interaction calculation here\n    pass\n\n# Define a function to combine features using weighted sum\ndef combine_features(text_features, image_features, audio_features):\n    # Implement weighted sum calculation here\n    pass\nIn this example, we define a simple ColPali model that uses late interaction to extract features from each modality and combines them using a weighted sum. The extract_features function calculates features for each modality, and the combine_features function calculates the final combined features.",
    "crumbs": [
      "Text Embedding"
    ]
  },
  {
    "objectID": "001_basic.html#rerankers",
    "href": "001_basic.html#rerankers",
    "title": "Text Embedding",
    "section": "4.4 Rerankers",
    "text": "4.4 Rerankers\n\nfrom fastembed.rerank.cross_encoder import TextCrossEncoder\n\nquery = \"Who is maintaining Qdrant?\"\ndocuments: list[str] = [\n    \"This is built to be faster and lighter than other embedding libraries e.g. Transformers, Sentence-Transformers, etc.\",\n    \"fastembed is supported by and maintained by Qdrant.\",\n]\nencoder = TextCrossEncoder(model_name=\"Xenova/ms-marco-MiniLM-L-6-v2\")\nscores = list(encoder.rerank(query, documents))\nscores",
    "crumbs": [
      "Text Embedding"
    ]
  },
  {
    "objectID": "002_qdrant_client.html",
    "href": "002_qdrant_client.html",
    "title": "Vector Database",
    "section": "",
    "text": "In this notebook, we will explore the vector database Qdrant. We will index and query the documents.\n\nfrom qdrant_client import QdrantClient\n\n\nclient = QdrantClient(\":memory:\")\n# or\n# client = QdrantClient(path=\"tmp/tmp.db\")\n# client.close()\n\n\ncat_facts = [\n    \"Cats have 32 muscles in each ear, allowing for their unique ability to rotate their ears independently.\",\n    \"A group of cats is called a 'clowder'.\",\n    \"Cats can't taste sweetness.\",\n    \"A cat's whiskers help them navigate in the dark.\",\n    \"Cats have three eyelids: upper, lower, and third (also known as nictitating membrane).\",\n    \"Cats spend 1/3 of their waking hours sleeping.\",\n    \"A cat's purr can be a sign of happiness or self-soothing.\",\n    \"Cats have unique nose prints, just like humans have fingerprints.\",\n    \"The world's largest domesticated cat was Muffin, who measured 48.5 inches long and weighed 46.3 pounds.\",\n    \"Cats can't see in complete darkness but their night vision is excellent due to a reflective layer in the back of their eyes called the tapetum lucidum.\",\n]\ndog_facts = [\n    \"A dog's sense of smell is up to 10,000 times more sensitive than a human's.\",\n    \"Dogs can hear sounds at frequencies as high as 40,000 Hz, while humans can only hear up to 20,000 Hz.\",\n    \"A group of dogs is called a 'pack'.\",\n    \"Dogs have three eyelids: upper, lower, and nictitating membrane (third eyelid).\",\n    \"Dogs can dream just like humans do, and their brain waves show similar patterns during sleep.\",\n    \"The world's smallest dog breed is the Chihuahua, with adults weighing as little as 2 pounds.\",\n    \"A dog's tail language is more complex than human body language, conveying emotions and intentions.\",\n    \"Dogs can be right- or left-pawed, just like humans are right- or left-handed.\",\n    \"The oldest known dog remains dates back to around 14,000 years ago, during the Late Pleistocene era.\",\n    \"Dogs have a unique nose print, just like humans have fingerprints.\",\n]",
    "crumbs": [
      "Vector Database"
    ]
  },
  {
    "objectID": "002_qdrant_client.html#goal",
    "href": "002_qdrant_client.html#goal",
    "title": "Vector Database",
    "section": "",
    "text": "In this notebook, we will explore the vector database Qdrant. We will index and query the documents.\n\nfrom qdrant_client import QdrantClient\n\n\nclient = QdrantClient(\":memory:\")\n# or\n# client = QdrantClient(path=\"tmp/tmp.db\")\n# client.close()\n\n\ncat_facts = [\n    \"Cats have 32 muscles in each ear, allowing for their unique ability to rotate their ears independently.\",\n    \"A group of cats is called a 'clowder'.\",\n    \"Cats can't taste sweetness.\",\n    \"A cat's whiskers help them navigate in the dark.\",\n    \"Cats have three eyelids: upper, lower, and third (also known as nictitating membrane).\",\n    \"Cats spend 1/3 of their waking hours sleeping.\",\n    \"A cat's purr can be a sign of happiness or self-soothing.\",\n    \"Cats have unique nose prints, just like humans have fingerprints.\",\n    \"The world's largest domesticated cat was Muffin, who measured 48.5 inches long and weighed 46.3 pounds.\",\n    \"Cats can't see in complete darkness but their night vision is excellent due to a reflective layer in the back of their eyes called the tapetum lucidum.\",\n]\ndog_facts = [\n    \"A dog's sense of smell is up to 10,000 times more sensitive than a human's.\",\n    \"Dogs can hear sounds at frequencies as high as 40,000 Hz, while humans can only hear up to 20,000 Hz.\",\n    \"A group of dogs is called a 'pack'.\",\n    \"Dogs have three eyelids: upper, lower, and nictitating membrane (third eyelid).\",\n    \"Dogs can dream just like humans do, and their brain waves show similar patterns during sleep.\",\n    \"The world's smallest dog breed is the Chihuahua, with adults weighing as little as 2 pounds.\",\n    \"A dog's tail language is more complex than human body language, conveying emotions and intentions.\",\n    \"Dogs can be right- or left-pawed, just like humans are right- or left-handed.\",\n    \"The oldest known dog remains dates back to around 14,000 years ago, during the Late Pleistocene era.\",\n    \"Dogs have a unique nose print, just like humans have fingerprints.\",\n]",
    "crumbs": [
      "Vector Database"
    ]
  },
  {
    "objectID": "002_qdrant_client.html#preparing-metadata",
    "href": "002_qdrant_client.html#preparing-metadata",
    "title": "Vector Database",
    "section": "2 Preparing Metadata",
    "text": "2 Preparing Metadata\n\ndocs = cat_facts + dog_facts\nmetadata = [{\"source\": \"cats\"}] * len(cat_facts) + [{\"source\": \"dogs\"}] * len(dog_facts)\nids = list(range(len(docs)))",
    "crumbs": [
      "Vector Database"
    ]
  },
  {
    "objectID": "002_qdrant_client.html#adding-documents-to-collection",
    "href": "002_qdrant_client.html#adding-documents-to-collection",
    "title": "Vector Database",
    "section": "3 Adding documents to collection",
    "text": "3 Adding documents to collection\nBelow, we add the documents together with the metadatas and ids to our vector database. The default embedding is used if we did not specify them\n\nclient.add(\n    collection_name=\"cats_and_dogs\",\n    documents=docs,\n    metadata=metadata,\n    ids=ids,\n)\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n\n\n\n# Default embedding used.\nprint(client.embedding_model_name)\n\nBAAI/bge-small-en",
    "crumbs": [
      "Vector Database"
    ]
  },
  {
    "objectID": "002_qdrant_client.html#querying-documents",
    "href": "002_qdrant_client.html#querying-documents",
    "title": "Vector Database",
    "section": "4 Querying documents",
    "text": "4 Querying documents\n\nsearch_result = client.query(\n    collection_name=\"cats_and_dogs\", query_text=\"what is the cat whisker for?\"\n)\nsearch_result\n\n[QueryResponse(id=3, embedding=None, sparse_embedding=None, metadata={'document': \"A cat's whiskers help them navigate in the dark.\", 'source': 'cats'}, document=\"A cat's whiskers help them navigate in the dark.\", score=0.8960122317479164),\n QueryResponse(id=1, embedding=None, sparse_embedding=None, metadata={'document': \"A group of cats is called a 'clowder'.\", 'source': 'cats'}, document=\"A group of cats is called a 'clowder'.\", score=0.8371795788861598),\n QueryResponse(id=6, embedding=None, sparse_embedding=None, metadata={'document': \"A cat's purr can be a sign of happiness or self-soothing.\", 'source': 'cats'}, document=\"A cat's purr can be a sign of happiness or self-soothing.\", score=0.8254998482294487),\n QueryResponse(id=4, embedding=None, sparse_embedding=None, metadata={'document': 'Cats have three eyelids: upper, lower, and third (also known as nictitating membrane).', 'source': 'cats'}, document='Cats have three eyelids: upper, lower, and third (also known as nictitating membrane).', score=0.8023456093614258),\n QueryResponse(id=7, embedding=None, sparse_embedding=None, metadata={'document': 'Cats have unique nose prints, just like humans have fingerprints.', 'source': 'cats'}, document='Cats have unique nose prints, just like humans have fingerprints.', score=0.798183377158493),\n QueryResponse(id=0, embedding=None, sparse_embedding=None, metadata={'document': 'Cats have 32 muscles in each ear, allowing for their unique ability to rotate their ears independently.', 'source': 'cats'}, document='Cats have 32 muscles in each ear, allowing for their unique ability to rotate their ears independently.', score=0.7954062431026273),\n QueryResponse(id=2, embedding=None, sparse_embedding=None, metadata={'document': \"Cats can't taste sweetness.\", 'source': 'cats'}, document=\"Cats can't taste sweetness.\", score=0.7869827548539508),\n QueryResponse(id=5, embedding=None, sparse_embedding=None, metadata={'document': 'Cats spend 1/3 of their waking hours sleeping.', 'source': 'cats'}, document='Cats spend 1/3 of their waking hours sleeping.', score=0.7801444313950741),\n QueryResponse(id=19, embedding=None, sparse_embedding=None, metadata={'document': 'Dogs have a unique nose print, just like humans have fingerprints.', 'source': 'dogs'}, document='Dogs have a unique nose print, just like humans have fingerprints.', score=0.7677823520283983),\n QueryResponse(id=12, embedding=None, sparse_embedding=None, metadata={'document': \"A group of dogs is called a 'pack'.\", 'source': 'dogs'}, document=\"A group of dogs is called a 'pack'.\", score=0.7629267089853339)]",
    "crumbs": [
      "Vector Database"
    ]
  },
  {
    "objectID": "002_qdrant_client.html#narrowing-search-with-filters",
    "href": "002_qdrant_client.html#narrowing-search-with-filters",
    "title": "Vector Database",
    "section": "5 Narrowing search with filters",
    "text": "5 Narrowing search with filters\n\nfrom qdrant_client.models import FieldCondition, Filter, MatchValue\n\nsearch_result = client.query(\n    collection_name=\"cats_and_dogs\",\n    query_text=\"what is the cat whisker for?\",\n    query_filter=Filter(\n        must=[FieldCondition(key=\"source\", match=MatchValue(value=\"cats\"))]\n    ),\n)\n\nfor res in search_result:\n    print(dict(doc=res.document, score=res.score))\n\n{'doc': \"A cat's whiskers help them navigate in the dark.\", 'score': 0.8960122052835192}\n{'doc': \"A group of cats is called a 'clowder'.\", 'score': 0.8371795647410071}\n{'doc': \"A cat's purr can be a sign of happiness or self-soothing.\", 'score': 0.8254998513657924}\n{'doc': 'Cats have three eyelids: upper, lower, and third (also known as nictitating membrane).', 'score': 0.8023456208085149}\n{'doc': 'Cats have unique nose prints, just like humans have fingerprints.', 'score': 0.7981833555293472}\n{'doc': 'Cats have 32 muscles in each ear, allowing for their unique ability to rotate their ears independently.', 'score': 0.7954061802722565}\n{'doc': \"Cats can't taste sweetness.\", 'score': 0.7869827666131266}\n{'doc': 'Cats spend 1/3 of their waking hours sleeping.', 'score': 0.780144426020518}\n{'doc': \"Cats can't see in complete darkness but their night vision is excellent due to a reflective layer in the back of their eyes called the tapetum lucidum.\", 'score': 0.7533777384436582}\n{'doc': \"The world's largest domesticated cat was Muffin, who measured 48.5 inches long and weighed 46.3 pounds.\", 'score': 0.7365538436735114}",
    "crumbs": [
      "Vector Database"
    ]
  },
  {
    "objectID": "004_production_ready.html",
    "href": "004_production_ready.html",
    "title": "1 Retrieval Augmented Generation (RAG) Application Planning",
    "section": "",
    "text": "%load_ext jupyter_ai\n\n\n%%ai ollama:llama3.2\n\n\"\"\"\nyou are an experienced AI engineer with 10 years of experience.\nyou are tasked to build a retrieval augmented generation, RAG application.\na vector store is an essential part for querying large amount of documents.\n\nplan before you act\n- suggest the metadata for the documents to embed\n- the metadata should aid in fast retrieval\n- explain each metadata fields\n- consider how to version the changes for the metadata when the document changes\n- consider how to support different embedding models\n- take into consideration of other edge cases\n- list down other edge cases too\n\"\"\"\n\n1 Retrieval Augmented Generation (RAG) Application Planning\n\n1.0.1 1. Suggested Metadata for Documents\nTo aid in fast retrieval, we suggest embedding the following metadata fields:\n\nDocument ID: A unique identifier for each document.\nCategory: The category or topic that the document belongs to.\nTextual Features: Relevant textual features such as word embeddings (e.g., Word2Vec, GloVe) and document frequency.\nMetadata Tags: Additional metadata tags such as author, date of creation, and keywords.\n\n\n\n1.0.2 2. Explanation of Metadata Fields\n\nDocument ID: A unique identifier for each document, which can be used to quickly identify the document in storage or when querying.\nCategory: The category or topic that the document belongs to. This information is useful for fast retrieval based on topic similarity.\nTextual Features: Relevant textual features such as word embeddings (e.g., Word2Vec, GloVe) and document frequency. These features provide semantic meaning to the text content of the document.\nMetadata Tags: Additional metadata tags such as author, date of creation, and keywords. These tags provide additional context about the document.\n\n\n\n1.0.3 3. Versioning Changes for Metadata when Documents Change\nTo version changes for metadata when documents change:\n\nCreate a separate field to track the version number of each document.\nIncrement the version number each time the metadata changes.\nStore the previous version number along with the updated metadata.\n\nExample:\n\n\n\n\n\n\n\n\n\n\nDocument ID\nVersion\nCategory\nTextual Features\nMetadata Tags\n\n\n\n\ndoc-1\n1.0\nTech\n[word embeddings]\nauthor: John, date: 2022-01-01\n\n\ndoc-1\n2.0\nTech\n[updated word embeddings]\nupdated tags: keywords\n\n\n\n\n\n1.0.4 4. Supporting Different Embedding Models\nTo support different embedding models:\n\nUse a modular architecture that allows the addition of new embedding models.\nImplement a standard interface for embedding models to ensure compatibility.\n\nExample:\n\nAdd new embedding model using a RESTful API or a message queue.\nUpdate the metadata field to use the new embedding model.\nRestart the application or re-run the embedding process.\n\n\n\n1.0.5 5. Edge Cases\nSome additional edge cases to consider:\n\nHandling duplicate documents: Implement measures to handle duplicate documents, such as using a unique identifier for each document or storing the most recent version of the metadata.\nDealing with sparse data: Handle sparse data by using techniques such as interpolation or imputation to estimate missing values.\nOptimizing storage and query performance: Optimize storage and query performance by indexing frequently used metadata fields.\n\nExample Edge Cases:\n\nDuplicate Documents: Duplicate documents can occur when multiple users submit the same document. To handle this, store a unique identifier for each document and use it to identify duplicates.\nSparse Data: Sparse data occurs when some documents have missing values in their metadata fields. To handle this, use interpolation or imputation techniques to estimate missing values.\n\nOther edge cases:\n\nHandling non-textual metadata\nDealing with large volumes of metadata updates\nOptimizing storage and query performance for large datasets",
    "crumbs": [
      "004 Production Ready"
    ]
  },
  {
    "objectID": "006_generate_colbert_multivectors.html",
    "href": "006_generate_colbert_multivectors.html",
    "title": "How to Generate ColBERT Multivectors with FastEmbed",
    "section": "",
    "text": "https://qdrant.tech/documentation/fastembed/fastembed-colbert/\nfrom fastembed import LateInteractionTextEmbedding\nLateInteractionTextEmbedding.list_supported_models()[0]\n\n{'model': 'colbert-ir/colbertv2.0',\n 'sources': {'hf': 'colbert-ir/colbertv2.0', 'url': None},\n 'model_file': 'model.onnx',\n 'description': 'Late interaction model',\n 'license': 'mit',\n 'size_in_GB': 0.44,\n 'additional_files': [],\n 'dim': 128,\n 'tasks': {}}\nembedding_model = LateInteractionTextEmbedding(\"colbert-ir/colbertv2.0\")",
    "crumbs": [
      "How to Generate ColBERT Multivectors with FastEmbed"
    ]
  },
  {
    "objectID": "006_generate_colbert_multivectors.html#embed-data",
    "href": "006_generate_colbert_multivectors.html#embed-data",
    "title": "How to Generate ColBERT Multivectors with FastEmbed",
    "section": "1 Embed data",
    "text": "1 Embed data\n\ndescriptions = [\n    \"In 1431, Jeanne d'Arc is placed on trial on charges of heresy. The ecclesiastical jurists attempt to force Jeanne to recant her claims of holy visions.\",\n    \"A film projectionist longs to be a detective, and puts his meagre skills to work when he is framed by a rival for stealing his girlfriend's father's pocketwatch.\",\n    \"A group of high-end professional thieves start to feel the heat from the LAPD when they unknowingly leave a clue at their latest heist.\",\n    \"A petty thief with an utter resemblance to a samurai warlord is hired as the lord's double. When the warlord later dies the thief is forced to take up arms in his place.\",\n    \"A young boy named Kubo must locate a magical suit of armour worn by his late father in order to defeat a vengeful spirit from the past.\",\n    \"A biopic detailing the 2 decades that Punjabi Sikh revolutionary Udham Singh spent planning the assassination of the man responsible for the Jallianwala Bagh massacre.\",\n    \"When a machine that allows therapists to enter their patients' dreams is stolen, all hell breaks loose. Only a young female therapist, Paprika, can stop it.\",\n    \"An ordinary word processor has the worst night of his life after he agrees to visit a girl in Soho whom he met that evening at a coffee shop.\",\n    \"A story that revolves around drug abuse in the affluent north Indian State of Punjab and how the youth there have succumbed to it en-masse resulting in a socio-economic decline.\",\n    \"A world-weary political journalist picks up the story of a woman's search for her son, who was taken away from her decades ago after she became pregnant and was forced to live in a convent.\",\n    \"Concurrent theatrical ending of the TV series Neon Genesis Evangelion (1995).\",\n    \"During World War II, a rebellious U.S. Army Major is assigned a dozen convicted murderers to train and lead them into a mass assassination mission of German officers.\",\n    \"The toys are mistakenly delivered to a day-care center instead of the attic right before Andy leaves for college, and it's up to Woody to convince the other toys that they weren't abandoned and to return home.\",\n    \"A soldier fighting aliens gets to relive the same day over and over again, the day restarting every time he dies.\",\n    \"After two male musicians witness a mob hit, they flee the state in an all-female band disguised as women, but further complications set in.\",\n    \"Exiled into the dangerous forest by her wicked stepmother, a princess is rescued by seven dwarf miners who make her part of their household.\",\n    \"A renegade reporter trailing a young runaway heiress for a big story joins her on a bus heading from Florida to New York, and they end up stuck with each other when the bus leaves them behind at one of the stops.\",\n    \"Story of 40-man Turkish task force who must defend a relay station.\",\n    \"Spinal Tap, one of England's loudest bands, is chronicled by film director Marty DiBergi on what proves to be a fateful tour.\",\n    \"Oskar, an overlooked and bullied boy, finds love and revenge through Eli, a beautiful but peculiar girl.\",\n]\n\n\ndescriptions_embeddings = list(embedding_model.embed(descriptions))\ndescriptions_embeddings[0].shape\n\n(48, 128)",
    "crumbs": [
      "How to Generate ColBERT Multivectors with FastEmbed"
    ]
  },
  {
    "objectID": "006_generate_colbert_multivectors.html#upload-embeddings-to-qdrant",
    "href": "006_generate_colbert_multivectors.html#upload-embeddings-to-qdrant",
    "title": "How to Generate ColBERT Multivectors with FastEmbed",
    "section": "2 Upload embeddings to Qdrant",
    "text": "2 Upload embeddings to Qdrant\n\nfrom qdrant_client import QdrantClient, models\n\nqdrant_client = QdrantClient(\":memory:\")\n\n\nqdrant_client.create_collection(\n    collection_name=\"movies\",\n    vectors_config=models.VectorParams(\n        size=128,  # Size of each vector produced by ColBERT\n        distance=models.Distance.COSINE,\n        multivector_config=models.MultiVectorConfig(\n            comparator=models.MultiVectorComparator.MAX_SIM\n        ),\n    ),\n)\n\nTrue\n\n\n\nmetadata = [\n    {\n        \"movie_name\": \"The Passion of Joan of Arc\",\n        \"movie_watch_time_min\": 114,\n        \"movie_description\": \"In 1431, Jeanne d'Arc is placed on trial on charges of heresy. The ecclesiastical jurists attempt to force Jeanne to recant her claims of holy visions.\",\n    },\n    {\n        \"movie_name\": \"Sherlock Jr.\",\n        \"movie_watch_time_min\": 45,\n        \"movie_description\": \"A film projectionist longs to be a detective, and puts his meagre skills to work when he is framed by a rival for stealing his girlfriend's father's pocketwatch.\",\n    },\n    {\n        \"movie_name\": \"Heat\",\n        \"movie_watch_time_min\": 170,\n        \"movie_description\": \"A group of high-end professional thieves start to feel the heat from the LAPD when they unknowingly leave a clue at their latest heist.\",\n    },\n    {\n        \"movie_name\": \"Kagemusha\",\n        \"movie_watch_time_min\": 162,\n        \"movie_description\": \"A petty thief with an utter resemblance to a samurai warlord is hired as the lord's double. When the warlord later dies the thief is forced to take up arms in his place.\",\n    },\n    {\n        \"movie_name\": \"Kubo and the Two Strings\",\n        \"movie_watch_time_min\": 101,\n        \"movie_description\": \"A young boy named Kubo must locate a magical suit of armour worn by his late father in order to defeat a vengeful spirit from the past.\",\n    },\n    {\n        \"movie_name\": \"Sardar Udham\",\n        \"movie_watch_time_min\": 164,\n        \"movie_description\": \"A biopic detailing the 2 decades that Punjabi Sikh revolutionary Udham Singh spent planning the assassination of the man responsible for the Jallianwala Bagh massacre.\",\n    },\n    {\n        \"movie_name\": \"Paprika\",\n        \"movie_watch_time_min\": 90,\n        \"movie_description\": \"When a machine that allows therapists to enter their patients' dreams is stolen, all hell breaks loose. Only a young female therapist, Paprika, can stop it.\",\n    },\n    {\n        \"movie_name\": \"After Hours\",\n        \"movie_watch_time_min\": 97,\n        \"movie_description\": \"An ordinary word processor has the worst night of his life after he agrees to visit a girl in Soho whom he met that evening at a coffee shop.\",\n    },\n    {\n        \"movie_name\": \"Udta Punjab\",\n        \"movie_watch_time_min\": 148,\n        \"movie_description\": \"A story that revolves around drug abuse in the affluent north Indian State of Punjab and how the youth there have succumbed to it en-masse resulting in a socio-economic decline.\",\n    },\n    {\n        \"movie_name\": \"Philomena\",\n        \"movie_watch_time_min\": 98,\n        \"movie_description\": \"A world-weary political journalist picks up the story of a woman's search for her son, who was taken away from her decades ago after she became pregnant and was forced to live in a convent.\",\n    },\n    {\n        \"movie_name\": \"Neon Genesis Evangelion: The End of Evangelion\",\n        \"movie_watch_time_min\": 87,\n        \"movie_description\": \"Concurrent theatrical ending of the TV series Neon Genesis Evangelion (1995).\",\n    },\n    {\n        \"movie_name\": \"The Dirty Dozen\",\n        \"movie_watch_time_min\": 150,\n        \"movie_description\": \"During World War II, a rebellious U.S. Army Major is assigned a dozen convicted murderers to train and lead them into a mass assassination mission of German officers.\",\n    },\n    {\n        \"movie_name\": \"Toy Story 3\",\n        \"movie_watch_time_min\": 103,\n        \"movie_description\": \"The toys are mistakenly delivered to a day-care center instead of the attic right before Andy leaves for college, and it's up to Woody to convince the other toys that they weren't abandoned and to return home.\",\n    },\n    {\n        \"movie_name\": \"Edge of Tomorrow\",\n        \"movie_watch_time_min\": 113,\n        \"movie_description\": \"A soldier fighting aliens gets to relive the same day over and over again, the day restarting every time he dies.\",\n    },\n    {\n        \"movie_name\": \"Some Like It Hot\",\n        \"movie_watch_time_min\": 121,\n        \"movie_description\": \"After two male musicians witness a mob hit, they flee the state in an all-female band disguised as women, but further complications set in.\",\n    },\n    {\n        \"movie_name\": \"Snow White and the Seven Dwarfs\",\n        \"movie_watch_time_min\": 83,\n        \"movie_description\": \"Exiled into the dangerous forest by her wicked stepmother, a princess is rescued by seven dwarf miners who make her part of their household.\",\n    },\n    {\n        \"movie_name\": \"It Happened One Night\",\n        \"movie_watch_time_min\": 105,\n        \"movie_description\": \"A renegade reporter trailing a young runaway heiress for a big story joins her on a bus heading from Florida to New York, and they end up stuck with each other when the bus leaves them behind at one of the stops.\",\n    },\n    {\n        \"movie_name\": \"Nefes: Vatan Sagolsun\",\n        \"movie_watch_time_min\": 128,\n        \"movie_description\": \"Story of 40-man Turkish task force who must defend a relay station.\",\n    },\n    {\n        \"movie_name\": \"This Is Spinal Tap\",\n        \"movie_watch_time_min\": 82,\n        \"movie_description\": \"Spinal Tap, one of England's loudest bands, is chronicled by film director Marty DiBergi on what proves to be a fateful tour.\",\n    },\n    {\n        \"movie_name\": \"Let the Right One In\",\n        \"movie_watch_time_min\": 114,\n        \"movie_description\": \"Oskar, an overlooked and bullied boy, finds love and revenge through Eli, a beautiful but peculiar girl.\",\n    },\n]\n\n\nqdrant_client.upload_points(\n    collection_name=\"movies\",\n    points=[\n        models.PointStruct(id=idx, payload=metadata[idx], vector=vector)\n        for idx, vector in enumerate(descriptions_embeddings)\n    ],\n)",
    "crumbs": [
      "How to Generate ColBERT Multivectors with FastEmbed"
    ]
  },
  {
    "objectID": "006_generate_colbert_multivectors.html#querying",
    "href": "006_generate_colbert_multivectors.html#querying",
    "title": "How to Generate ColBERT Multivectors with FastEmbed",
    "section": "3 Querying",
    "text": "3 Querying\n\nqdrant_client.query_points(\n    collection_name=\"movies\",\n    query=list(\n        embedding_model.query_embed(\n            \"A movie for kids with fantasy elements and wonders\"\n        )\n    )[0],\n    limit=1,\n    with_vectors=False,  # Returns vectors\n    with_payload=True,  # Returns metadata\n)\n\nQueryResponse(points=[ScoredPoint(id=4, version=0, score=12.06346668699247, payload={'movie_name': 'Kubo and the Two Strings', 'movie_watch_time_min': 101, 'movie_description': 'A young boy named Kubo must locate a magical suit of armour worn by his late father in order to defeat a vengeful spirit from the past.'}, vector=None, shard_key=None, order_value=None)])",
    "crumbs": [
      "How to Generate ColBERT Multivectors with FastEmbed"
    ]
  },
  {
    "objectID": "008_smalldocling.html",
    "href": "008_smalldocling.html",
    "title": "OCR with SmallDocling",
    "section": "",
    "text": "https://huggingface.co/ds4sd/SmolDocling-256M-preview\n\n# Prerequisites:\n# pip install torch\n# pip install docling_core\n# pip install transformers\n\nfrom pathlib import Path\n\nimport torch\nfrom docling_core.types.doc import DoclingDocument\nfrom docling_core.types.doc.document import DocTagsDocument\nfrom transformers import AutoModelForVision2Seq, AutoProcessor\nfrom transformers.image_utils import load_image\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Load images\nimage = load_image(\n    \"images/us_stock_market_news.png\"\n    # \"https://upload.wikimedia.org/wikipedia/commons/7/76/GazettedeFrance.jpg\"\n)\n\n# Initialize processor and model\nprocessor = AutoProcessor.from_pretrained(\"ds4sd/SmolDocling-256M-preview\")\nmodel = AutoModelForVision2Seq.from_pretrained(\n    \"ds4sd/SmolDocling-256M-preview\",\n    torch_dtype=torch.bfloat16,\n    _attn_implementation=\"flash_attention_2\" if DEVICE == \"cuda\" else \"eager\",\n).to(DEVICE)\n\n# Create input messages\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\"},\n            {\"type\": \"text\", \"text\": \"Convert this page to docling.\"},\n        ],\n    },\n]\n\n# Prepare inputs\nprompt = processor.apply_chat_template(messages, add_generation_prompt=True)\ninputs = processor(text=prompt, images=[image], return_tensors=\"pt\")\ninputs = inputs.to(DEVICE)\n\n# Generate outputs\ngenerated_ids = model.generate(**inputs, max_new_tokens=8192)\nprompt_length = inputs.input_ids.shape[1]\ntrimmed_generated_ids = generated_ids[:, prompt_length:]\ndoctags = processor.batch_decode(\n    trimmed_generated_ids,\n    skip_special_tokens=False,\n)[0].lstrip()\n\n# Populate document\ndoctags_doc = DocTagsDocument.from_doctags_and_image_pairs([doctags], [image])\nprint(doctags)\n# create a docling document\ndoc = DoclingDocument(name=\"Document\")\ndoc.load_from_doctags(doctags_doc)\n\n# export as any format\n# HTML\n# Path(\"Out/\").mkdir(parents=True, exist_ok=True)\n# output_path_html = Path(\"Out/\") / \"example.html\"\n# doc.save_as_html(output_path_html)\n# MD\nprint(doc.export_to_markdown())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n\n\n&lt;doctag&gt;&lt;picture&gt;&lt;loc_40&gt;&lt;loc_14&gt;&lt;loc_110&gt;&lt;loc_32&gt;&lt;logo&gt;&lt;/picture&gt;\n&lt;section_header_level_1&gt;&lt;loc_40&gt;&lt;loc_61&gt;&lt;loc_421&gt;&lt;loc_94&gt;US equity bull market remains intact despite fragile sentiment&lt;/section_header_level_1&gt;\n&lt;section_header_level_1&gt;&lt;loc_40&gt;&lt;loc_97&gt;&lt;loc_176&gt;&lt;loc_107&gt;UBS House View - Daily US&lt;/section_header_level_1&gt;\n&lt;text&gt;&lt;loc_40&gt;&lt;loc_116&gt;&lt;loc_285&gt;&lt;loc_147&gt;Ulrike Hoffmann-Burchardi, Head CIO Global Equities, UBS Financial Services Inc. (UBS FS) Solita Marcelli, GWM Chief Investment Officer Americas, UBS Financial Services Inc. (UBS FS) Mark Haeefe, Global Wealth Management Chief Investment Officer, UBS AG David Lefkowitz, CFA, CIO Head of US Equities, UBS Financial Services Inc. (UBS FS) Sundeep Gantori, CFA, CAIA, Equity Strategist, UBS AG Singapore Branch Daisy Teng, Strategist, UBS AG Singapore Branch&lt;/text&gt;\n&lt;section_header_level_1&gt;&lt;loc_40&gt;&lt;loc_170&gt;&lt;loc_103&gt;&lt;loc_176&gt;From the studio:&lt;/section_header_level_1&gt;\n&lt;text&gt;&lt;loc_40&gt;&lt;loc_178&gt;&lt;loc_300&gt;&lt;loc_192&gt;Video: CIO Mark Haeefele's on investing through heightened political uncertainty (3:40)&lt;/text&gt;\n&lt;text&gt;&lt;loc_40&gt;&lt;loc_194&gt;&lt;loc_300&gt;&lt;loc_208&gt;Podcast: The Al sector playbook after NVIDIA's results, with Sundeep, Wayne, and Jon (11:10)&lt;/text&gt;\n&lt;section_header_level_1&gt;&lt;loc_40&gt;&lt;loc_215&gt;&lt;loc_114&gt;&lt;loc_221&gt;Thought of the day&lt;/section_header_level_1&gt;\n&lt;text&gt;&lt;loc_40&gt;&lt;loc_223&gt;&lt;loc_300&gt;&lt;loc_245&gt;Investor sentiment took another hit on Thursday amid fresh tariff headlines and concerns over NVIDIA's near-term growth outlook, with the S&P 500 falling 1.6%, erasing all of its gains for the year. The Nasdaq was down 2.8%.&lt;/text&gt;\n&lt;text&gt;&lt;loc_40&gt;&lt;loc_255&gt;&lt;loc_300&gt;&lt;loc_293&gt;US President Donald Trump said his proposed 25% tariffs on Mexican and Canadian goods will take effect on 4 March, alongside an extra 10% duty on Chinese imports on top of the 10% tariffs that went into effect early February. He added that fentanyl was still coming into the US from these countries \"at very high and unacceptable levels.\"&lt;/text&gt;\n&lt;text&gt;&lt;loc_40&gt;&lt;loc_301&gt;&lt;loc_300&gt;&lt;loc_338&gt;Investors have had to contend with fast-moving headlines over the past few weeks and months-the Federal Reserve's signal of a slower pace of monetary easing, sticky inflation readings, the emergence of low-cost AI models like DeepSeek, weaker consumer confidence, and still-elevated geopolitical uncertainty in the Middle East and over the war in Ukraine.&lt;/text&gt;\n&lt;text&gt;&lt;loc_40&gt;&lt;loc_347&gt;&lt;loc_300&gt;&lt;loc_400&gt;Indeed, data points are showing that investor sentiment is poor. Earlier this week, the American Association of Individual Investors (AAI) reported that only 19% of respondents to their weekly survey are expecting stocks to be higher over the next six months. This is lower than 98% of all observations since the survey started in 1987. More technical measures of investor sentiment, such as the put-call ratio, also suggest high levels of investor fear.&lt;/text&gt;\n&lt;text&gt;&lt;loc_40&gt;&lt;loc_409&gt;&lt;loc_300&gt;&lt;loc_431&gt;But while we have cautioned that volatility is likely to be higher this year due to policy uncertainty and trade frictions, we reiterate our view that the bull market is intact, and we expect US equities to end the year higher.&lt;/text&gt;\n&lt;text&gt;&lt;loc_40&gt;&lt;loc_439&gt;&lt;loc_300&gt;&lt;loc_453&gt;Very low sentiment readings tend to be a contrarian indicator. Perhaps somewhat counterintuitively, stocks typically perform well after poor&lt;/text&gt;\n&lt;text&gt;&lt;loc_40&gt;&lt;loc_464&gt;&lt;loc_444&gt;&lt;loc_479&gt;This report has been prepared by UBS Financial Services Inc. (UBS FS), UBS AG, UBS AG Singapore Branch. Please see important disclaimers and disclosures at the end of the document.&lt;/text&gt;\n&lt;text&gt;&lt;loc_310&gt;&lt;loc_14&gt;&lt;loc_400&gt;&lt;loc_34&gt;28 February 2025, 11:29AM UTC Chief Investment Office GWM Investment Research&lt;/text&gt;\n&lt;section_header_level_1&gt;&lt;loc_310&gt;&lt;loc_170&gt;&lt;loc_423&gt;&lt;loc_176&gt;What to watch: 3 March 2025&lt;/section_header_level_1&gt;\n&lt;unordered_list&gt;&lt;list_item&gt;&lt;loc_310&gt;&lt;loc_178&gt;&lt;loc_424&gt;&lt;loc_184&gt;· US ISM manufacturing survey&lt;/list_item&gt;\n&lt;list_item&gt;&lt;loc_310&gt;&lt;loc_189&gt;&lt;loc_450&gt;&lt;loc_203&gt;· Eurozone flash estimate of consumer price inflation for February&lt;/list_item&gt;\n&lt;/unordered_list&gt;\n&lt;/doctag&gt;&lt;end_of_utterance&gt;\n&lt;!-- image --&gt;\n\n## US equity bull market remains intact despite fragile sentiment\n\n## UBS House View - Daily US\n\nUlrike Hoffmann-Burchardi, Head CIO Global Equities, UBS Financial Services Inc. (UBS FS) Solita Marcelli, GWM Chief Investment Officer Americas, UBS Financial Services Inc. (UBS FS) Mark Haeefe, Global Wealth Management Chief Investment Officer, UBS AG David Lefkowitz, CFA, CIO Head of US Equities, UBS Financial Services Inc. (UBS FS) Sundeep Gantori, CFA, CAIA, Equity Strategist, UBS AG Singapore Branch Daisy Teng, Strategist, UBS AG Singapore Branch\n\n## From the studio:\n\nVideo: CIO Mark Haeefele's on investing through heightened political uncertainty (3:40)\n\nPodcast: The Al sector playbook after NVIDIA's results, with Sundeep, Wayne, and Jon (11:10)\n\n## Thought of the day\n\nInvestor sentiment took another hit on Thursday amid fresh tariff headlines and concerns over NVIDIA's near-term growth outlook, with the S&amp;P 500 falling 1.6%, erasing all of its gains for the year. The Nasdaq was down 2.8%.\n\nUS President Donald Trump said his proposed 25% tariffs on Mexican and Canadian goods will take effect on 4 March, alongside an extra 10% duty on Chinese imports on top of the 10% tariffs that went into effect early February. He added that fentanyl was still coming into the US from these countries \"at very high and unacceptable levels.\"\n\nInvestors have had to contend with fast-moving headlines over the past few weeks and months-the Federal Reserve's signal of a slower pace of monetary easing, sticky inflation readings, the emergence of low-cost AI models like DeepSeek, weaker consumer confidence, and still-elevated geopolitical uncertainty in the Middle East and over the war in Ukraine.\n\nIndeed, data points are showing that investor sentiment is poor. Earlier this week, the American Association of Individual Investors (AAI) reported that only 19% of respondents to their weekly survey are expecting stocks to be higher over the next six months. This is lower than 98% of all observations since the survey started in 1987. More technical measures of investor sentiment, such as the put-call ratio, also suggest high levels of investor fear.\n\nBut while we have cautioned that volatility is likely to be higher this year due to policy uncertainty and trade frictions, we reiterate our view that the bull market is intact, and we expect US equities to end the year higher.\n\nVery low sentiment readings tend to be a contrarian indicator. Perhaps somewhat counterintuitively, stocks typically perform well after poor\n\nThis report has been prepared by UBS Financial Services Inc. (UBS FS), UBS AG, UBS AG Singapore Branch. Please see important disclaimers and disclosures at the end of the document.\n\n28 February 2025, 11:29AM UTC Chief Investment Office GWM Investment Research\n\n## What to watch: 3 March 2025\n\n- · US ISM manufacturing survey\n- · Eurozone flash estimate of consumer price inflation for February",
    "crumbs": [
      "OCR with SmallDocling"
    ]
  },
  {
    "objectID": "ar101_001_collaborative_filtering.html",
    "href": "ar101_001_collaborative_filtering.html",
    "title": "Advance Retrieval 101: Use Collaborative Filtering to Build a Movie Recommendation System",
    "section": "",
    "text": "https://qdrant.tech/documentation/advanced-tutorials/collaborative-filtering/\nfrom collections import defaultdict\n\nimport pandas as pd\nfrom qdrant_client import QdrantClient, models\nfrom qdrant_client.models import NamedSparseVector, PointStruct, SparseVector",
    "crumbs": [
      "Advance Retrieval 101: Use Collaborative Filtering to Build a Movie Recommendation System"
    ]
  },
  {
    "objectID": "ar101_001_collaborative_filtering.html#prepare-the-data",
    "href": "ar101_001_collaborative_filtering.html#prepare-the-data",
    "title": "Advance Retrieval 101: Use Collaborative Filtering to Build a Movie Recommendation System",
    "section": "1 Prepare the data",
    "text": "1 Prepare the data\n\n# Load CSV file\nratings_df = pd.read_csv(\"data/ml-latest-small/ratings.csv\", low_memory=False)\nmovies_df = pd.read_csv(\"data/ml-latest-small/movies.csv\", low_memory=False)\n\n# Convert movieId in ratings_df and movies_df to string\nratings_df[\"movieId\"] = ratings_df[\"movieId\"].astype(str)\nmovies_df[\"movieId\"] = movies_df[\"movieId\"].astype(str)\n\nrating = ratings_df[\"rating\"]\n\n# Normalize ratings\nratings_df[\"rating\"] = (rating - rating.mean()) / rating.std()\n\n# Merge rating with movie metadata to get movie titles\nmerged_df = ratings_df.merge(\n    movies_df[[\"movieId\", \"title\"]], left_on=\"movieId\", right_on=\"movieId\", how=\"inner\"\n)\n\n# Aggregate ratings to handle duplicate (userId, title) pairs\nratings_agg_df = merged_df.groupby([\"userId\", \"movieId\"]).rating.mean().reset_index()\nratings_agg_df.head()\n\n\n\n\n\n\n\n\nuserId\nmovieId\nrating\n\n\n\n\n0\n1\n1\n0.478109\n\n\n1\n1\n1009\n-0.481096\n\n\n2\n1\n101\n1.437315\n\n\n3\n1\n1023\n1.437315\n\n\n4\n1\n1024\n1.437315",
    "crumbs": [
      "Advance Retrieval 101: Use Collaborative Filtering to Build a Movie Recommendation System"
    ]
  },
  {
    "objectID": "ar101_001_collaborative_filtering.html#convert-to-sparse",
    "href": "ar101_001_collaborative_filtering.html#convert-to-sparse",
    "title": "Advance Retrieval 101: Use Collaborative Filtering to Build a Movie Recommendation System",
    "section": "2 Convert to Sparse",
    "text": "2 Convert to Sparse\n\n# Convert ratings to sparse vectors.\nuser_sparse_vectors = defaultdict(lambda: {\"values\": [], \"indices\": []})\nfor row in ratings_agg_df.itertuples():\n    user_sparse_vectors[row.userId][\"values\"].append(row.rating)\n    user_sparse_vectors[row.userId][\"indices\"].append(int(row.movieId))",
    "crumbs": [
      "Advance Retrieval 101: Use Collaborative Filtering to Build a Movie Recommendation System"
    ]
  },
  {
    "objectID": "ar101_001_collaborative_filtering.html#upload-the-data",
    "href": "ar101_001_collaborative_filtering.html#upload-the-data",
    "title": "Advance Retrieval 101: Use Collaborative Filtering to Build a Movie Recommendation System",
    "section": "3 Upload the data",
    "text": "3 Upload the data\n\ndef data_generator():\n    for user_id, sparse_vector in user_sparse_vectors.items():\n        yield PointStruct(\n            id=user_id,\n            vector={\n                \"ratings\": SparseVector(\n                    indices=sparse_vector[\"indices\"], values=sparse_vector[\"values\"]\n                )\n            },\n            payload={\"user_id\": user_id, \"movie_id\": sparse_vector[\"indices\"]},\n        )\n\n\nclient = QdrantClient(\":memory:\")\nclient.create_collection(\n    collection_name=\"movies\",\n    sparse_vectors_config={\"ratings\": models.SparseVectorParams()},\n    vectors_config={},\n)\nclient.upload_points(\n    collection_name=\"movies\",\n    points=data_generator(),\n)",
    "crumbs": [
      "Advance Retrieval 101: Use Collaborative Filtering to Build a Movie Recommendation System"
    ]
  },
  {
    "objectID": "ar101_001_collaborative_filtering.html#define-query",
    "href": "ar101_001_collaborative_filtering.html#define-query",
    "title": "Advance Retrieval 101: Use Collaborative Filtering to Build a Movie Recommendation System",
    "section": "4 Define query",
    "text": "4 Define query\n\nmy_ratings = {\n    603: 1,  # Matrix\n    13475: 1,  # Star Trek\n    11: 1,  # Star Wars\n    1091: -1,  # The Thing\n    862: 1,  # Toy Story\n    597: -1,  # Titanic\n    680: -1,  # Pulp Fiction\n    13: 1,  # Forrest Gump\n    120: 1,  # Lord of the Rings\n    87: -1,  # Indiana Jones\n    562: -1,  # Die Hard\n}\n\n\n# Create sparse vector from my_ratings\ndef to_vector(ratings):\n    vector = SparseVector(values=[], indices=[])\n    for movie_id, rating in ratings.items():\n        vector.values.append(rating)\n        vector.indices.append(movie_id)\n    return vector",
    "crumbs": [
      "Advance Retrieval 101: Use Collaborative Filtering to Build a Movie Recommendation System"
    ]
  },
  {
    "objectID": "ar101_001_collaborative_filtering.html#run-the-query",
    "href": "ar101_001_collaborative_filtering.html#run-the-query",
    "title": "Advance Retrieval 101: Use Collaborative Filtering to Build a Movie Recommendation System",
    "section": "5 Run the query",
    "text": "5 Run the query\n\n# Perform the search\nresults = client.query_points(\n    collection_name=\"movies\", query=to_vector(my_ratings), using=\"ratings\", limit=20\n).points\n\n\n# Convert results to scores and sort by score\ndef results_to_scores(results):\n    movie_scores = defaultdict(lambda: 0)\n    for result in results:\n        for movie_id in result.payload[\"movie_id\"]:\n            movie_scores[movie_id] += result.score\n    return movie_scores\n\n\n# Convert results to scores and sort by score\nmovie_scores = results_to_scores(results)\ntop_movies = sorted(movie_scores.items(), key=lambda x: x[1], reverse=True)\n\n\nfor movieId, score in top_movies[:5]:\n    movie = movies_df[movies_df['movieId'] == str(movieId)]\n    print(movie.title.values[0], score)\n\nForrest Gump (1994) 44.63442826271057\nPulp Fiction (1994) 43.19412624835968\nPretty Woman (1990) 42.71452331542969\nStar Wars: Episode V - The Empire Strikes Back (1980) 37.91998839378357\nAmerican Beauty (1999) 37.91849493980408",
    "crumbs": [
      "Advance Retrieval 101: Use Collaborative Filtering to Build a Movie Recommendation System"
    ]
  },
  {
    "objectID": "ar101_003_navigate_codebase.html",
    "href": "ar101_003_navigate_codebase.html",
    "title": "Advanced Retrieval 101: Navigate Your Codebase with Semantic Search and Qdrant",
    "section": "",
    "text": "https://qdrant.tech/documentation/advanced-tutorials/code-search/\n# !curl https://storage.googleapis.com/tutorial-attachments/code-search/structures.jsonl -O\n\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 4805k  100 4805k    0     0   913k      0  0:00:05  0:00:05 --:--:-- 1137k\n# !tail structures.jsonl\nimport json\n\nstructures = []\nwith open(\"structures.jsonl\", \"r\") as fp:\n    for i, row in enumerate(fp):\n        entry = json.loads(row)\n        structures.append(entry)",
    "crumbs": [
      "Advanced Retrieval 101: Navigate Your Codebase with Semantic Search and Qdrant"
    ]
  },
  {
    "objectID": "ar101_003_navigate_codebase.html#code-to-natural-language-conversion",
    "href": "ar101_003_navigate_codebase.html#code-to-natural-language-conversion",
    "title": "Advanced Retrieval 101: Navigate Your Codebase with Semantic Search and Qdrant",
    "section": "1 Code to natural language conversion",
    "text": "1 Code to natural language conversion\n\nimport re\nfrom typing import Any, Dict\n\nimport inflection\n\n\ndef textify(chunk: Dict[str, Any]) -&gt; str:\n    # Get rid of all the camel case / snake case\n    # - inflection.underscore changes the camel case to snake case\n    # - inflection.humanize converts the snake case to human readable form\n    name = inflection.humanize(inflection.underscore(chunk[\"name\"]))\n    signature = inflection.humanize(inflection.underscore(chunk[\"signature\"]))\n\n    # Check if docstring is provided\n    docstring = \"\"\n    if chunk[\"docstring\"]:\n        docstring = f\"that does {chunk['docstring']} \"\n\n    # Extract the location of that snippet of code\n    context = (\n        f\"module {chunk['context']['module']} \" f\"file {chunk['context']['file_name']}\"\n    )\n    if chunk[\"context\"][\"struct_name\"]:\n        struct_name = inflection.humanize(\n            inflection.underscore(chunk[\"context\"][\"struct_name\"])\n        )\n        context = f\"defined in struct {struct_name} {context}\"\n\n    # Combine all the bits and pieces together\n    text_representation = (\n        f\"{chunk['code_type']} {name} \"\n        f\"{docstring}\"\n        f\"defined as {signature} \"\n        f\"{context}\"\n    )\n\n    # Remove any special characters and concatenate the tokens\n    tokens = re.split(r\"\\W\", text_representation)\n    tokens = filter(lambda x: x, tokens)\n    return \" \".join(tokens)",
    "crumbs": [
      "Advanced Retrieval 101: Navigate Your Codebase with Semantic Search and Qdrant"
    ]
  },
  {
    "objectID": "ar101_003_navigate_codebase.html#natural-language-embeddings",
    "href": "ar101_003_navigate_codebase.html#natural-language-embeddings",
    "title": "Advanced Retrieval 101: Navigate Your Codebase with Semantic Search and Qdrant",
    "section": "2 Natural language embeddings",
    "text": "2 Natural language embeddings\n\ntext_representations = list(map(textify, structures))\n\n\nfrom sentence_transformers import SentenceTransformer\n\nnlp_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\nnlp_embeddings = nlp_model.encode(\n    text_representations,\n    show_progress_bar=True,\n)",
    "crumbs": [
      "Advanced Retrieval 101: Navigate Your Codebase with Semantic Search and Qdrant"
    ]
  },
  {
    "objectID": "ar101_003_navigate_codebase.html#code-embeddings",
    "href": "ar101_003_navigate_codebase.html#code-embeddings",
    "title": "Advanced Retrieval 101: Navigate Your Codebase with Semantic Search and Qdrant",
    "section": "3 Code embeddings",
    "text": "3 Code embeddings\n\n# Extract the code snippets from the structures to a separate list\ncode_snippets = [structure[\"context\"][\"snippet\"] for structure in structures]\ncode_model = SentenceTransformer(\n    \"jinaai/jina-embeddings-v2-base-code\", trust_remote_code=True\n)\ncode_model.max_seq_length = 8192  # increase the context length window\ncode_embeddings = code_model.encode(\n    code_snippets,\n    batch_size=4,\n    show_progress_bar=True,\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA new version of the following files was downloaded from https://huggingface.co/jinaai/jina-bert-v2-qk-post-norm:\n- configuration_bert.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n/Users/alextanhongpin/.cache/huggingface/modules/transformers_modules/jinaai/jina-bert-v2-qk-post-norm/3baf9e3ac750e76e8edd3019170176884695fb94/configuration_bert.py:29: UserWarning: optimum is not installed. To use OnnxConfig and BertOnnxConfig, make sure that `optimum` package is installed\n  warnings.warn(\"optimum is not installed. To use OnnxConfig and BertOnnxConfig, make sure that `optimum` package is installed\")\n\n\n\n\n\nA new version of the following files was downloaded from https://huggingface.co/jinaai/jina-bert-v2-qk-post-norm:\n- modeling_bert.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.",
    "crumbs": [
      "Advanced Retrieval 101: Navigate Your Codebase with Semantic Search and Qdrant"
    ]
  },
  {
    "objectID": "ar101_003_navigate_codebase.html#create-collection",
    "href": "ar101_003_navigate_codebase.html#create-collection",
    "title": "Advanced Retrieval 101: Navigate Your Codebase with Semantic Search and Qdrant",
    "section": "4 Create collection",
    "text": "4 Create collection\n\nfrom qdrant_client import QdrantClient, models\n\nclient = QdrantClient(\":memory:\")\nclient.create_collection(\n    \"qdrant-sources\",\n    vectors_config={\n        \"text\": models.VectorParams(\n            size=nlp_embeddings.shape[1],\n            distance=models.Distance.COSINE,\n        ),\n        \"code\": models.VectorParams(\n            size=code_embeddings.shape[1],\n            distance=models.Distance.COSINE,\n        ),\n    },\n)\n\nTrue\n\n\n\nimport uuid\n\npoints = [\n    models.PointStruct(\n        id=uuid.uuid4().hex,\n        vector={\n            \"text\": text_embedding,\n            \"code\": code_embedding,\n        },\n        payload=structure,\n    )\n    for text_embedding, code_embedding, structure in zip(\n        nlp_embeddings, code_embeddings, structures\n    )\n]\n\nclient.upload_points(\"qdrant-sources\", points=points, batch_size=64)",
    "crumbs": [
      "Advanced Retrieval 101: Navigate Your Codebase with Semantic Search and Qdrant"
    ]
  },
  {
    "objectID": "ar101_003_navigate_codebase.html#querying-the-codebase",
    "href": "ar101_003_navigate_codebase.html#querying-the-codebase",
    "title": "Advanced Retrieval 101: Navigate Your Codebase with Semantic Search and Qdrant",
    "section": "5 Querying the codebase",
    "text": "5 Querying the codebase\n\n5.1 Using text embedding\n\nquery = \"How do I count points in a collection?\"\n\nhits = client.query_points(\n    \"qdrant-sources\",\n    query=nlp_model.encode(query).tolist(),\n    using=\"text\",\n    limit=5,\n).points\n\n\nimport pprint\n\npprint.pp(hits[0].payload)\n\n{'name': 'count',\n 'signature': 'async fn count (& self , collection_name : & str , request : '\n              'CountRequestInternal , read_consistency : Option &lt; '\n              'ReadConsistency &gt; , shard_selection : ShardSelectorInternal ,) '\n              '-&gt; Result &lt; CountResult , StorageError &gt;',\n 'code_type': 'Function',\n 'docstring': '= \" Count points in the collection.\"',\n 'line': 120,\n 'line_from': 108,\n 'line_to': 132,\n 'context': {'module': 'toc',\n             'file_path': 'lib/storage/src/content_manager/toc/point_ops.rs',\n             'file_name': 'point_ops.rs',\n             'struct_name': 'TableOfContent',\n             'snippet': '    /// Count points in the collection.\\n'\n                        '    ///\\n'\n                        '    /// # Arguments\\n'\n                        '    ///\\n'\n                        '    /// * `collection_name` - in what collection do '\n                        'we count\\n'\n                        '    /// * `request` - [`CountRequestInternal`]\\n'\n                        '    /// * `shard_selection` - which local shard to '\n                        'use\\n'\n                        '    ///\\n'\n                        '    /// # Result\\n'\n                        '    ///\\n'\n                        '    /// Number of points in the collection.\\n'\n                        '    ///\\n'\n                        '    pub async fn count(\\n'\n                        '        &self,\\n'\n                        '        collection_name: &str,\\n'\n                        '        request: CountRequestInternal,\\n'\n                        '        read_consistency: Option&lt;ReadConsistency&gt;,\\n'\n                        '        shard_selection: ShardSelectorInternal,\\n'\n                        '    ) -&gt; Result&lt;CountResult, StorageError&gt; {\\n'\n                        '        let collection = '\n                        'self.get_collection(collection_name).await?;\\n'\n                        '        collection\\n'\n                        '            .count(request, read_consistency, '\n                        '&shard_selection)\\n'\n                        '            .await\\n'\n                        '            .map_err(|err| err.into())\\n'\n                        '    }\\n'}}\n\n\n\n\n5.2 Using code embedding\n\nhits = client.query_points(\n    \"qdrant-sources\",\n    query=code_model.encode(query).tolist(),\n    using=\"code\",\n    limit=5,\n).points\n\n\npprint.pp(hits[0].payload)\n\n{'name': 'count_indexed_points',\n 'signature': 'fn count_indexed_points (& self) -&gt; usize',\n 'code_type': 'Function',\n 'docstring': None,\n 'line': 612,\n 'line_from': 612,\n 'line_to': 614,\n 'context': {'module': 'field_index',\n             'file_path': 'lib/segment/src/index/field_index/geo_index.rs',\n             'file_name': 'geo_index.rs',\n             'struct_name': 'GeoMapIndex',\n             'snippet': '    fn count_indexed_points(&self) -&gt; usize {\\n'\n                        '        self.points_count()\\n'\n                        '    }\\n'}}\n\n\n\n\n5.3 Using text + code embedding\n\nresponses = client.query_batch_points(\n    \"qdrant-sources\",\n    requests=[\n        models.QueryRequest(\n            query=nlp_model.encode(query).tolist(),\n            using=\"text\",\n            with_payload=True,\n            limit=5,\n        ),\n        models.QueryRequest(\n            query=code_model.encode(query).tolist(),\n            using=\"code\",\n            with_payload=True,\n            limit=5,\n        ),\n    ],\n)\n\nresults = [response.points for response in responses]\n\n\npprint.pp(results[0][0].payload)\n\n{'name': 'count',\n 'signature': 'async fn count (& self , collection_name : & str , request : '\n              'CountRequestInternal , read_consistency : Option &lt; '\n              'ReadConsistency &gt; , shard_selection : ShardSelectorInternal ,) '\n              '-&gt; Result &lt; CountResult , StorageError &gt;',\n 'code_type': 'Function',\n 'docstring': '= \" Count points in the collection.\"',\n 'line': 120,\n 'line_from': 108,\n 'line_to': 132,\n 'context': {'module': 'toc',\n             'file_path': 'lib/storage/src/content_manager/toc/point_ops.rs',\n             'file_name': 'point_ops.rs',\n             'struct_name': 'TableOfContent',\n             'snippet': '    /// Count points in the collection.\\n'\n                        '    ///\\n'\n                        '    /// # Arguments\\n'\n                        '    ///\\n'\n                        '    /// * `collection_name` - in what collection do '\n                        'we count\\n'\n                        '    /// * `request` - [`CountRequestInternal`]\\n'\n                        '    /// * `shard_selection` - which local shard to '\n                        'use\\n'\n                        '    ///\\n'\n                        '    /// # Result\\n'\n                        '    ///\\n'\n                        '    /// Number of points in the collection.\\n'\n                        '    ///\\n'\n                        '    pub async fn count(\\n'\n                        '        &self,\\n'\n                        '        collection_name: &str,\\n'\n                        '        request: CountRequestInternal,\\n'\n                        '        read_consistency: Option&lt;ReadConsistency&gt;,\\n'\n                        '        shard_selection: ShardSelectorInternal,\\n'\n                        '    ) -&gt; Result&lt;CountResult, StorageError&gt; {\\n'\n                        '        let collection = '\n                        'self.get_collection(collection_name).await?;\\n'\n                        '        collection\\n'\n                        '            .count(request, read_consistency, '\n                        '&shard_selection)\\n'\n                        '            .await\\n'\n                        '            .map_err(|err| err.into())\\n'\n                        '    }\\n'}}\n\n\n\n\n5.4 Grouping the results\n\nresults = client.query_points_groups(\n    \"qdrant-sources\",\n    query=code_model.encode(query).tolist(),\n    using=\"code\",\n    group_by=\"context.module\",\n    limit=5,\n    group_size=1,\n)\n\n\npprint.pp(results.groups[0].hits[0].payload)\n\n{'name': 'count_indexed_points',\n 'signature': 'fn count_indexed_points (& self) -&gt; usize',\n 'code_type': 'Function',\n 'docstring': None,\n 'line': 612,\n 'line_from': 612,\n 'line_to': 614,\n 'context': {'module': 'field_index',\n             'file_path': 'lib/segment/src/index/field_index/geo_index.rs',\n             'file_name': 'geo_index.rs',\n             'struct_name': 'GeoMapIndex',\n             'snippet': '    fn count_indexed_points(&self) -&gt; usize {\\n'\n                        '        self.points_count()\\n'\n                        '    }\\n'}}",
    "crumbs": [
      "Advanced Retrieval 101: Navigate Your Codebase with Semantic Search and Qdrant"
    ]
  },
  {
    "objectID": "e_002_rag_deepseek.html",
    "href": "e_002_rag_deepseek.html",
    "title": "Essentials: 5 Minute RAG with Qdrant and DeepSeek",
    "section": "",
    "text": "https://qdrant.tech/documentation/rag-deepseek/\n\nfrom qdrant_client import QdrantClient, models\n\n\nclient = QdrantClient(\":memory:\")\nclient.set_model(\"BAAI/bge-base-en-v1.5\")\nclient.add(\n    collection_name=\"knowledge-base\",\n    # The collection is automatically created if it doesn't exist.\n    documents=[\n        \"Qdrant is a vector database & vector similarity search engine. It deploys as an API service providing search for the nearest high-dimensional vectors. With Qdrant, embeddings or neural network encoders can be turned into full-fledged applications for matching, searching, recommending, and much more!\",\n        \"Docker helps developers build, share, and run applications anywhere — without tedious environment configuration or management.\",\n        \"PyTorch is a machine learning framework based on the Torch library, used for applications such as computer vision and natural language processing.\",\n        \"MySQL is an open-source relational database management system (RDBMS). A relational database organizes data into one or more data tables in which data may be related to each other; these relations help structure the data. SQL is a language that programmers use to create, modify and extract data from the relational database, as well as control user access to the database.\",\n        \"NGINX is a free, open-source, high-performance HTTP server and reverse proxy, as well as an IMAP/POP3 proxy server. NGINX is known for its high performance, stability, rich feature set, simple configuration, and low resource consumption.\",\n        \"FastAPI is a modern, fast (high-performance), web framework for building APIs with Python 3.7+ based on standard Python type hints.\",\n        \"SentenceTransformers is a Python framework for state-of-the-art sentence, text and image embeddings. You can use this framework to compute sentence / text embeddings for more than 100 languages. These embeddings can then be compared e.g. with cosine-similarity to find sentences with a similar meaning. This can be useful for semantic textual similar, semantic search, or paraphrase mining.\",\n        \"The cron command-line utility is a job scheduler on Unix-like operating systems. Users who set up and maintain software environments use cron to schedule jobs (commands or shell scripts), also known as cron jobs, to run periodically at fixed times, dates, or intervals.\",\n    ],\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n['60aabd4c0a9a4323a4d613a068c36e33',\n 'b8f3fbe4a4734476a25d6e07ce6f962c',\n '2d5bec3dd79e415187c34dcd23332281',\n '5f30e4332ed745e3921756ba3b6db0f3',\n 'e714a72590ee451588adce8f05debdf5',\n '1e2833b9c65a4101918c4b8fd80d311d',\n '908559de554048d4bdd6342e6dfc96f6',\n '7034b9f4df0f4e1bbd849de14b2a8601']\n\n\n\nprompt = \"\"\"\nWhat tools should I need to use to build a web service using vector embeddings for search?\n\"\"\"\n\n\nfrom ollama import ChatResponse, chat\n\n\ndef query_deepseek(prompt) -&gt; str:\n    response: ChatResponse = chat(\n        model=\"deepseek-r1:8b\",\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": prompt,\n            },\n        ],\n    )\n    return response.message.content\n\n\nquery_deepseek(prompt)\n\n\"&lt;think&gt;\\nOkay, so I'm trying to figure out what tools I need to build a web service that uses vector embeddings for search. I remember hearing about vector embeddings in the context of machine learning and natural language processing, but I'm not exactly sure how they work or how they can be applied to building a search function.\\n\\nFirst, I think I need some way to generate these vectors from text inputs. Maybe there are libraries that can create embeddings based on the input data. The assistant mentioned TensorFlow and PyTorch as deep learning frameworks. Since I have some Python experience, maybe I should start with those. But what exactly do they offer in terms of vector embeddings?\\n\\nI remember there's something called Word2Vec which creates word vectors. So perhaps using a pre-trained model or implementing one could help generate these embeddings for each query. Also, FastText comes up often as an alternative to Word2 Vec, and it can handle subwords, which might be useful if my application deals with partial matches.\\n\\nNext, I need a way to store these vectors so they can be searched efficiently. The assistant talked about FAISS, which is a library for similarity search. That sounds like exactly what I need because it's optimized for fast approximate searches. But I think FAISS is more for similarity-based searches rather than exact matches. How do I handle when the query isn't an exact match but close to one?\\n\\nThen there's Milvus, another vector search engine that can handle large datasets. It seems like both FAISS and Milvus are good options, but maybe depending on the scale of my application. If it's small, FAISS might be easier to set up. But if I expect a lot of queries or a large corpus, Milvus could be better.\\n\\nAfter that, building an API is essential so other services can send requests and receive results. The assistant mentioned Flask for microservices and FastAPI as an alternative. I've heard FastAPI is faster and more modern, so maybe it's worth learning that instead of Flask. Also, using ASGI instead of WSGI could make the server more efficient.\\n\\nFor the search logic itself, implementing a function that takes user input, generates embeddings, and then searches through the stored vectors makes sense. But how do I handle ranking? Do I just get the most similar vector, or do I have to implement some way to score them?\\n\\nOn the database side, I need something that can store vectors efficiently and allow for fast lookups. Maybe using a NoSQL database like MongoDB or PostgreSQL with specific data types that can handle vectors. But how do I structure the documents so that they're easily searchable? Should each document have its vector stored as well as metadata?\\n\\nTesting will be important too, especially since embeddings can sometimes have accuracy issues. The assistant mentioned sentence embedding models, which might be necessary if I'm dealing with phrases instead of individual words.\\n\\nThe user experience is another consideration. How do users input their search queries? Do they type in text, submit a form, and get results based on that? Also, how to handle cases where the query doesn't exactly match anything but has similar concepts.\\n\\nCost is something to think about too. If I'm using cloud-based services like AWS or Google Cloud with their managed solutions, it might be more accessible than setting up everything from scratch, especially if I don't have a lot of computational resources on my own servers.\\n\\nI also wonder about the efficiency of generating embeddings. For each query, is there a limit to how many vectors can be processed quickly? If the application is going to handle a lot of queries, I might need to optimize the embedding generation process, maybe using pre-processing steps or caching results.\\n\\nAnother thing is handling multiple languages. If my service needs to support different languages, I'll have to decide on monolingual or multilingual embeddings. Word2Vec can be trained for specific languages, while BERT embeddings are multilingual. But training BERT might require more resources and expertise.\\n\\nIn terms of deployment, containerization with Docker could help package the application into a container that can run anywhere. Caching strategies might also improve performance by storing frequently accessed results or embeddings to reduce processing time on subsequent queries.\\n\\nDocumentation is important too. Users will need clear instructions on how to use the service effectively. Also, error handling and rate limiting should be considered to prevent abuse or overuse of the service.\\n\\nI'm a bit overwhelmed with all these aspects, but breaking it down into steps might help. First, choose the vector embedding method, then pick the search engine, set up the API, store data appropriately, implement the query logic, handle results ranking, and ensure efficient processing. Testing each component as I go would make it manageable.\\n\\nI should probably start by installing one of the deep learning libraries like TensorFlow or PyTorch and try a simple embedding model. Then experiment with FAISS or Milvus to see how they integrate with my data. Once that's working, move on to building the API with FastAPI and set up a basic search endpoint.\\n\\nI'm also thinking about data preprocessing. What format do I need my text data in? Should I use TF-IDF for term frequency, or is embedding generation sufficient? Maybe a combination of both could improve the search results by using TF-IDF for ranking and embeddings for similarity.\\n\\nOverall, it's a complex task but manageable with the right tools and step-by-step approach. I'll need to stay updated on the latest libraries and maybe join some forums or communities where people discuss vector search applications to troubleshoot any issues.\\n&lt;/think&gt;\\n\\nTo build a web service using vector embeddings for search, follow this structured approach:\\n\\n### 1. **Vector Embedding Tools**\\n   - **Libraries**: Utilize deep learning frameworks like TensorFlow or PyTorch for generating word or sentence embeddings. Pre-trained models such as Word2Vec, FastText, or BERT can be used.\\n     - Example: Use Word2Vec for monolingual embeddings or BERT for multilingual embeddings.\\n\\n### 2. **Vector Search Engines**\\n   - **FAISS**: Implement for efficient similarity search with exact or approximate matches using binary search techniques.\\n   - **Milvus**: Suitable for large datasets and scalable applications, offering efficient vector searches.\\n\\n### 3. **API Development**\\n   - **Framework Selection**: Use FastAPI for a modern, high-performance API, or Flask if starting simpler.\\n   - **ASGI**: Utilize ASGI to enhance server efficiency compared to WSGI.\\n\\n### 4. **Database Integration**\\n   - **NoSQL Databases**: Use MongoDB or PostgreSQL with appropriate data types (e.g., JSONB for storing vectors).\\n   - **Data Structure**: Store documents with text content, embeddings, and metadata for efficient queries.\\n\\n### 5. **Query Processing and Search Logic**\\n   - **Embedding Generation**: For each query, generate embeddings using the chosen model.\\n   - **Search Functionality**: Implement logic to search through stored vectors, handle exact matches, similar embeddings, and ranking mechanisms.\\n\\n### 6. **User Interface and Experience**\\n   - **Input Handling**: Allow text input through forms or APIs for user queries.\\n   - **Output**: Return relevant results based on embedding similarity scores.\\n\\n### 7. **Efficiency Considerations**\\n   - **Preprocessing and Caching**: Optimize query processing with preprocessing steps and caching strategies to improve response times.\\n   - **Dockerization**: Containerize the application for easy deployment and scalability.\\n\\n### 8. **Testing and Validation**\\n   - **Testing Frameworks**: Use pytest or similar tools to validate embedding accuracy and search effectiveness.\\n   - **Sentence Embeddings**: Consider using pre-trained sentence embeddings for phrase-based searches.\\n\\n### 9. **Scalability and Resources**\\n   - **Cloud Services**: Utilize AWS, Google Cloud, or Azure for managed solutions that scale with your application needs.\\n   - **Monitoring**: Implement monitoring tools to track performance and handle high loads efficiently.\\n\\n### 10. **Documentation and User Experience**\\n   - **Comprehensive Documentation**: Provide clear guides for users on how to use the service effectively.\\n   - **Error Handling and Rate Limiting**: Implement measures to manage errors and prevent abuse.\\n\\nBy following these steps, you can build a robust web service that leverages vector embeddings for effective search, providing a seamless user experience while efficiently managing resources.\"\n\n\n\nresults = client.query(\n    collection_name=\"knowledge-base\",\n    query_text=prompt,\n    limit=3,\n)\nresults\n\n[QueryResponse(id='60aabd4c0a9a4323a4d613a068c36e33', embedding=None, sparse_embedding=None, metadata={'document': 'Qdrant is a vector database & vector similarity search engine. It deploys as an API service providing search for the nearest high-dimensional vectors. With Qdrant, embeddings or neural network encoders can be turned into full-fledged applications for matching, searching, recommending, and much more!'}, document='Qdrant is a vector database & vector similarity search engine. It deploys as an API service providing search for the nearest high-dimensional vectors. With Qdrant, embeddings or neural network encoders can be turned into full-fledged applications for matching, searching, recommending, and much more!', score=0.6944532100424724),\n QueryResponse(id='908559de554048d4bdd6342e6dfc96f6', embedding=None, sparse_embedding=None, metadata={'document': 'SentenceTransformers is a Python framework for state-of-the-art sentence, text and image embeddings. You can use this framework to compute sentence / text embeddings for more than 100 languages. These embeddings can then be compared e.g. with cosine-similarity to find sentences with a similar meaning. This can be useful for semantic textual similar, semantic search, or paraphrase mining.'}, document='SentenceTransformers is a Python framework for state-of-the-art sentence, text and image embeddings. You can use this framework to compute sentence / text embeddings for more than 100 languages. These embeddings can then be compared e.g. with cosine-similarity to find sentences with a similar meaning. This can be useful for semantic textual similar, semantic search, or paraphrase mining.', score=0.6269391815273497),\n QueryResponse(id='1e2833b9c65a4101918c4b8fd80d311d', embedding=None, sparse_embedding=None, metadata={'document': 'FastAPI is a modern, fast (high-performance), web framework for building APIs with Python 3.7+ based on standard Python type hints.'}, document='FastAPI is a modern, fast (high-performance), web framework for building APIs with Python 3.7+ based on standard Python type hints.', score=0.5811869898549256)]\n\n\n\ncontext = \"\\n\".join(r.document for r in results)\ncontext\n\n\nmetaprompt = f\"\"\"\nYou are a software architect. \nAnswer the following question using the provided context. \nIf you can't find the answer, do not pretend you know it, but answer \"I don't know\".\n\nQuestion: {prompt.strip()}\n\nContext: \n{context.strip()}\n\nAnswer:\n\"\"\"\n\n# Look at the full metaprompt\nprint(metaprompt)\n\n\nquery_deepseek(metaprompt)\n\n\ndef rag(question: str, n_points: int = 3) -&gt; str:\n    results = client.query(\n        collection_name=\"knowledge-base\",\n        query_text=question,\n        limit=n_points,\n    )\n\n    context = \"\\n\".join(r.document for r in results)\n\n    metaprompt = f\"\"\"\n    You are a software architect. \n    Answer the following question using the provided context. \n    If you can't find the answer, do not pretend you know it, but only answer \"I don't know\".\n    \n    Question: {question.strip()}\n    \n    Context: \n    {context.strip()}\n    \n    Answer:\n    \"\"\"\n\n    return query_deepseek(metaprompt)\n\n\nprint(rag(\"What can the stack for a web api look like?\"))\n\n&lt;think&gt;\nOkay, so I need to figure out what a stack for a web API might look like. Let me start by understanding the question. The user is asking about the structure of a web API's stack, given some context about FastAPI, NGINX, and Docker.\n\nFrom the context, FastAPI is described as a modern, high-performance web framework for building APIs using Python 3.7+. So that might be part of the application layer or backend.\n\nNGINX is mentioned as a high-performance HTTP server and reverse proxy, which typically sits in front of other applications. It's known for its performance, so maybe it's part of the infrastructure layer handling incoming requests.\n\nDocker is a containerization platform that helps build, share, and run applications without managing environments. So Docker would be related to the deployment or orchestration layer, allowing the application to be containerized and deployed efficiently.\n\nPutting this together, the stack for a web API might include FastAPI as the backend framework, NGINX as the reverse proxy and HTTP server, and Docker for containerization and deployment. That makes sense because each component serves a specific role in the overall architecture.\n\nI should structure it step by step: starting from the application (FastAPI), then the reverse proxy (NGINX), and then the containerization/Docker for deployment. This layered approach ensures that each part is handling its responsibilities efficiently.\n&lt;/think&gt;\n\nThe stack for a web API could consist of FastAPI as the backend framework, NGINX as the reverse proxy and HTTP server, and Docker for containerization and deployment.\n\n**Answer:**\nA typical web API stack might include:\n- **Application Layer**: FastAPI (for building APIs with Python).\n- **Reverse Proxy/HTTP Server**: NGINX (handling incoming requests and routing).\n- **Containerization/Orchestration**: Docker (deploying the application efficiently).\n\nThis layered approach ensures efficient handling of both backend logic and infrastructure.\n\n\n\nprint(rag(\"Where is the nearest grocery store?\"))\n\n&lt;think&gt;\nOkay, so the user is asking where the nearest grocery store is. Hmm, but looking at the context provided, it's all about software stuff: Qdrant, FastAPI, NGINX. None of that mentions anything related to grocery stores or locations. So I can't find any information here that would help answer their question. I should just say I don't know because there's no relevant data in the context. No point in making something up or guessing; better to be honest.\n&lt;/think&gt;\n\nI don't know.\n\n\n\nprint(rag(prompt))\n\n&lt;think&gt;\nOkay, so I need to figure out the tools required to build a web service that uses vector embeddings for search. Let me start by understanding what each part means.\n\nFirst, vector embeddings are like converting text or data into numerical vectors, right? These vectors can then be used to find similar items, which is useful for things like search or recommendations.\n\nLooking at the context provided, there are three tools mentioned: Qdrant, SentenceTransformers, and FastAPI. Let me break them down.\n\nQdrant is described as a vector database and search engine. It acts as an API service, so it probably handles the actual storage and searching of these vectors. That makes sense because building a database from scratch would be time-consuming. So if I need to deploy a vector search service quickly, Qdrant might be the way to go.\n\nNext, SentenceTransformers is a Python framework for generating embeddings. It can handle text in over 100 languages and provides sentence embeddings. This tool is useful for tasks like semantic search or paraphrase mining because it generates vectors that represent the meaning of the text. So if I need to create these embeddings, SentenceTransformers would be necessary.\n\nThen there's FastAPI, which is a Python web framework known for its high performance. It uses standard type hints and is great for building APIs. Since the goal is to build a web service, having an efficient API layer is crucial. So combining FastAPI with the other tools might allow me to serve embeddings and perform searches via an API.\n\nPutting it all together: I think the process would involve using SentenceTransformers to generate the embeddings (maybe from text input), storing them in Qdrant's database, and then using FastAPI to create an endpoint that allows users to query this database. This way, clients can send a query, get it processed through FastAPI, which communicates with Qdrant to find the most relevant vectors based on cosine similarity or other similarity measures.\n\nWait, but do I need anything else? Maybe some kind of middleware or orchestration tools? Or perhaps a reverse proxy like Gunicorn if I'm deploying multiple services. But the question specifically asks for tools to build the web service using vector embeddings, so maybe those aren't necessary unless specified.\n\nI should also consider deployment aspects. If Qdrant is already providing an API service, maybe it's hosted somewhere or I need to deploy it myself with Docker or Kubernetes. FastAPI would be deployed alongside it, perhaps on the same infrastructure.\n\nSo in summary, the three main tools needed are:\n\n1. SentenceTransformers for generating embeddings from text.\n2. Qdrant for storing and searching these vectors.\n3. FastAPI to create the web interface and API endpoints that interact with both SentenceTransformers and Qdrant.\n\nI don't think there's a need for other tools unless there's more complexity, like data preprocessing or additional features beyond basic search. But based on the provided context, these three seem essential.\n&lt;/think&gt;\n\nTo build a web service using vector embeddings for search, you should use:\n\n1. **SentenceTransformers**: Generate text embeddings from input text.\n2. **Qdrant**: Store and search these embeddings efficiently.\n3. **FastAPI**: Build the API endpoints to handle requests and interact with Qdrant.\n\nThese tools collectively allow you to create a service that processes text, converts it into vectors using SentenceTransformers, stores them in Qdrant for fast similarity searches, and serves results via an API built with FastAPI.",
    "crumbs": [
      "Essentials: 5 Minute RAG with Qdrant and DeepSeek"
    ]
  },
  {
    "objectID": "ss101_001_basic.html",
    "href": "ss101_001_basic.html",
    "title": "Semantic Search 101: Build your first Semantic Search Engine in 5 minutes",
    "section": "",
    "text": "https://qdrant.tech/documentation/beginner-tutorials/search-beginners/\nfrom fastembed import TextEmbedding\nfrom qdrant_client import QdrantClient, models\n\n# [model[\"model\"] for model in TextEmbedding.list_supported_models()]\nembedding_model = TextEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\nmodel = [\n    model\n    for model in TextEmbedding.list_supported_models()\n    if \"all-MiniLM\" in model[\"model\"]\n][0]\nmodel\n\n{'model': 'sentence-transformers/all-MiniLM-L6-v2',\n 'dim': 384,\n 'description': 'Text embeddings, Unimodal (text), English, 256 input tokens truncation, Prefixes for queries/documents: not necessary, 2021 year.',\n 'license': 'apache-2.0',\n 'size_in_GB': 0.09,\n 'sources': {'url': 'https://storage.googleapis.com/qdrant-fastembed/sentence-transformers-all-MiniLM-L6-v2.tar.gz',\n  'hf': 'qdrant/all-MiniLM-L6-v2-onnx'},\n 'model_file': 'model.onnx'}",
    "crumbs": [
      "Semantic Search 101: Build your first Semantic Search Engine in 5 minutes"
    ]
  },
  {
    "objectID": "ss101_001_basic.html#add-the-dataset",
    "href": "ss101_001_basic.html#add-the-dataset",
    "title": "Semantic Search 101: Build your first Semantic Search Engine in 5 minutes",
    "section": "1 Add the dataset",
    "text": "1 Add the dataset\n\ndocuments = [\n    {\n        \"name\": \"The Time Machine\",\n        \"description\": \"A man travels through time and witnesses the evolution of humanity.\",\n        \"author\": \"H.G. Wells\",\n        \"year\": 1895,\n    },\n    {\n        \"name\": \"Ender's Game\",\n        \"description\": \"A young boy is trained to become a military leader in a war against an alien race.\",\n        \"author\": \"Orson Scott Card\",\n        \"year\": 1985,\n    },\n    {\n        \"name\": \"Brave New World\",\n        \"description\": \"A dystopian society where people are genetically engineered and conditioned to conform to a strict social hierarchy.\",\n        \"author\": \"Aldous Huxley\",\n        \"year\": 1932,\n    },\n    {\n        \"name\": \"The Hitchhiker's Guide to the Galaxy\",\n        \"description\": \"A comedic science fiction series following the misadventures of an unwitting human and his alien friend.\",\n        \"author\": \"Douglas Adams\",\n        \"year\": 1979,\n    },\n    {\n        \"name\": \"Dune\",\n        \"description\": \"A desert planet is the site of political intrigue and power struggles.\",\n        \"author\": \"Frank Herbert\",\n        \"year\": 1965,\n    },\n    {\n        \"name\": \"Foundation\",\n        \"description\": \"A mathematician develops a science to predict the future of humanity and works to save civilization from collapse.\",\n        \"author\": \"Isaac Asimov\",\n        \"year\": 1951,\n    },\n    {\n        \"name\": \"Snow Crash\",\n        \"description\": \"A futuristic world where the internet has evolved into a virtual reality metaverse.\",\n        \"author\": \"Neal Stephenson\",\n        \"year\": 1992,\n    },\n    {\n        \"name\": \"Neuromancer\",\n        \"description\": \"A hacker is hired to pull off a near-impossible hack and gets pulled into a web of intrigue.\",\n        \"author\": \"William Gibson\",\n        \"year\": 1984,\n    },\n    {\n        \"name\": \"The War of the Worlds\",\n        \"description\": \"A Martian invasion of Earth throws humanity into chaos.\",\n        \"author\": \"H.G. Wells\",\n        \"year\": 1898,\n    },\n    {\n        \"name\": \"The Hunger Games\",\n        \"description\": \"A dystopian society where teenagers are forced to fight to the death in a televised spectacle.\",\n        \"author\": \"Suzanne Collins\",\n        \"year\": 2008,\n    },\n    {\n        \"name\": \"The Andromeda Strain\",\n        \"description\": \"A deadly virus from outer space threatens to wipe out humanity.\",\n        \"author\": \"Michael Crichton\",\n        \"year\": 1969,\n    },\n    {\n        \"name\": \"The Left Hand of Darkness\",\n        \"description\": \"A human ambassador is sent to a planet where the inhabitants are genderless and can change gender at will.\",\n        \"author\": \"Ursula K. Le Guin\",\n        \"year\": 1969,\n    },\n    {\n        \"name\": \"The Three-Body Problem\",\n        \"description\": \"Humans encounter an alien civilization that lives in a dying system.\",\n        \"author\": \"Liu Cixin\",\n        \"year\": 2008,\n    },\n]\n\n\nembeddings = list(\n    embedding_model.embed([document[\"description\"] for document in documents])\n)",
    "crumbs": [
      "Semantic Search 101: Build your first Semantic Search Engine in 5 minutes"
    ]
  },
  {
    "objectID": "ss101_001_basic.html#create-a-collection",
    "href": "ss101_001_basic.html#create-a-collection",
    "title": "Semantic Search 101: Build your first Semantic Search Engine in 5 minutes",
    "section": "2 Create a collection",
    "text": "2 Create a collection\n\nclient = QdrantClient(\":memory:\")\nclient.create_collection(\n    collection_name=\"my_books\",\n    vectors_config=models.VectorParams(\n        size=model[\"dim\"], distance=models.Distance.COSINE\n    ),\n)\n\nTrue",
    "crumbs": [
      "Semantic Search 101: Build your first Semantic Search Engine in 5 minutes"
    ]
  },
  {
    "objectID": "ss101_001_basic.html#upload-data-to-collection",
    "href": "ss101_001_basic.html#upload-data-to-collection",
    "title": "Semantic Search 101: Build your first Semantic Search Engine in 5 minutes",
    "section": "3 Upload data to collection",
    "text": "3 Upload data to collection\n\nclient.upload_points(\n    collection_name=\"my_books\",\n    points=[\n        models.PointStruct(id=idx, vector=embeddings[idx], payload=doc)\n        for idx, doc in enumerate(documents)\n    ],\n)",
    "crumbs": [
      "Semantic Search 101: Build your first Semantic Search Engine in 5 minutes"
    ]
  },
  {
    "objectID": "ss101_001_basic.html#query",
    "href": "ss101_001_basic.html#query",
    "title": "Semantic Search 101: Build your first Semantic Search Engine in 5 minutes",
    "section": "4 Query",
    "text": "4 Query\n\nhits = client.query_points(\n    collection_name=\"my_books\",\n    query=list(embedding_model.embed(\"alien invasion\"))[0],\n    limit=3,\n).points\n\nfor hit in hits:\n    print(hit.payload, \"score:\", hit.score)\n\n{'name': 'The War of the Worlds', 'description': 'A Martian invasion of Earth throws humanity into chaos.', 'author': 'H.G. Wells', 'year': 1898} score: 0.5700932336027419\n{'name': \"The Hitchhiker's Guide to the Galaxy\", 'description': 'A comedic science fiction series following the misadventures of an unwitting human and his alien friend.', 'author': 'Douglas Adams', 'year': 1979} score: 0.5040469745831534\n{'name': 'The Three-Body Problem', 'description': 'Humans encounter an alien civilization that lives in a dying system.', 'author': 'Liu Cixin', 'year': 2008} score: 0.4590294008142361",
    "crumbs": [
      "Semantic Search 101: Build your first Semantic Search Engine in 5 minutes"
    ]
  },
  {
    "objectID": "ss101_001_basic.html#narrow-down-the-query",
    "href": "ss101_001_basic.html#narrow-down-the-query",
    "title": "Semantic Search 101: Build your first Semantic Search Engine in 5 minutes",
    "section": "5 Narrow down the query",
    "text": "5 Narrow down the query\n\nhits = client.query_points(\n    collection_name=\"my_books\",\n    query=list(embedding_model.embed(\"alien invasion\"))[0],\n    query_filter=models.Filter(\n        must=[models.FieldCondition(key=\"year\", range=models.Range(gte=2000))]\n    ),\n    limit=1,\n).points\n\nfor hit in hits:\n    print(hit.payload, \"score:\", hit.score)\n\n{'name': 'The Three-Body Problem', 'description': 'Humans encounter an alien civilization that lives in a dying system.', 'author': 'Liu Cixin', 'year': 2008} score: 0.4590294008142361",
    "crumbs": [
      "Semantic Search 101: Build your first Semantic Search Engine in 5 minutes"
    ]
  },
  {
    "objectID": "ss101_003_hybrid_search.html",
    "href": "ss101_003_hybrid_search.html",
    "title": "Semantic Search 101: Build a Hybrid Search Service with FastEmbed and Qdrant",
    "section": "",
    "text": "https://qdrant.tech/documentation/beginner-tutorials/hybrid-search-fastembed/\n\n# Import client library\nfrom qdrant_client import QdrantClient, models\n\nclient = QdrantClient(path=\"tmp/startups\")\n\n\n---------------------------------------------------------------------------\nBlockingIOError                           Traceback (most recent call last)\nFile ~/Documents/go/python-fastembed/.venv/lib/python3.12/site-packages/portalocker/portalocker.py:118, in lock(file_, flags)\n    117 try:\n--&gt; 118     LOCKER(file_, flags)\n    119 except OSError as exc_value:\n    120     # Python can use one of several different exception classes to\n    121     # represent timeout (most likely is BlockingIOError and IOError),\n   (...)    126     # inherit) and check the errno (which should be EACCESS or EAGAIN\n    127     # according to the spec).\n\nBlockingIOError: [Errno 35] Resource temporarily unavailable\n\nThe above exception was the direct cause of the following exception:\n\nAlreadyLocked                             Traceback (most recent call last)\nFile ~/Documents/go/python-fastembed/.venv/lib/python3.12/site-packages/qdrant_client/local/qdrant_local.py:134, in QdrantLocal._load(self)\n    133 try:\n--&gt; 134     portalocker.lock(\n    135         self._flock_file,\n    136         portalocker.LockFlags.EXCLUSIVE | portalocker.LockFlags.NON_BLOCKING,\n    137     )\n    138 except portalocker.exceptions.LockException:\n\nFile ~/Documents/go/python-fastembed/.venv/lib/python3.12/site-packages/portalocker/portalocker.py:131, in lock(file_, flags)\n    128 if exc_value.errno in (errno.EACCES, errno.EAGAIN):\n    129     # A timeout exception, wrap this so the outer code knows to try\n    130     # again (if it wants to).\n--&gt; 131     raise exceptions.AlreadyLocked(\n    132         exc_value,\n    133         fh=file_,\n    134     ) from exc_value\n    135 else:\n    136     # Something else went wrong; don't wrap this so we stop\n    137     # immediately.\n\nAlreadyLocked: [Errno 35] Resource temporarily unavailable\n\nDuring handling of the above exception, another exception occurred:\n\nRuntimeError                              Traceback (most recent call last)\nCell In[20], line 4\n      1 # Import client library\n      2 from qdrant_client import QdrantClient,models\n----&gt; 4 client = QdrantClient(path=\"tmp/startups\")\n\nFile ~/Documents/go/python-fastembed/.venv/lib/python3.12/site-packages/qdrant_client/qdrant_client.py:130, in QdrantClient.__init__(self, location, url, port, grpc_port, prefer_grpc, https, api_key, prefix, timeout, host, path, force_disable_check_same_thread, grpc_options, auth_token_provider, cloud_inference, check_compatibility, **kwargs)\n    125     self._client = QdrantLocal(\n    126         location=location,\n    127         force_disable_check_same_thread=force_disable_check_same_thread,\n    128     )\n    129 elif path is not None:\n--&gt; 130     self._client = QdrantLocal(\n    131         location=path,\n    132         force_disable_check_same_thread=force_disable_check_same_thread,\n    133     )\n    134 else:\n    135     if location is not None and url is None:\n\nFile ~/Documents/go/python-fastembed/.venv/lib/python3.12/site-packages/qdrant_client/local/qdrant_local.py:67, in QdrantLocal.__init__(self, location, force_disable_check_same_thread)\n     65 self.aliases: dict[str, str] = {}\n     66 self._flock_file: Optional[TextIOWrapper] = None\n---&gt; 67 self._load()\n     68 self._closed: bool = False\n\nFile ~/Documents/go/python-fastembed/.venv/lib/python3.12/site-packages/qdrant_client/local/qdrant_local.py:139, in QdrantLocal._load(self)\n    134     portalocker.lock(\n    135         self._flock_file,\n    136         portalocker.LockFlags.EXCLUSIVE | portalocker.LockFlags.NON_BLOCKING,\n    137     )\n    138 except portalocker.exceptions.LockException:\n--&gt; 139     raise RuntimeError(\n    140         f\"Storage folder {self.location} is already accessed by another instance of Qdrant client.\"\n    141         f\" If you require concurrent access, use Qdrant server instead.\"\n    142     )\n\nRuntimeError: Storage folder tmp/startups is already accessed by another instance of Qdrant client. If you require concurrent access, use Qdrant server instead.\n\n\n\n\nclient.set_model(\"sentence-transformers/all-MiniLM-L6-v2\")\n\n# comment this line to use dense vectors only\nclient.set_sparse_model(\"prithivida/Splade_PP_en_v1\")\n\n\nvectors_config = client.get_fastembed_vector_params()\nvectors_config\n\n{'fast-all-minilm-l6-v2': VectorParams(size=384, distance=&lt;Distance.COSINE: 'Cosine'&gt;, hnsw_config=None, quantization_config=None, on_disk=None, datatype=None, multivector_config=None)}\n\n\n\nsparse_vectors_config = client.get_fastembed_sparse_vector_params()\nsparse_vectors_config\n\n{'fast-sparse-splade_pp_en_v1': SparseVectorParams(index=SparseIndexParams(full_scan_threshold=None, on_disk=None, datatype=None), modifier=None)}\n\n\n\nif not client.collection_exists(\"startups\"):\n    client.create_collection(\n        collection_name=\"startups\",\n        vectors_config=vectors_config,\n        # comment this line to use dense vectors only\n        sparse_vectors_config=sparse_vectors_config,\n    )\n\n\nimport json\n\npayload_path = \"startups_demo.json\"\nmetadata = []\ndocuments = []\n\nwith open(payload_path) as fd:\n    for line in fd:\n        obj = json.loads(line)\n        documents.append(obj.pop(\"description\"))\n        metadata.append(obj)\n\n\ndocuments = documents[:1000]\nmetadata = metadata[:1000]\n\n\nfrom tqdm.notebook import tqdm\n\n\nclient.add(\n    collection_name=\"startups\",\n    documents=documents,\n    metadata=metadata,\n    # parallel=0,  # Use all available CPU cores to encode data.\n    # Requires wrapping code into if __name__ == '__main__' block\n    ids=tqdm(range(len(documents))),\n)\n\n\ndef search(text: str, query_filter=None):\n    search_result = client.query(\n        collection_name=\"startups\",\n        query_text=text,\n        query_filter=query_filter,  # If you don't want any filters for now\n        limit=5,  # 5 the closest results\n    )\n    # `search_result` contains found vector ids with similarity scores\n    # along with the stored payload\n\n    # Select and return metadata\n    metadata = [hit.metadata for hit in search_result]\n    return metadata\n\n\nsearch(\"robotics\")\n\n[{'document': 'Creative materials for better brand awarness \\nRoboToaster main goal is to assist companies and individuals create brand awareness. Whether you need engaging design, quality video, or amazing events. RoboToaster LLC, strives to create compelling and consistent stories through visual marketing tactics to give ...',\n  'name': 'RoboToaster',\n  'images': 'https://d1qb2nb5cznatu.cloudfront.net/startups/i/128687-71dfa1846b8ebc0da88e1fe933d51c73-thumb_jpg.jpg?buster=1349796491',\n  'alt': 'RoboToaster -  video advertising events brand marketing',\n  'link': 'http://RoboToaster.co',\n  'city': 'Chicago'},\n {'document': \"A new way to trade.\\nDesignByRobots' Trading App automates a critical part of the stock trading process-- making the leap from an incomprehensible number of possible directions to take your strategy development process to a manageable set of back-tested strategies to select from. ...\",\n  'name': 'DesignByRobots',\n  'images': 'https://d1qb2nb5cznatu.cloudfront.net/startups/i/25679-e154adc712fd35fbfeb3b93087d20b8b-thumb_jpg.jpg?buster=1317835125',\n  'alt': 'DesignByRobots -  software trading',\n  'link': 'http://designbyrobots.com/',\n  'city': 'Chicago'},\n {'document': 'Projection immersion (aka virtual reality) beyond #Oculus for Brick & Mortar \\nOur mission is to make immersive technology as purposeful as possible. In order to do this, we employ cost efficient systems in developing vital solutions.\\nWe are developing a core tech + platform (software) for user locomotion in 3D virtual environments. Initial ...',\n  'name': 'Semiautomatic Semiotics',\n  'images': 'https://d1qb2nb5cznatu.cloudfront.net/startups/i/1302-f3947d958b2d41eda43f69185abfd0d5-thumb_jpg.jpg?buster=1315770608',\n  'alt': 'Semiautomatic Semiotics  -  digital signage virtual reality San Jose',\n  'link': 'http://semiautomatic3d.com',\n  'city': 'Chicago'},\n {'document': 'Gesture Recognition Platform\\nWe at Rithmio have pushed the boundaries in the gesture recognition market by providing advanced gesture recognition as a platform. Our software libraries are built to run on any platform, including wearables, smartphones, or any other connected motion sensing ...',\n  'name': 'Rithmio',\n  'images': 'https://d1qb2nb5cznatu.cloudfront.net/startups/i/505509-8455465d442dc9cf2112f2caa4b29771-thumb_jpg.jpg?buster=1412717451',\n  'alt': 'Rithmio -  fitness human computer interaction Wearables',\n  'link': 'http://rithmio.com',\n  'city': 'Chicago'},\n {'document': 'Your apps. Simplified.\\nCloudbot is a mobile and web application that is an efficient solution to having your personal data and relationships scattered around on different services.\\nPeople rely on different applications to access the little bits of their lives saved in the cloud. ...',\n  'name': 'Cloudbot',\n  'images': 'https://d1qb2nb5cznatu.cloudfront.net/startups/i/1873-a83644b8a2d18d4b91742faf5c4eab7b-thumb_jpg.jpg?buster=1315741829',\n  'alt': 'Cloudbot -  mobile messaging social media platforms development platforms',\n  'link': 'http://cloudbot.com',\n  'city': 'Chicago'}]\n\n\n\nquery_filter = models.Filter(\n    must=[models.FieldCondition(key=\"city\", match=models.MatchValue(value=\"Chicago\"))]\n)\nsearch(\"robotic\", query_filter)\n\n[{'document': 'Creative materials for better brand awarness \\nRoboToaster main goal is to assist companies and individuals create brand awareness. Whether you need engaging design, quality video, or amazing events. RoboToaster LLC, strives to create compelling and consistent stories through visual marketing tactics to give ...',\n  'name': 'RoboToaster',\n  'images': 'https://d1qb2nb5cznatu.cloudfront.net/startups/i/128687-71dfa1846b8ebc0da88e1fe933d51c73-thumb_jpg.jpg?buster=1349796491',\n  'alt': 'RoboToaster -  video advertising events brand marketing',\n  'link': 'http://RoboToaster.co',\n  'city': 'Chicago'},\n {'document': \"A new way to trade.\\nDesignByRobots' Trading App automates a critical part of the stock trading process-- making the leap from an incomprehensible number of possible directions to take your strategy development process to a manageable set of back-tested strategies to select from. ...\",\n  'name': 'DesignByRobots',\n  'images': 'https://d1qb2nb5cznatu.cloudfront.net/startups/i/25679-e154adc712fd35fbfeb3b93087d20b8b-thumb_jpg.jpg?buster=1317835125',\n  'alt': 'DesignByRobots -  software trading',\n  'link': 'http://designbyrobots.com/',\n  'city': 'Chicago'},\n {'document': 'Projection immersion (aka virtual reality) beyond #Oculus for Brick & Mortar \\nOur mission is to make immersive technology as purposeful as possible. In order to do this, we employ cost efficient systems in developing vital solutions.\\nWe are developing a core tech + platform (software) for user locomotion in 3D virtual environments. Initial ...',\n  'name': 'Semiautomatic Semiotics',\n  'images': 'https://d1qb2nb5cznatu.cloudfront.net/startups/i/1302-f3947d958b2d41eda43f69185abfd0d5-thumb_jpg.jpg?buster=1315770608',\n  'alt': 'Semiautomatic Semiotics  -  digital signage virtual reality San Jose',\n  'link': 'http://semiautomatic3d.com',\n  'city': 'Chicago'},\n {'document': 'Virtual reality operating system\\nWe believe that the adoption of VR technology will represent a sea change in how people interact with computers and collaborate with each other.\\nAnarchist is a Chicago-based software startup building a virtual reality operating system and developer toolkit. Smart ...',\n  'name': 'Anarchist',\n  'images': 'https://d1qb2nb5cznatu.cloudfront.net/startups/i/470881-ad62c25b4e82d5cd7f4561911e5a7493-thumb_jpg.jpg?buster=1408821642',\n  'alt': 'Anarchist -  productivity software developer tools digital entertainment virtual reality',\n  'link': 'http://anarchist.com',\n  'city': 'Chicago'},\n {'document': 'Your apps. Simplified.\\nCloudbot is a mobile and web application that is an efficient solution to having your personal data and relationships scattered around on different services.\\nPeople rely on different applications to access the little bits of their lives saved in the cloud. ...',\n  'name': 'Cloudbot',\n  'images': 'https://d1qb2nb5cznatu.cloudfront.net/startups/i/1873-a83644b8a2d18d4b91742faf5c4eab7b-thumb_jpg.jpg?buster=1315741829',\n  'alt': 'Cloudbot -  mobile messaging social media platforms development platforms',\n  'link': 'http://cloudbot.com',\n  'city': 'Chicago'}]\n\n\n\nsearch(\"finance\")\n\n[{'document': 'Financial Education for Young Professionals\\nSilver Step educates new financial consumers on the basics of wealth management, and lets them discover service providers that fit their needs.\\nThink of us as Yelp for financial services, with an educational component.',\n  'name': 'Silver Step',\n  'images': 'https://d1qb2nb5cznatu.cloudfront.net/startups/i/31271-7023073ac555ea3a430a4b8dbdbcb615-thumb_jpg.jpg?buster=1325694652',\n  'alt': 'Silver Step -  financial services education',\n  'link': 'http://www.thesilverstep.com',\n  'city': 'Chicago'},\n {'document': 'Changing the way you borrow with safer, faster, better financial products\\nAvant is changing the way consumers borrow money. Utilizing advanced algorithms and machine-learning capabilities, the company offers a unique and highly customized approach to the personal loan process. The combination of technology, analytics and customer service ...',\n  'name': 'Avant',\n  'images': 'https://d1qb2nb5cznatu.cloudfront.net/startups/i/168565-45d9c198ede0987843d850c5a963f084-thumb_jpg.jpg?buster=1434042946',\n  'alt': 'Avant -  analytics machine learning big data web development',\n  'link': 'https://www.avant.com',\n  'city': 'Chicago'},\n {'document': 'Technologically-enhanced financial mentoring for teens\\nMoneythink (MT) targets urban 11th and 12th graders. These students have dreams, but between age 17 and 20, they face many financial decisions that can make or break their future: getting/keeping a job, using a paycheck, credit cards, paying for college. To prepare ...',\n  'name': 'Moneythink',\n  'images': 'https://d1qb2nb5cznatu.cloudfront.net/startups/i/282361-0ae5f20b6d9a7433190bf03064b22902-thumb_jpg.jpg?buster=1382098658',\n  'alt': 'Moneythink -  financial services education ventures for good nonprofits',\n  'link': 'http://moneythink.org',\n  'city': 'Chicago'},\n {'document': 'The Private Network for Investments',\n  'name': 'Fundology',\n  'images': 'https://d1qb2nb5cznatu.cloudfront.net/startups/i/175739-0cbc4996113b629ab83dd3c528eedf8f-thumb_jpg.jpg?buster=1366819766',\n  'alt': 'Fundology -  financial services investment management finance',\n  'link': 'http://www.fundology.com',\n  'city': 'Chicago'},\n {'document': 'Personal Financial Execution (TM) system\\nSpendbot is a Personal Financial Execution solution to create a spending plan, optimize it, and stick to it so as to align spending with informed values. We combine cloud-based data visualization, algorithms, expert systems, and partner integrations to deliver ...',\n  'name': 'Spendbot',\n  'images': 'https://d1qb2nb5cznatu.cloudfront.net/startups/i/247454-ffa9f5098f7664641acc300d26590c9a-thumb_jpg.jpg?buster=1376078533',\n  'alt': 'Spendbot -  lead generation ventures for good personal finance health and wellness',\n  'link': 'http://www.spendbot.com',\n  'city': 'Chicago'}]",
    "crumbs": [
      "Semantic Search 101: Build a Hybrid Search Service with FastEmbed and Qdrant"
    ]
  }
]